<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>blog by Joe Moon</title>
		<link>http://blog.byjoemoon.com/</link>
		<description>Joe Moon on tech and other things.</description>
		<language>en-us</language>
		<pubDate>Mon, 12 Mar 2012 17:11:08 PDT</pubDate>
		<webMaster>mail@byjoemoon.com (Joe Moon)</webMaster>
		<atom:link href="http://blog.byjoemoon.com/feed/" rel="self" type="application/rss+xml" />

		
		<item>
			<title>An Amateur's Lament</title>
			<link>http://blog.byjoemoon.com/amateurs-lament</link>
			<pubDate>Wed, 30 Nov 2011 00:00:00 PST</pubDate>
			<guid>http://blog.byjoemoon.com/amateurs-lament</guid>
			<description><html><p>Amazon&rsquo;s <a href="http://www.youtube.com/watch?v=jUtmOApIslE">advertisement for the Kindle
Fire</a> opens with a Voltaire
quote:</p>

<blockquote>
  <p>The instruction we find in books is like fire. We fetch it from our
neighbors, kindle it at home, communicate it to others, and it becomes
property of all.</p>
</blockquote>

<h3 id="visions-of-the-future">Visions of the Future</h3>

<p>Taken from a certain angle, the metaphor makes sense. There&rsquo;s a
<a href="http://en.wikipedia.org/wiki/Prometheus">Promethean</a> ideal of content &ndash;
music, movies, books &ndash; no longer constrained by the physical media of
the past, and no longer subject to the old Olympian gate-keepers: record
labels, Hollywood, and publishers. The promise of these devices is
accessibility of content to all. The killer feature of each one is the
price, which is aggressive to begin with and, if the past is any
indicator, subject to relentless and continuous cuts.</p>

<p>Apple has a similar vision of the future but approaches from the
opposite direction. It&rsquo;s more an offer of luxury to the masses, a
democratization of high-end design. But the result is basically the
same. The emphasis is on the device itself, but it&rsquo;s a vision of
technological access made available to everyone.</p>

<p>Both utopian visions, though, hide the threat of dystopia not far
underneath the surface, as utopian visions often do. In the case of
Apple&rsquo;s &ldquo;post-PC&rdquo; devices, it&rsquo;s a subtle one, hidden in the very
experience that makes the devices attractive.</p>

<h3 id="production-and-consumption">Production and Consumption</h3>

<p>The iPad is physically not much more than a touchscreen, which is
primarily just a screen with the addition of an input system. While the
direct-manipulation paradigm is indisputably more intuitive, it&rsquo;s also
<a href="http://blog.byjoemoon.com/post/9325300749/a-different-kind-of-gui">less
robust</a>.
You simply can&rsquo;t get the precision of a mouse, or the tactile feedback
of a physical keyboard. It&rsquo;s a user-interface optimized for consumption
and shallow interactions like navigation. Which isn&rsquo;t to say that it&rsquo;s
impossible to create things on an iPad; there are plenty of examples to
the contrary. But its primary purpose is for output, not input, unlike
the general-purpose PC. The iPad implicitly privileges consumption over
production. So insofar as these post-PC devices will replace PCs, the
market- and mindshare of consumption-oriented devices will grow at the
expense of production-oriented ones.</p>

<h3 id="distribution-control">Distribution Control</h3>

<p>Amazon&rsquo;s proposition with the Kindle line is subtly different. While the
Kindle (which term I use generically hereafter to refer to the entire
line of Kindles, including the e-book readers and the Kindle Fire) is
openly a pure consumption device, it&rsquo;s also cheap enough to be offered
as a supplement to PCs as opposed to a replacement. Their purpose is
narrow: to remove all friction from the consumption of digital media.
Amazon, as a retail company, is trying to aggressively commoditize its
complements while controlling the distribution channel.</p>

<p>For Amazon to make money through this model, though, everything has to
flow through Amazon. And with how much of the e-book retail business
Amazon already controls, it&rsquo;s an outcome that doesn&rsquo;t seem unlikely.
With a free month&rsquo;s subscription to Amazon&rsquo;s Prime service with every
purchase of a Kindle Fire, they&rsquo;re instructing consumers on how to
easily stream movies from their catalog and buy music directly in
addition to buying everything else, from toilet paper to power tools.
Meanwhile, music streaming services are becoming more popular. The
inexorable trend seems to be toward subscription services for
everything. It isn&rsquo;t difficult to imagine a future in which you can play
a flat rate to Amazon for access to an all-you-can-eat streaming service
for all content: music, magazines, newspapers, books, movies, tv shows;
all on free hardware ranging from e-ink devices, tablets, set-top boxes
(hooked up to televisions bought from Amazon with one click and 2-day
shipping).</p>

<h3 id="centralization">Centralization</h3>

<p>In Apple&rsquo;s vision of the future, you buy apps and content on iTunes,
which you consume on your various iDevices. In Amazon&rsquo;s, you get a free
Kindle with a subscription to Amazon Prime, and stream all your content.
The real price of the ease and ubiquity of access offered by both of
these companies is centralization of control. These visions represent a
step backward into our recent past of broadcast media, with a few
powerful entities acting as gatekeepers to our collective culture. Which
may seem inoffensive if you prefer curated, professionally produced
content anyway. But then you might realize that Apple plays an actively
censorial role in its App Store. You might find that Amazon has
bizarrely tone-deaf and comically ironic behavior in its recent past,
like remotely deleting purchased copies of George Orwell&rsquo;s <em>1984</em> from
Kindles.</p>

<p>The most frightening and subversive part of Amazon&rsquo;s strategy to me is
the Silk Browser. While offering the proposition of a better browsing
experience, Amazon has begun to insinuate itself into our great
decentralizing force: the World Wide Web.</p>

<p>It is perhaps simply the historical cycle of a new technology&rsquo;s
decentralizing force dissipating into subsequent centralized control
described by Tim Wu in <em><a href="http://timwu.org/">The Master Switch</a></em>.</p>

<h3 id="lamentation">Lamentation</h3>

<p>My lament, then, is for the dying bazaar. For the crushed dream of the
blogging revolution, replaced by closed and proprietary systems like
Facebook. For the notion of putting the means of production into the
hands of the masses, replaced by ubiquitous and unimaginably slick
consumption devices. For a flourishing chaos of freely exchanged ideas,
curated, ranked, and sorted by people, organizations, and algorithms
that are themselves curated, ranked, and sorted. For a more
participatory World Wide Web.</p>

<h3 id="an-alternative-vision-of-the-future">An Alternative Vision of the Future</h3>

<p>Not that it&rsquo;s over. As I&rsquo;ve noted
<a href="http://blog.byjoemoon.com/post/166900257/why-i-trust-google">before</a>,
Google is a huge and powerful engineering monastery that has a symbiotic
relationship with the Web and DNA consisting of decentralized production
and distribution. And there are plenty of smaller efforts to make the
Web a competitive endeavor, as well certain types of participatory
<a href="http://blog.byjoemoon.com/post/6542036868/project-depth">depth</a> and
richness of experience that only the Web can provide. And we&rsquo;ve only
begun to see the new frontier of possible decentralized business models
for content production afforded by the open Web.</p>

<p>Looking around the Web, though, it&rsquo;s not that hard to understand why
people might seek refuge in closed systems. A lot of it is unavoidable.
Some places on the Web will always be cesspools. But there are some
matters of convention we can change by consensus to make the Web a
better, more participatory place that encourages deep, serious thought
and collaboration as well as frivolous ephemera. There are ways to
optimize reading, civil discussion, and community-building. And the
beauty of a consensual, conventional system is that I just have to
convince enough people that this is worth doing.</p>

<h3 id="more-reading-on-apple">More Reading on Apple</h3>

<ul>
  <li><a href="http://teleogistic.net/2011/10/done-with-apple/">&ldquo;Done with Apple&rdquo;</a>
by Boone B Gorges</li>
  <li><a href="https://freedom-to-tinker.com/blog/felten/ipad-disneyland-computers">&ldquo;Disneyland of
computers&rdquo;</a>
by Ed Felten</li>
</ul>

<h3 id="more-reading-on-amazon">More Reading on Amazon</h3>

<ul>
  <li><a href="http://www.wired.com/magazine/2011/11/ff_bezos/all/1">Interview of Amazon CEO Jeff
Bezos</a> by
Steven Levy</li>
  <li><a href="http://www.theatlantic.com/technology/archive/2011/10/what-would-happen-if-amazon-ruled-publishing/246854/">&ldquo;What Would Happen if Amazon Ruled
Publishing?&rdquo;</a>
by Rebecca J. Rosen</li>
  <li><a href="http://www.themillions.com/2011/08/the-e-reader-of-sand-the-kindle-and-the-inner-conflict-between-consumer-and-booklover.html">&ldquo;The E-Reader of Sand: The Kindle and the Inner Conflict Between
Consumer and
Booklover,&rdquo;</a>
Mark O&rsquo;Connell on the Kindle as Borgesian &ldquo;Book of Sand&rdquo;</li>
</ul>

<h3 id="more-reading-on-post-pc">More Reading on &ldquo;Post-PC&rdquo;</h3>

<ul>
  <li>Hacker News thread on <a href="http://news.ycombinator.com/item?id=3113192">&ldquo;Why I&rsquo;m scared of the post-PC
era.&rdquo;</a></li>
  <li>Hacker News thread on <a href="http://news.ycombinator.com/item?id=2955472">&ldquo;The post-PC
era&rdquo;</a></li>
  <li><a href="http://futureoftheinternet.org/the-pc-is-dead-why-no-angry-nerds">&ldquo;The PC is dead. Why no angry
nerds?&rdquo;</a>
by Jonathan Zittrain</li>
</ul>

<h3 id="more-reading">More Reading</h3>

<ul>
  <li><a href="http://esr.ibiblio.org/?p=3335">&ldquo;World without web&rdquo;</a> by Eric S.
Raymond</li>
  <li>Tim Wu on <a href="http://bits.blogs.nytimes.com/2010/11/14/one-on-one-tim-wu-author-of-the-master-switch/"><em>The Master
Switch</em></a></li>
</ul>
</html></description>
		</item>
		
		<item>
			<title>We Are What We Choose</title>
			<link>http://blog.byjoemoon.com/we-are-what-we-choose</link>
			<pubDate>Wed, 09 Nov 2011 00:00:00 PST</pubDate>
			<guid>http://blog.byjoemoon.com/we-are-what-we-choose</guid>
			<description><html><p>In <a href="http://thesocietypages.org/cyborgology/2011/11/03/equipment-why-you-cant-convince-a-cyborg-shes-a-cyborg/">this post on
Cyborgology</a>,
PJ Rey convincingly speaks to the project of dismissing the concept of
digital dualism. He argues that as technology becomes sufficiently
advanced and ubiquitous, we begin to think of it as natural extensions
of ourselves.</p>

<blockquote>
  <p>As such, is [sic] not hyperbolic to claim, for example, that Facebook
is a piece of equipment that has become an extension of our very
consciousness. As equipment, social media fundamentally alters who we
are.</p>
</blockquote>

<p>Rings true for me. But more than anything else it emphasizes the
importance of thinking carefully about the ways in which we are
extending ourselves.</p>

<p>Noam Chomsky
<a href="http://www.brightestyoungthings.com/articles/the-secret-of-noam-a-chomsky-interview.htm">provoked</a>
a great discussion in the last few weeks by offhandedly dismissing new
media as &ldquo;extremely rapid, very shallow communication.&rdquo; Nathan Jurgenson
<a href="http://www.salon.com/2011/10/23/why_chomsky_is_wrong_about_twitter/singleton/">responded in a piece in
Salon</a>,
talking about the great benefits of new media, e.g. its utility in
popular movements like the Arab Spring and &ldquo;Occupy&rdquo; protests, as well as
the biases inherent in Chomsky&rsquo;s somewhat crotchety dismissal.</p>

<blockquote>
  <p>Chomksy, of all people, ought to take note. When he defends his form
of communicating (printed books and periodical essays) with claims
that tweeting/texting lacks depth, he is implicitly suggesting that
nonwhites and those in the Third World are inherently communicating
less deeply than their white and first-world counterparts. He doesn&rsquo;t
seem to know enough about the reality of social media to examine his
own assumptions.</p>
</blockquote>

<p>But ultimately I agree most with <a href="http://figureground.ca/2011/10/31/why-chomsky-is-wrong-about-twitter-a-rebuttal/">this
post</a>
by Mike Plugh.</p>

<blockquote>
  <p>Chomsky&rsquo;s critiques, while they seem to have some overtones of
morality, read in a particular way, aren&rsquo;t all that far off the mark.
The shallowness of social media forms, like Twitter, is both the
strength and the weakness of their nature. As a medium, Twitter is
largely superficial, shallow, and evanescent. One can only be so
‘deep&rsquo; in 140 characters or less, and so the superficiality of the
messages prompt higher involvement by the audience to derive meaning
(a largely positive characteristic), while at the same time leaving
out important depth behind the communication.</p>
</blockquote>

<blockquote>
  <p>…</p>
</blockquote>

<blockquote>
  <p>Jurgenson rightly points out that, &ldquo;To some degree (and we can debate
how large this is) social media has enabled a mass manufacturing of
dissent.&rdquo; However, dissent is a contrary position, and a position
fueled by dissatisfaction and emotion. Immediate communication of a
type less-considered and nuanced is largely emotional in nature.
Again, this is a strength when outrage and action are the prescription
for intolerable circumstances, but do nothing little for producing
solutions or the important enlightened perspectives that must
accompany any change for it to be lasting and serve humanity most
broadly. Dissent as a state of moral indignation is useful and
important, but is also the same fuel that ignites the mob or the riot
when lacking in substance or focus. Manufacturing consent, as Chomsky
famously observed, is a process of propaganda which relies on
appealing superficially to people&rsquo;s gut emotions and less deeply
considered convictions.</p>
</blockquote>

<p>Both posts are entirely worth reading.</p>

<p>I take from this discussion that not only do different digital tools
have different strengths and weaknesses for different activities, but
they each encourage certain types of behavior. To quote
<a href="http://blog.byjoemoon.com/post/3112676038/the-end-of-comments">myself</a>:</p>

<blockquote>
  <p>[The] quality of the connections on Facebook is very poor. And this is
because the signals are low bandwidth by design. Input fields for text
updates are small and restrictive. Photos are easy to share but poorly
presented, the system implemented not to share rich, expressive
photos, but low-grade, mundane, documentarian ones: I was here; I did
this; I was with these people. Ubiquitous interaction is reduced to
comment fields that encourage short, <em>ad hoc</em> reactions, or&ndash;worse&ndash;to
the elemental, further irreducible unary piece of information: the
&ldquo;like&rdquo; (though I suppose the &ldquo;poke&rdquo; contains even less information).
Facebook casts an extremely wide net into the waters of the collective
mind-space of its users, but that net penetrates only into the
shallowest of depths.</p>
</blockquote>

<blockquote>
  <p>Twitter provides accessibility with a similar sacrifice of bandwidth.
I&rsquo;ve always felt like Twitter&rsquo;s great innovation was not the arbitrary
character limit, but the frictionless interface that collapses the
separation between input and output. The system itself doesn&rsquo;t judge
how noteworthy your expression is, leaving that to the network at
large in as close to a democratic way as is really conceivable. But
the limitations, also similarly to Facebook&rsquo;s, enforce a shallowness
of sentiment, encourage pith over comprehensiveness, and discourage
real discourse, or any sort of conversation at all.</p>
</blockquote>

<p><a href="http://blog.byjoemoon.com/post/6542036868/project-depth">And the web itself sometimes seems inimical to depth of
thought</a>.</p>

<p>That we are subsuming technologies into ourselves seems trivial compared
to the point that what we are subsuming encourage us to think only in a
certain way. If we examine carefully what we are becoming, I don&rsquo;t think
we&rsquo;ll like what we find.</p>

<p>And some, like <a href="http://www.theatlantic.com/magazine/archive/2008/07/is-google-making-us-stupid/6868/">Nicholas
Carr</a>,
have already, and they don&rsquo;t like what they find at all. He, and,
presumably, Chomsky, would advocate, then, a retreat back to the old
media. Full-length books are what encourage the deep, sustained
concentration necessary for truly worthwhile thought. I think &ldquo;Digital
Fasting&rdquo; movements (restrictions of internet/screen time) also stem from
a sentiment that these technologies are having a pernicious effect on
us.</p>

<p>But a retreat to old media can&rsquo;t be the correct answer. Not only because
of the limits of physical media&ndash;like relegation to passive consumption
and the lack of communal experience&ndash;but also because the old media have
assumptions of their own built in that are just as pernicious. There is
an authoritarian centralization of power inherent to the old publishing
model, for example, as well as limited viable formats. Ideas of a
certain size, too long for long-form, but too short for full-length
books, simply had no way to exist in old media. They had to either be
cut down or inflated to fill viable formats.</p>

<p>New media have no such restrictions. In fact, our new digital tools are
the most versatile, plastic, and accessible that we&rsquo;ve ever had. We
should be honest and clear about what our tools are doing to us. But we
should then focus our energy on creating the tools that we want, instead
of trying to stop the inexorable tide of change.</p>
</html></description>
		</item>
		
		<item>
			<title>In Defense of the New Google Reader</title>
			<link>http://blog.byjoemoon.com/in-defense-of-the-new-google-reader</link>
			<pubDate>Wed, 02 Nov 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/in-defense-of-the-new-google-reader</guid>
			<description><html><p>Brian Shih, a former PM for Google Reader, wrote <a href="http://brianshih.com/78073742">a scathing take-down
of the recent changes to Google Reader</a>
on Monday.</p>

<p>And plenty of his criticisms are well deserved. It&rsquo;s certainly true that
the visual changes seem to prioritize consistency with the rest of
Google&rsquo;s visual redesign over the practical considerations of the
product.</p>

<p>I think the visual criticisms are ultimately pretty minor, though. There
were plenty of complaints about the preview of Gmail&rsquo;s recent user
interface changes as well, but in the official release, Google seems to
have addressed most of them. And Shih himself agrees that Google should
be trying to visually unify its products. Finally, let&rsquo;s not kid
ourselves: the old Reader was uglier, and not terribly more usable. It
was just what we got used to.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> Will Google eventually find the
right balance between visual consistency and actual usability? I&rsquo;ll
reserve judgment for now.</p>

<p>The real meat of Shih&rsquo;s criticism, though, is for the new sharing model.
Again, he agrees that Google <em>should</em> integrate Reader with Plus. But he
goes on to make some criticisms that don&rsquo;t fully take into account the
differences between the two products, and others that simply get the
facts wrong.</p>

<p>First, he complains that the new sharing flow makes sharing items
<em>harder</em>. Fair enough. But, as I wrote in <a href="http://blog.byjoemoon.com/post/10755504272/intimacy-and-performance-on-facebook">an earlier blog
post</a>,
this is purposeful. It adds friction at the point of sharing, makes you
pause to consider who the audience for this particular item is. Which is
how it should be.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>Second, he complains that you must publicly +1 a post in order to share
it. This is incorrect, as he notes in an update.</p>

<p>Third, he complains at the lack of a transition from the old sharing
model to the new one under Google+. And this seems to be what
<a href="http://kirbybits.wordpress.com/2011/10/21/wherein-i-try-to-explain-why-google-reader-is-the-best-social-network-created-so-far/">many</a>
people seem to be most angry about. Google Reader users are widely upset
that the communities that have organically grown in their feed reader
has disappeared. What these people forget is that the old sharing model
and the communities that formed in it were opaque, irrational, and hard
to control. Discoverability was basically nonexistent, loosely based on
your Gmail contacts. It was never very clear who could see your posts,
and it was never clear who could see the comments you make on posts.</p>

<p>The new sharing model, under Google+, is perfectly clear about who can
see your posts, who can see your comments, granular about who you can
share with, and gives you more control over comment moderation. In
short: it&rsquo;s better in every way.</p>

<p>I agree with Shih that it&rsquo;s unfortunate not to be able to consume shared
content from Reader. In fact, I&rsquo;d go so far as to say I&rsquo;d prefer to
consume, +1, and share all my content through Reader&rsquo;s interface, and
not have to visit the G+ site ever again.</p>

<p>The one glaring omission from the new sharing model to me is the
omission of an important point on <a href="http://blog.byjoemoon.com/post/582452757/webs-and-streams">the spectrum
public-ness</a>:
public but not broadcast. Under G+ now, if I want to share an item
publicly, I am also thrusting it in front of everyone who has me
circled. But sometimes I don&rsquo;t want to do that. Sometimes I want to
share something about bipedal robots, for example, that I don&rsquo;t care if
it&rsquo;s public, but I also don&rsquo;t want to stuff in everyone&rsquo;s feed who knows
me, for the same reason that I don&rsquo;t care if anyone knows about my fear
of bipedal robots, but I also don&rsquo;t bring them up in conversation with
everyone I meet.</p>

<p>What G+ needs is publicly subscribable Circles. These would be feeds
that you can post to that are public, but only people who have chosen to
subscribe to them can see. It fits nicely with the granularity of G+,
and adds the organic growth model of the old Reader in a much clearer
way.</p>

<p>Google is notorious for releasing new products that seem half-baked.
Only a few days after the public outcry over changes in Reader, there is
<a href="http://www.theverge.com/2011/11/2/2533292/gmail-app-iphone-ipad">a new
one</a>
over the new native Gmail client for iOS. Google does this a lot. But it
also iterates on these products at a furious rate. And it appears to do
this in reponse to public feedback. Again, the new Gmail redesign has
addressed almost all of the initial criticisms as far as I can tell.
Which means the public outcry is good. It tells Google what to fix. I
just wish the outcry was over stuff that matters instead of what strikes
me as simple aversion to change.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>There were plenty of browser extensions and user styles that ‘fixed&rsquo;
the old Google Reader appearance along many of precisely the ways
Shih complains about here. <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>I do also lament the removal of keyboard shortcuts, but I assume
they&rsquo;ll be added back in soon.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Intimacy is Performance</title>
			<link>http://blog.byjoemoon.com/intimacy-is-performance</link>
			<pubDate>Wed, 19 Oct 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/intimacy-is-performance</guid>
			<description><html><p>I was surprised to find a lot of interesting discussion around <a href="http://blog.byjoemoon.com/post/10755504272/intimacy-and-performance-on-facebook">my last
post about Intimacy and Performance on
Facebook</a>.
Some of it was about the distinction between ‘intimacy&rsquo; and
‘performance&rsquo; and a lot was about
<a href="http://www.susansontag.com/SusanSontag/books/onPhotographyExerpt.shtml">mediated</a>
<a href="http://smarterware.org/8584/joe-moon-on-facebooks-new-timeline-feature">experience</a>.
Both of the discussion tracks got me thinking about our relationship to
communication technology.</p>

<p>We can measure the progress of communication technology in two ways:</p>

<ul>
  <li>The reduction of physical limits on social interaction.</li>
  <li>The increase of signal fidelity of digital communication to
pre-technological social communication.</li>
</ul>

<p>Inventions like writing, telegraphy, telephony, etc. break down the
limits of physical interactions geographically and temporally, but each
introduces new limits. Telephony, for example, broke down the barrier of
distance, but introduced limits of one-to-one and voice-only. As
telephony and its replacements improved, these introduced limits
disappeared. With group video chat technology, we can now have fairly
high-fidelity social experiences across long distances without those
early limits, and we can safely expect the fidelity to continue to
increase.</p>

<h3 id="intimacy-is-performance">Intimacy is Performance</h3>

<p>At least part of the reason I posed a distinction between intimacy and
performance in my last post is that that&rsquo;s one of the limits that our
technology imposes on our social interactions. Offline, intimacy is much
easier to understand as a subset of performance: performance in a
smaller context. In our analog social interactions, we have many
different contexts that overlap in complex ways, and we perform slightly
differently in all of them. In the context of close friends and family,
that performance is called ‘intimacy.&rsquo;</p>

<h3 id="limits">Limits</h3>

<p>Earlier communication technologies were easier to understand and adopt
because they were simpler, and they more easily mapped to physical
interactions. Letter correspondence and telephony were easy to
understand as versions of a face to face conversation. Email, SMS, and
chat are similarly easy to understand. But the increasingly complex
world of online social networking does not map to the offline very well.</p>

<p>Chris Poole, founder of 4chan,
<a href="http://www.readwriteweb.com/archives/4chans_chris_poole_facebook_google_are_doing_it_wr.php">provided</a>
one interesting criticism at Web 2.0 conference, which @dannygilligan
<a href="https://twitter.com/#!/dannygilligan/statuses/126075365318852608">sums up
nicely</a>
on Twitter: &ldquo;Sharing on the web not a question of who you share with but
who you share as.&rdquo; But that&rsquo;s only half-correct. The fact is, offline
these two concepts &ldquo;who you share with&rdquo; and &ldquo;who you share as&rdquo; are
really the same thing: context. It&rsquo;s only online that they&rsquo;re separated
artificially, and this asymmetry is one of the limits of the digital
social medium.</p>

<p>Many of these limits are ones of sharp distinction: <a href="http://blog.byjoemoon.com/post/582452757/webs-and-streams">between private and
public; and between one-to-one and
one-to-all</a>.
While things like email lists made the representation of different
contexts possible, they didn&rsquo;t make it easy or frictionless. Facebook
and Google+, with their respective list features, each take a step
toward removing some of the friction by offering more granular contexts,
but these steps are not big or particularly inspired ones.</p>

<p>Even with Google+ Circles, to take an example, the asymmetricality and
nonobviousness of the social context places friction on every
interaction. When you post to a Circle, you decide what your context is.
But when you comment on someone else&rsquo;s post, it takes some work to
figure out the context, and thus, your behavior. That&rsquo;s friction. This
is what I was trying to get at with an earlier
<a href="http://blog.byjoemoon.com/post/7072771434/a-new-metaphor-for-social-networking">post</a>
on ‘place&rsquo; as a more intuitive metaphor for social networking.</p>

<p>Context is something that might be hard to think about when implementing
a social network, precisely because it&rsquo;s so ubiquitous, obvious, and
unconscious in an offline setting. But these are reasons why it should
be a prime consideration for any online social interaction service.</p>

<h3 id="digital-augmentation-of-social-interaction">Digital Augmentation of Social Interaction</h3>

<p>Of course, in the digital medium, there are many novel concepts that
simply can&rsquo;t be analogized to offline interactions easily. The fact of a
permanently and publicly accessible record of interactions is the most
obvious one. The spectrum of publicity is another. I would argue that
the spaces on that spectrum that digital media open up to us are
genuinely enriching and useful. It means, for example, that @mention
conversations on Twitter, which aren&rsquo;t broadcast (don&rsquo;t appear in
followers&rsquo; timelines with exceptions) but are publicly accessible if you
look for them, provide access to an entire category of information on
the internet that was never available before.</p>

<p>I think these are the type of technological innovations that are drastic
enough, i.e. don&rsquo;t map well enough to previous paradigms, that they
represent inevitable shifts in the culture. It&rsquo;s hard to recognize calls
for a new ‘forgetfulness&rsquo; on the internet as anything other than
nostalgic Ludditism, at least for me. I think making social context
clear through user interfaces will mitigate some of these effects. For
example, a permanent record of statements you make within a specific
context will only ever be available to others who share that context, so
the eternal memory of the internet doesn&rsquo;t matter as much as the eternal
memory of the public internet. But some of the digital age&rsquo;s affordances
will be drastically culture-changing, not features or bugs, but simply
facts of digital life.</p>

<h3 id="presence">Presence</h3>

<p>There are also more ambiguous artifacts of digital representation. The
combination of ubiquitous access and time-shifting capabilities afforded
by the internet blur the old distinction between real-time presence
(face to face, telephone) and correspondence (letters, email, SMS).
There is now more of a spectrum of synchronous to asynchronous
communication, with some technologies, like Twitter and chat, existing
on a broad range of it.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<h3 id="collapse">Collapse</h3>

<p>It seems pretty clear to me that as digital identity and analog identity
become increasingly collapsed, the notion of presence will resolve away
from binary states and toward a constantly fluctuating analog amount. We
will always be available for digital communication at some level, but we
will be more or less digitally present depending on what we&rsquo;re doing and
what kind of access we have at any given point. This is already true
enough that the idea of going completely ‘off the grid&rsquo; for some amount
of time is increasingly popular and we are already starting develop
<a href="http://www.nytimes.com/2010/12/12/fashion/12THISLIFE.html?pagewanted=all">social
norms</a>
on behavior related to immediate level of physical-digital presence.</p>

<p>Nathan Jurgenson, who writes about &ldquo;<a href="http://thesocietypages.org/cyborgology/2011/02/24/digital-dualism-versus-augmented-reality/">digital
dualism</a>&rdquo;
on <a href="http://thesocietypages.org/cyborgology/">his blog</a>, emphasizes that
the distinction is artificial.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> I sort of disagree. I think
the virtual is currently another context with varying degrees of overlap
with other contexts, just as ‘work&rsquo; and ‘home&rsquo; and ‘friends&rsquo; are
different contexts. And somewhat paradoxically, I think the distinction
of ‘digital&rsquo; as a separate context will fade and collapse as our digital
tools more accurately represent the distinctions of our offline
contexts.</p>

<p>But I don&rsquo;t think publicity-pushing policies like Facebook&rsquo;s help to
close the gaps of digital dualism. Facebook&rsquo;s goal is to obliterate
context online by defaulting everything to public. If anything, this
abuse of users&rsquo; trust is driving them to compartmentalize their digital
lives more. It drives them toward more conscious and more active
management of their digital identities, which is the exact opposite of
what Facebook wants.</p>

<p>The problem left to solve is removing the friction of cognition involved
in placing yourself in and managing your social context.</p>

<h3 id="further-reading">Further Reading</h3>

<ol>
  <li><a href="http://www.theatlantic.com/technology/archive/2011/10/does-facebook-emphasize-the-me-or-the-i/246467/">Alexis Madrigal on
identity</a>.</li>
  <li><a href="http://www.danah.org/">danah boyd</a>&rsquo;s
<a href="http://smg.media.mit.edu/people/danah/thesis/">thesis</a> on &ldquo;Managing
representation in a digital world.&rdquo;</li>
</ol>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Fascinating research by danah boyd on teenagers&rsquo; risk reduction
strategies on Facebook.
http://www.zephoria.org/thoughts/archives/2010/11/08/risk-reduction-strategies-on-facebook.html<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><a href="http://www.williamgibsonbooks.com/archive/2003_01_28_archive.asp">William Gibson on the subject of
cyborgs</a>:
&ldquo;The real cyborg, cybernetic organism in the broader sense, had been
busy arriving as I watched DR. SATAN on that wooden television in 1952. I was becoming a part of something, in the act of watching
that screen. We all were. We are today. The human species was
already in process of growing itself an extended communal nervous
system, then, and was doing things with it that had previously been
impossible: viewing things at a distance, viewing things that had
happened in the past, watching dead men talk and hearing their
words. What had been absolute limits of the experiential world had
in a very real and literal way been profoundly and amazingly
altered, extended, changed. And would continue to be. And the real
marvel of this was how utterly we took it all for granted.</p>

      <p>Science fiction&rsquo;s cyborg was a literal chimera of meat and machine.
The world&rsquo;s cyborg was an extended human nervous system: film,
radio, broadcast television, and a shift in perception so profound
that I believe we&rsquo;re yet to understand it. Watching television, we
each became aspects of an electronic brain. We became augmented. In
the Eighties, when Virtual Reality was the buzzword, we were
presented with images of…television! If the content is sufficiently
engrossing, however, you don&rsquo;t need wraparound deep-immersion
goggles to shut out the world. You grow your own. You are there.
Watching the content you most want to see, you see nothing else.</p>

      <p>The physical union of human and machine, long dreaded and long
anticipated, has been an accomplished fact for decades, though we
tend not to see it. We tend not to see it because we are it, and
because we still employ Newtonian paradigms that tell us that
&ldquo;physical&rdquo; has only to do with what we can see, or touch. Which of
course is not the case. The electrons streaming into a child&rsquo;s eye
from the screen of the wooden television are as physical as anything
else. As physical as the neurons subsequently moving along that
child&rsquo;s optic nerves. As physical as the structures and chemicals
those neurons will encounter in the human brain. We are implicit,
here, all of us, in a vast physical construct of artificially linked
nervous systems. Invisible. We cannot touch it.</p>

      <p>We are it. We are already the Borg, but we seem to need myth to
bring us to that knowledge.&rdquo; <a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Intimacy and Performance on Facebook</title>
			<link>http://blog.byjoemoon.com/intimacy-and-performance-on-facebook</link>
			<pubDate>Tue, 27 Sep 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/intimacy-and-performance-on-facebook</guid>
			<description><html><p>Facebook launched three interesting new features last Thursday:
Timeline, Ticker, and Open Graph.</p>

<h3 id="timeline">Timeline</h3>

<p>Timeline is a replacement for the profile page that collects and
displays your Facebook status updates, photos, links, etc. in a clean,
easily perusable way. One of the first things I thought of after seeing
<a href="http://www.facebook.com/about/timeline">the announcement</a>, was <a href="http://www.youtube.com/watch?v=GIHq8nry9hY">this
&ldquo;Dear Sophie&rdquo; ad</a> from
Google.</p>

<p>Superficially, they&rsquo;re very similar. Both showcase the use of technology
to capture meaningful events and share them. Even the types of
relationships and events are similar. But the different presentations
betray an important distinction in purpose between the two sets of
services.</p>

<p>The &ldquo;Dear Sophie&rdquo; ad shows technology in service of creating records and
sharing moments in an intimate way, and ends with the sentiment: &ldquo;The
web is what you make of it.&rdquo; While the Timeline promotional video shows
similar vignettes, <a href="http://www.youtube.com/watch?v=v67PFmVvqDs">Mark Zuckerberg&rsquo;s
presentation</a> closes the
video with: &ldquo;Timeline is the story of your life. It has three pieces:
all your stories, your apps and a new way to express who you are.&rdquo;</p>

<p>In other words, while &ldquo;Dear Sophie&rdquo; is an expression of an intimate set
of messages from one individual to another, the Timeline is an
aggregated set of shared moments, collected in the service of public
display, an artifact of self-expression and identity. And what I find
jarring about this formulation is the same thing that bothers me about
the alarming trend of weddings in which the photographers and
videographers have free reign, even during the ceremony, in order to get
the best, most cinematic record of the event, at the expense of the
event itself and everyone participating. It&rsquo;s a conflation of the record
of the event with the event itself, or even a privileging of the record
over what gives the record its meaning and power. At the same time it
(ingeniously) adds to the pressure to record all meaningful events on
Facebook in order to make sure it becomes part of your
identity.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<h3 id="ticker-and-open-graph">Ticker and Open Graph</h3>

<p>Open Graph adds a way for Facebook apps and Web sites to make updates
for you in various ways. For example, if you give it the proper
permissions, the newly integrated Spotify app will automatically share
each new song you listen to. But these updates don&rsquo;t go into your main
newsfeed, they go instead into the Ticker, a new area on your Facebook
page that shows a running update of your friends&rsquo; auto-shared activities
online. Other examples of Ticker activity include: reading an article,
watching a video, playing a game.</p>

<p>Zuckerberg refers to this as &ldquo;frictionless sharing,&rdquo; a dream of a kind
of meta-panopticon with which everyone can see what everyone else is
doing in real time. There are parts of this that I actually find kind of
compelling. The Ticker is a nod toward &ldquo;<a href="http://www.disambiguity.com/ambient-intimacy/">ambient
intimacy</a>&rdquo; the idea that
you can approximate the low-level intimacy of occupying the same space
as someone, in a digital way.</p>

<p>But, by being public, the Ticker fails at achieving intimacy. Because if
being in the same room with someone creates intimacy, being in the same
room as everyone creates the opposite. It turns all of your activity
into performance. And what
<a href="http://thisismynext.com/2011/09/25/facebook-frictionless-sharing-timeline-panopticon/">many</a>
have hinted at is that removing friction from sharing just displaces
that friction. If everything I do on the web is under the public gaze, I
have to reflect for a moment before I take any action &ndash; before I listen
to a song, watch a video, play a game, or <em>click</em> on a link. It simply
moves the friction from sharing onto the activity, in the worst kind of
self-censorial way.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup><sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>This is especially true in combination with the Timeline, which
aggregates Ticker activity into &ldquo;a new way to express who you are.&rdquo; Not
only does the Open Graph and the Ticker turn your online activity into a
performance, it then turns that performance into your very identity.</p>

<h3 id="intimacy-vs-performance">Intimacy vs. Performance</h3>

<p>Both intimacy and performance are important parts of what makes social
networks like Facebook compelling. Initially, the performance part was
the profile, where you listed your biographical information, your likes
and dislikes, your expressions of identity (in the narrow ways that
Facebook allowed). The performance part was important for discovering
new people and expanding your network, for actually forming the
connections. The intimacy part was pretty much everything else: your
status updates, messages, comments, photos, and videos. It consisted of
the kind of stuff in the two videos above. It was what injected meaning
into those connections that you&rsquo;d made.</p>

<p>At every turn, Facebook seems to have subverted the intimacy of social
experiences by turning them into public performances. Not only has the
intimacy of what was once private slowly eroded into the public, but
more and more of Facebook users&rsquo; online activity is being drawn into the
performative identity. If an anthropomorphized Facebook had a Facebook
profile, its Timeline would show a clear progression of updates that
moves from mostly private toward all public, all the time.</p>

<p>The worst part is that I don&rsquo;t think this has the effect that Facebook
wants. I don&rsquo;t think a frog-boiling style of slow erosion of privacy
means people just continue to share in the same way except in public. It
means that the people who understand what&rsquo;s going on become wary, stop
trusting, and eventually stop using the service. And people who don&rsquo;t
understand what&rsquo;s happening will eventually hit situations in which
something doesn&rsquo;t work the way they thought it did (often
embarrassingly), and the uncertainty of their mental model will result
in less usage and make that usage more tentative and more careful.</p>

<p>Ultimately, it just means less intimacy. Less signal. Less of exactly
what this kind of technology is supposed to enable.</p>

<h3 id="further-reading">Further Reading</h3>

<p><a href="http://www.slate.com/id/2304425/">Not Sharing is Caring</a>, Farhad Manjoo</p>

<p><a href="http://adrianshort.co.uk/2011/09/25/its-the-end-of-the-web-as-we-know-it/">It&rsquo;s the end of the web as we know
it</a>,
Adrian Short</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>There&rsquo;s a lot more to unpack about this, but that would be another
essay, and I think others are saying it better than I can. For
example: <a href="https://twitter.com/#!/boone">@boone</a>: <a href="https://twitter.com/#!/boone/statuses/118794804510466048">The idea that life
narrative should be loosely joined out of small anecdotal pieces
over long periods of time is a non-trivial
thesis.</a>
(There&rsquo;s more good stuff in his tweet stream.)<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>While it&rsquo;s true that you have to explicitly white-list apps that
post to your Ticker, the uncertainty about whether the next click
will be public or not that results from any sort of auto-sharing
system is almost worse than everything being public.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>It seems pertinent to note that Google+ actually <em>adds</em> friction to
sharing by making you explicitly think about which Circle to share
to with every item. Which, I submit, is precisely where you want
friction in a sharing interface that respects your privacy.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>You are the product.</title>
			<link>http://blog.byjoemoon.com/you-are-the-product</link>
			<pubDate>Tue, 06 Sep 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/you-are-the-product</guid>
			<description><html><p>In a talk last Friday at the
<a href="http://2011.dconstruct.org/conference/don-norman">dConstruct</a>
conference, design luminary Don Norman repeated an increasingly popular
refrain about Google: &ldquo;[In] fact, the advertisers are the users and you
are the product.&rdquo; It&rsquo;s the kind of glib soundbite that <a href="http://daringfireball.net/linked/2011/09/05/norman-google">certain
bloggers</a>
can&rsquo;t resist, but collapses on close scrutiny.</p>

<p>The argument goes something like this: You are only truly a company&rsquo;s
customer if you pay them directly for products or services. Google users
don&rsquo;t pay Google directly, therefore Google users are not Google
customers, but products that Google sells to advertisers. The logic that
underlies this argument is that a user must be a customer <em>or</em> a
product, never both, and the implication is that it&rsquo;s bad to be a
‘product.&rsquo;</p>

<p>The sentiment wouldn&rsquo;t be compelling if it didn&rsquo;t have some kernel of
truth. It&rsquo;s certainly true that any advertising-based business model has
a different relationship with its users than a more traditional one that
sells products directly to customers. But the dichotomy between customer
and product is a false one, and it&rsquo;s precisely the incentive structure
of the ad-based model that shows it to be false.</p>

<h3 id="accountability">Accountability</h3>

<p>Ultimately, the argument is one about accountability. Apple, to take an
obvious example, is accountable to its customers because customers pay
them. Their incentive is to make the highest quality product so that
customers keep paying for them. Google, the argument goes, is not
accountable to its users, because the users don&rsquo;t pay Google. It takes
only a moment&rsquo;s consideration to realize that this line of reasoning is
facile at best, and actively dishonest at worst. Because of course
Google is still accountable to its users, just not through direct
monetary transactions. Google is accountable to its users because
advertisers won&rsquo;t pay if they don&rsquo;t have users. Google is also
accountable to advertisers, but that doesn&rsquo;t negate its accountability
to its users. It still needs happy users to make money.</p>

<h3 id="alternatives">Alternatives</h3>

<p>It&rsquo;s really an argument against advertising in general, and ignores the
fact that advertising has historically enabled the production of all
kinds of media, from network television shows to newspapers, to
magazines to <em>ahem</em> tech blogs,<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> that would otherwise have
been impossible. It ignores the fact that Google, specifically, has
given the world access to a mindbogglingly useful utility (its search
engine) for free, supported by ads that are often actually useful. And
that&rsquo;s not to mention all the other amazing products that Google offers
for free.</p>

<h3 id="incentives">Incentives</h3>

<p>There are certainly trade-offs. Because Google can target ads better
with more information, it has an incentive to gather as much data about
its users as possible. But this incentive has always existed for media
companies like newspapers and magazines. Information about the
subscribership was always more valuable than the subscriptions
themselves, because publishers as a matter of course sold their
subscribers&rsquo; information to advertisers. At least Google just uses the
data internally to target ads. This data-gathering incentive will always
leave Google at a point on a spectrum of privacy that some people will
consider invasive and others won&rsquo;t mind.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>But it&rsquo;s not like a direct transaction business doesn&rsquo;t have some bad
incentives. Apple, as a vendor, wants to extract as much money as
possible from its customers. Most of the time this incentive translates
to creating high-quality products. But sometimes it translates to
hindering competition through artifical extra-market means like patent
litigation. Apple also has the incentive to produce its products as
cheaply as possible. Most of the time this incentive translates to
making its operations as efficient as possible. But sometimes it
translates to sacrificing worker conditions. These are both
controversial issues, but my point is that there are good and bad
incentives in any business model, and there are plenty of examples of
abuse in both.</p>

<h3 id="currency">Currency</h3>

<p>I think it&rsquo;s easy for rich people to say things like &ldquo;you are the
product.&rdquo; But I don&rsquo;t think most people, even having considered all the
implications, consider themselves products. I certainly don&rsquo;t. We just
think of ourselves as paying for the products with alternative currency,
and we&rsquo;re glad that we can avail ourselves of these products in exchange
for resources that we have in great supply: our attention and our
information.</p>

<h3 id="meaningless">Meaningless</h3>

<p>The incentives are real. Some are good and some are bad. They can be
useful to point out as explanatory forces for real practices, but
without evidence of real malfeasance, sentiments like &ldquo;you are the
product&rdquo; are just empty rhetoric.</p>

<p>via
<a href="http://gigaom.com/2011/09/05/don-norman-google-doesnt-get-people-it-sells-them/">GigaOm</a></p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The most dishonest part about someone like John Gruber making this
argument is that Daring Fireball is, of course, wholly ad-supported.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>To a large degree, I find Google&rsquo;s gathering of my information &ndash; my
personal data, my browsing habits, my interests &ndash; to be mostly
inoffensive. Keeping that information secret doesn&rsquo;t do me much
good, but Google can aggregate my data with that of others and make
it hugely useful. That&rsquo;s pure wealth creation as far as I&rsquo;m
concerned. <a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Google's Open Hand and Closed Fist</title>
			<link>http://blog.byjoemoon.com/googles-open-hand-and-closed-fist</link>
			<pubDate>Mon, 29 Aug 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/googles-open-hand-and-closed-fist</guid>
			<description><html><p>Paul Buchheit coined Google&rsquo;s unofficial motto&ndash;&ldquo;Don&rsquo;t be evil&rdquo;&ndash;early
in the company&rsquo;s existence. But Google has only been pushing the vague
notion of &ldquo;open&rdquo; in the last few years. The notion is vague at least
partly because it&rsquo;s so all-encompassing and partly because of Google&rsquo;s
penchant for using it in whatever way is most convenient at the time.
But that sword cuts both ways, as it makes it easy for Google&rsquo;s critics
to <a href="http://techcrunch.com/2011/03/26/open/">call the company out</a> when
they don&rsquo;t live up to an equally vague standard. So, what exactly does
Google mean by ‘open,&rsquo; and how open is Google?</p>

<h3 id="open">Open</h3>

<p>As laid out in this <a href="http://googleblog.blogspot.com/2009/12/meaning-of-open.html">this blog
post</a>,
Google refers to a couple of different things with the term: &ldquo;open
technology&rdquo; and &ldquo;open information.&rdquo; Open technology, in turn, consists
of: open source; and open standards and APIs. Google is unequivocally
open on these two technology vectors with respect to certain products:
Chrome, Chrome OS, Wave, Buzz, Pubsubhubbub, etc.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<h3 id="closed">Closed</h3>

<p>On others, they&rsquo;re decidedly closed. Google&rsquo;s search and advertising
businesses are very opaque.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> Which is certainly
understandable, since Google makes the vast majority of its money on
search and ads. It can&rsquo;t really be open with these because their entire
business depends on them, and I think most reasonable people can
understand that.</p>

<p>Many have also rightly criticized Google for its notoriously bad
customer service, but I don&rsquo;t think this qualifies as ‘closedness&rsquo; as
much as poor execution. From what I understand, this opacity isn&rsquo;t a
result of any desire to keep secrets so much as an unwillingness to
regard the problem as a non-engineering one and devote adequate human
resources.</p>

<p>I&rsquo;m not sure if Google&rsquo;s level of secrecy in general is less or greater
than the average large corporation, but my sense is that it&rsquo;s not. It&rsquo;s
a little beside the point, anyway, because this isn&rsquo;t one of the things
Google refers to when it claims to be open.</p>

<h3 id="android">Android</h3>

<p>The venue in which Google has taken the most criticism, Android, is a
complex and ambiguous one. There are several vectors of criticism here,
too. Part of the problem is that there are two distinct parts to
Android. There is the completely open source part, the source code to
which is available to anyone who wants to download it and run on any
device they feel like.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> Then there&rsquo;s the ‘Google experience&rsquo;
Android that ships with Google&rsquo;s closed source apps: Market, Gmail,
Maps, Navigation, and Voice; and only with explicit approval from
Google. For clarity, I&rsquo;ll refer to them as AOSP (for Android Open Source
Project, as its generally referred to in open source circles) and
Android, respectively.</p>

<h3 id="aosp">AOSP</h3>

<p>AOSP has taken some heat from the Open Source Software (OSS) community
for two reasons. First, there&rsquo;s been some discussion of Google possibly
violating the <a href="http://www.gnu.org/copyleft/gpl.html">GPL</a> license, but
there&rsquo;s mostly disagreement on the topic and, from what I gather, even
if true it&rsquo;s a borderline violation. Second is AOSP&rsquo;s somewhat
unconventional operation. Unlike most OSS projects, even other Google
ones like Chrome and Chrome OS, Google develops most of the software
internally with little to no transparency or input from the community,
only periodically dumping the releases into the public. This has been
the Android team&rsquo;s modus operandi since the beginning. In this respect,
Google is more closed with AOSP than most OSS projects, even other
Google ones. With Honeycomb, the latest version of Android, this has
gotten even worse. The delay between Google&rsquo;s release of the source code
to its partners and its release to the public is the longest to date.</p>

<p>But let&rsquo;s put this into perspective by considering the total spectrum of
open to closed. From this vantage, the difference in openness between
AOSP and other OSS projects is basically neglibile compared to the
difference in openness between AOSP and any other major player in the
industry. Because there are no other OSS mobile operating
systems.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> So is Google open with AOSP? I would say yes, even
at its most closed.</p>

<h3 id="developers">Developers</h3>

<p>Another vector within Android is in relation to developers. Here,
Android as a whole is as open as it gets. The Android Market is
available to anyone who wants to develop for it.<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup> Developers
can use any tools they want. And app distribution isn&rsquo;t even restricted
to the Market; i.e. you can install any compatible app from any source,
including simple internet downloads. Amazon has even released their own
app store for Android.</p>

<h3 id="consumers">Consumers</h3>

<p>A third vector is in relation to consumers. Here, again, Android is
completely open. On the devices that Google was directly involved with,
like the original G1, the original Droid, and Google&rsquo;s flagship Nexus
devices, the hardware is unlocked and root access is readily available,
which means you can install third party mods or ROMs, i.e. custom
modifications to the operating system. Some of these third party mods,
like CyanogenMod, have an active, thriving open source development
community, and are adding features faster than Android proper.</p>

<h3 id="manufacturers-and-carriers">Manufacturers and Carriers</h3>

<p>The caveat here, of course, is that the majority of handsets are locked
down by the manufacturers, probably at least in part at the behest of
carriers.<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup> And so here we come to a vector on which Google is
not completely open. Google&rsquo;s policies toward vendors in the early days
of Android were quite open. And the vendors took full advantage, by
locking down the hardware and adding extensive user interface
customizations on top of stock Android in an effort to differentiate
their brands. They also began to pre-install apps that you couldn&rsquo;t
uninstall. It even extended to such liberties as making Bing the default
search engine with no option to switch. The result was a fragmented
Android ecosystem as well as a generally poorer experience for the end
user.</p>

<p>But Google is increasingly using control of its closed source
application suite and access to the Android Market to exert pressure on
the manufacturers and carriers away from these practices. Some of the
details that came out of the Skyhook<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup> trial even indicate that
Google now requires explicit approval over each model before it ships.</p>

<p>So what we see here is Google stuck between opposing vectors: open to
consumers vs. open to vendors. It can&rsquo;t be completely open to both. They
seem increasingly to be choosing openness for consumers, which
necessarily means being more closed with the vendors. So is Google being
less open in some ways with Android? Yes. Do I care? Not a single iota,
because it means they&rsquo;re being more open along a different, more
important vector. In fact, I encourage it.</p>

<h3 id="mostly-open">Mostly Open</h3>

<p>So is Google perfectly open in every way? Of course not. But overall, if
we can consider openness on a spectrum, then, yes, they are more open by
a ridiculous margin than any of their competitors in any business except
search and advertising. In that business they are exactly as open as
everyone else (which is to say: not at all). Is that enough to
legitimately claim to be ‘open&rsquo; as a PR/marketing tactic? I say yes. I
obviously think there&rsquo;s a lot of nuance that they elide over, but I
don&rsquo;t think it&rsquo;s ultimately disingenous. I would appreciate if they
addressed this nuance more, but I have no idea if it would be practical.
(Because does the general public know or care about any of this?
Probably not. Of the people who do care, will any of them be swayed?
Seems unlikely.)</p>

<h3 id="dont-be-evil">&ldquo;Don&rsquo;t be evil.&rdquo;</h3>

<p>So, is Google open out of moral sentiment or out of self-interest? Well,
here&rsquo;s what&rsquo;s special about the company: both. Google has managed to
invent a business model wherein their incentives align with consumers&rsquo;.
Their promotion of an open, decentralized, interoperable Web accessed
through commoditized (maximally available) hardware and software serves
Google and the world at large simultaneously. It also sidesteps the
adversarial nature of selling things to customers. And despite what I
think is a <a href="http://blog.byjoemoon.com/post/7590977101/googles-existential-crisis">fundamental
conflict</a>,
Google does this by making advertising useful.</p>

<p>Are there also some incentives that are bad? Yes. Google has a strong
incentive to collect data. This is mitigated by their need to keep their
user base happy. Which means they have to weigh data collection against
privacy intrusion. But the incentive exists. How is Google doing on this
front? Not perfectly, but certainly better than others, and certainly
not as bad as they could probably get away with without getting in
trouble with the general public.</p>

<p>And I think this is where the company&rsquo;s moral founding principles
emerge. &ldquo;Don&rsquo;t be <a href="http://www.aaronsw.com/weblog/googevil">evil</a>&rdquo; may be
glib and too broad by far. It may lack the nuance to address some real
issues. But it propagates, in a blunt way, from the founders and
leadership down into important decision-making, recruiting, etc. Google
has made compromises, certainly. The partnership with Verizon on net
neutrality and its complex history in China are the two examples that
dismayed me the most. But Google&rsquo;s public and open espousal of such a
simple and powerful principle as &ldquo;Don&rsquo;t be evil&rdquo; leaves them open to
criticism. It makes them accountable for their actions to more than just
the shareholders in an important way.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Google+ is a notable exception to this list. Google hasn&rsquo;t said much
about it, but my working theory is that, after the disastrous
failures of both Wave and Buzz, their plan with Google+ is to try to
offer a solid, compelling user experience first to gain significant
adoption, and only then start to open up the technology for people
to start creating their own interoperable social networking
services. I think some of its inexplicably draconian policies, <a href="http://epeus.blogspot.com/2011/08/google-plus-must-stop-this-identity.html">on
pseudonyms</a>,
for example, make sense from this perspective.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Though, they <em>do</em> <a href="http://insidesearch.blogspot.com/2011/08/another-look-under-hood-of-search.html">talk about
it</a>.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>This is the Android that Andy Rubin was referring to in his infamous
<a href="http://twitter.com/#!/Arubin/status/27808662429">tweet</a>.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Though Nokia did partner with Intel to create an open source mobile
operating system called Meego, it never grew out of vaporware
status. <a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>Though the ability to sell paid applications on the market is
restricted on a country-by-country basis, with full access only
slowly being rolled out. <a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>This means it takes time for third party mod developers to find the
necessary exploits to get root access and bootloader access before
they can install custom mods. While this often happens before the
devices are even released, it also often takes weeks or months, and
sometimes never happens at all. There&rsquo;s been a push away from these
restrictive policies, however. HTC recently released a bootloader
unlocking tool that gives you full access to the hardware, though
its use voids the device&rsquo;s warranty.<a href="#fnref:6" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>More <a href="http://www.socialaw.com/slip.htm?cid=20416&amp;sid=121">here</a>.<a href="#fnref:7" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>A Different Kind of GUI</title>
			<link>http://blog.byjoemoon.com/a-different-kind-of-gui</link>
			<pubDate>Tue, 23 Aug 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/a-different-kind-of-gui</guid>
			<description><html><p>&ldquo;<a href="http://artlung.com/smorgasborg/C_R_Y_P_T_O_N_O_M_I_C_O_N.shtml">In the beginning was the command line</a>.&rdquo;</p>

<p>But then the command line became graphics and dwelt among us. From its earliest days, Apple privileged the lay user over the technical one, regarding the need to understand technical details as friction. With the Lisa and the Macintosh, Apple picked up where <a href="http://en.wikipedia.org/wiki/Xerox_Parc">Xerox Palo Alto Research Center</a> left off. The guiding principles of intuitiveness and discoverability led Apple to replace the command line interface (CLI) with the graphical user interface (GUI), featuring windows, menus, icons, and a pointing device (<a href="http://en.wikipedia.org/wiki/WIMP_(computing)">WIMP</a>).</p>

<p>For these historical reasons, the GUI has always been pointer-driven, at least with respect to computer operating systems. Apple has only recently begun the transition to something new: iOS&rsquo;s direct manipulation interface; still a GUI, but no longer driven by the same windows or menus or a pointing device. With this transition, Apple achieved its oldest dream more successfully than anyone anticipated. And it now seems to be pulling the rest of its product line into that dream. To use Steve Jobs&rsquo; terminology, they&rsquo;re slowly transitioning out of <a href="http://motherjones.com/kevin-drum/2010/06/cars-trucks-and-ipads">the &ldquo;truck&rdquo; business</a>.</p>

<h3 id="the-mouse">The Mouse</h3>

<p>It&rsquo;s enlightening to consider what made the mouse such an indispensable companion to the GUI for so long.</p>

<ol>
  <li>It&rsquo;s intuitive. It&rsquo;s easy to grasp the basic concept that the cursor
 is an extension of the hand, that its motion corresponds to the
 motion of the mouse.</li>
  <li>All interactions are built on a minimal set of axiomatic actions:
 move, hover, click, release, and drag.</li>
  <li>It provides highly precise and unrestricted motion of the cursor, as
 opposed to a joystick, which limits the cursor&rsquo;s direction and
 velocity.</li>
</ol>

<p>With careful application of the WIMP metaphor, these features made every
aspect of a GUI accessible to the user. But this was only the minimal
set afforded by the technology that was available at the time. With
touch screen technology and the direct manipulation paradigm, Apple has
not just removed an entire layer of abstraction, but also reduced that
minimal set of actions even further to a more intuitive level: tap,
swipe, and pinch. The resulting interface is qualitatively more
intuitive and trivially easy for new users.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>But restricting the minimal set of actions that far has implications
that I think Apple is only starting to realize. With such a limited
interaction bandwidth, for example, any given complex action has to take
a correspondingly large number of sequential inputs. Apple&rsquo;s way of
mitigating this effect in iOS appears to be gestures, some of which,
like five-finger pinch, are frankly ridiculous. Traditional desktop GUIs
mitigate this by overloading the basic controls with incrementally added
extra functionality that breaks metaphor: double-click, right-click, the
scroll wheel, any number of additional mouse buttons. These are all
extremely useful additions (because they increase the interaction
bandwidth), but this phenomenon is actually part of what makes modern
desktop GUIs inaccessible to novices and is why I imagine Apple has
traditionally been so resistant to these kinds of innovations. But I
think the underlying problem isn&rsquo;t simply the tendency for cruft to
accumulate, but the severely limiting nature of the metaphor to begin
with. iOS, just a few years old, already contains more undiscoverable,
out-of-metaphor inputs (gestures) than discoverable strictly in-metaphor
ones.</p>

<p>But even aside from the unintuitiveness, there are a lot of reasons not
to like the mouse:</p>

<ol>
  <li>The arbitrary motion of a mouse cursor makes the effort analog, as
 opposed to the digital motion of keying. Commands, even repetitive
 ones, can&rsquo;t be relegated to muscle memory the way command line
 commands or keyboard navigation can.</li>
  <li>Because of this, mousing is a conscious process. Hitting precise
 targets takes a lot more cognitive effort and close attention than
 keying.</li>
  <li>Moving the mouse hand back and forth between mouse and keyboard is
 often an annoying cognitive task-switch and that breaks flow.</li>
  <li>The requirement of moving back and forth between mouse and keyboard
 creates friction for typing. And since typing is one of the main
 ways of producing content, this friction is a particular type that
 only applies to production and not to consumption.</li>
  <li>It&rsquo;s slower, or feels slower. (According to <a href="http://www.asktog.com/TOI/toi06KeyboardVMouse1.html">Bruce
 Tognazzini</a>,
 Apple R&amp;D found that mousing is in fact faster than keyboarding.
 This strikes me as highly dubious today not only for all the reasons
 given above, but also because the article is from 1989 and the
 research is presumably from before that. I believe the research was
 also done on non-expert users.</li>
  <li>While the high degree of precision afforded by the mouse is a boon
 to applications that benefit from it, as the dominant mode of
 interaction, it forces that level of precision on every transaction.
 For every dialogue box with two simple choices, for example, the
 user still must traverse an arbitrary distance and hit a cursor
 target that is tiny relative to the rest of the screen, which is,
 for the transaction, completely unused. So to make an input that&rsquo;s
 essentially a binary 0 or 1, the user in real terms has to make a
 huge, well-calibrated, cognitively expensive analog input that&rsquo;s
 largely wasted.</li>
</ol>

<p>With OS X 10.7 Lion, Apple presents a clear and well-documented
iOSification of OS X.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> A prominent sign to me is the seeming
move toward deprecating the mouse, replacing and adding functionality
with touchpad gestures. I empathize with the desire to leave the mouse
behind. But where some of the iOS gestures are quite silly, the
decisions Apple has made about Lion&rsquo;s touchpad gestures are strange in a
different way. I&rsquo;ve discussed
<a href="http://blog.byjoemoon.com/post/3556631202/touch-ui-is-not-the-future-of-everything">before</a>
Apple&rsquo;s attempt to render the direct manipulation interface of iOS on
the fundamentally indirect interface of the desktop. But aside from
that, these gestures are no more efficient or discoverable than keyboard
shortcuts and they do make you move your hand off the keyboard. While
they are arguably a bit more intuitive and reminiscent<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> of a
real touch-based direct manipulation input system, it&rsquo;s at heart really
just a case of trading one set of undiscoverable metaphors for another.
At least with keyboard shortcuts, it&rsquo;s possible to expose functionality
because the keys have a conventional labeling system built in. Gestures
do not. I don&rsquo;t see any real way to reconcile the inherent and
fundamental difference between the two paradigms (direct and indirect)
with the <a href="http://procrastineering.blogspot.com/2011/07/myth-of-dying-mouse.html">ergonomic
constraints</a>
of different types of computing. Maybe Apple&rsquo;s solution will be to
further marginalize the needs of the technical user and simply move to a
touch-only interface, with touch-screen laptops as their physically
largest products. It&rsquo;ll be interesting to see.</p>

<h3 id="building-blocks">Building Blocks</h3>

<p>But if this does mark the beginning of the end of the pointing
device-driven model for consumer computing, it seems like a good time to
rethink the technical user&rsquo;s computing experience, unburdened by the
need to cater to the novice or non-technical user. While the CLI seems
to be regaining prominence and popularity among some subset of users, I
don&rsquo;t think a return to the command line is practical. The GUI has
introduced some concepts and workflows that are very powerful. As real
computers with dedicated input devices become more and more relegated to
technical users, maybe a more robust but less intuitive interface
becomes more viable. In fact, maybe we can abandon ‘intuitiveness&rsquo; as
the primary motivator or at least stop defining the word as minimal
interaction bandwidth.</p>

<p>Some starting points:</p>

<ol>
  <li>A purposefully high-bandwidth interaction model, i.e. a large but
 consistent set of initial undiscoverable actions to learn, that, at
 best, isn&rsquo;t meaningfully less ‘intuitive,&rsquo;<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> but instead
 simply has a learning curve with a different shape.<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup></li>
  <li>The mouse is too powerful to get rid of entirely. There are many
 applications for which it is an ideal or at least very appropriate
 input device, such as graphics rendering, photo manipulation, and
 first-person shooters. So, to minimize travel between mouse and
 keyboard, functions of the mouse and the mouse-hand side of the
 keyboard should overlap as much as possible. These probably include:
 motion and selection.</li>
  <li>I&rsquo;ll revisit <a href="http://10gui.com/">10/GUI</a> again, for some of the
 fantastic insights therein:
    <ul>
      <li>Single axis of windows (Con10uum).</li>
      <li>Different levels of interaction.</li>
    </ul>
  </li>
  <li>Direct manipulation interfaces suggest that cursors are unnecessary.
 In our thought experiment, they would probably exist only in certain
 applications, specifically mouse-based ones.</li>
  <li>We can draw inspiration from keyboard-only UIs.
    <ul>
      <li><a href="http://www.vim.org/">Vim</a> is a venerable text editor that I&rsquo;ve
recently converted to. Its interface is modal, and its main mode
of operation is through a command-based console.</li>
      <li>Windows&rsquo; keyboard access of menus, with visual indicators.</li>
    </ul>
  </li>
</ol>

<h3 id="fresh-start">Fresh Start</h3>

<p>My initial idea doesn&rsquo;t stray too far from traditional WIMP systems.
We&rsquo;ll retain the windows, icons, and menus, but relegate the pointing
device to only when necessary or appropriate. Let&rsquo;s start with windows:</p>

<p>I&rsquo;ll steal Con10uum&rsquo;s single axis for windows with no allowance for
vertical resizing, since I agree that the extra axis really only adds to
the clutter and complexity.</p>

<p>Without a cursor, we navigate by using four directional keys on the
keyboard, which I&rsquo;ll just call [left], [right], [up], and [down], to
select and highlight whole objects e.g. windows, more like navigating
console game menus:</p>

<p><img src="http://media.tumblr.com/tumblr_lqf42q0BC81qzvz3x.png" alt="image" /></p>

<p>In this single axis system, z-order is really the same as horizontal
order. So we&rsquo;ll use z-order in a different way, by adding nested
elements, and two more directional keys: [in] and [out].</p>

<p><img src="http://media.tumblr.com/tumblr_lqf47kWy801qzvz3x.png" alt="image" /></p>

<p>With any element selected in the UI, we can hit [menu] to invoke a
consistent menu of possible actions to take on the element, with a
corresponding key or sequence of keys for each action.</p>

<p><img src="http://media.tumblr.com/tumblr_lqf47yVuVp1qzvz3x.png" alt="image" /></p>

<p>This makes possible command sequences that can become unconscious,
relegated to muscle memory in the same way typing words can be. It makes
movement sequences equally unconscious, as any Vim user will tell you.
The benefits of making these into unconscious processes are many. Not
only are they potentially faster and less interruptive to cognitive
flow, but they can accumulate in a way that analog pointer-based UIs
cannot. Once you learn the sequence for a particular menu command and
use it enough to internalize it, you no longer have to think about it
and can then spend that cognition on learning a new sequence. Learning
to use the interface becomes an almost linguistic exercise.
Pointer-based UIs can never reach this level because every action, every
invocation of a menu or icon is a conscious act that demands much more
attention.</p>

<p>Vim gives us other directions to explore with a primarily
keyboard-driven interface. The basic principles of fewest number of key
presses and proximity to the home row keys seem like good ones. Also,
shortcuts to specific locations, e.g. the [os] key would take you
directly to the operating system level and the [application] key to the
application level. We can also take Vim&rsquo;s concept of iteration, so [5],
[left] would take you left five times. While Vim lives on the far end of
a brutal learning curve, it&rsquo;s a rich source of insight into how powerful
a user interface can be when discoverability and intuitiveness are cast
aside.</p>

<p>Starting with a high-bandwidth interaction model at the OS level has the
added benefit of leaving fewer UI decisions to applications. A user can
access more actions through a consistent menu/key sequence throughout
the system, obviating the need to learn new keyboard shortcuts and
interactions for each new application.</p>

<h3 id="this-is-just-a-rough-draft-of-an-idea">This is just a rough draft of an idea.</h3>

<p>It&rsquo;s an attempt to start thinking in ways unbound by the conventions
we&rsquo;ve been living with for decades. I imagine there are many different
directions one can take, starting from the new assumptions we are
allowed post-mouse.</p>

<p>The implications of a system like this are probably too big to patch
onto the systems we have now. There would be a keyboard-driven interface
analog to <a href="http://en.wikipedia.org/wiki/Fitts's_law">Fitts&rsquo;s Law</a> that
would have implications for application design as well as operating
system design. E.g. an application designer would always want to
minimize the number of key presses it would take to move from any
location to any other location, which would require a careful balance
between breadth and depth of element distribution.</p>

<p>Anything in the near future will probably have to look a lot more like
what we already have, and be filled with the compromises that we see in
Lion and early demonstrations of Windows 8. But recent advancements
provide us with a new context in which to fundamentally reconsider human
computer interaction. There&rsquo;s never been a time when it was more
possible to try something truly new, or to more significantly change the
landscape of future computing. My fear is that the trend toward simpler,
more tightly controlled, narrower user experiences optimized for passive
consumption will dominate. My hope is that we use the opportunity to
create user experiences that encourage more active consumption, more
substantial production, a generally richer, denser,
<a href="http://blog.byjoemoon.com/post/6542036868/project-depth">deeper</a> world.</p>

<p>Illustrations by Chris Klink.</p>

<h3 id="further-reading">Further Reading</h3>

<ol>
  <li>Christopher Mims. <a href="http://www.technologyreview.com/computing/38047/">&ldquo;Is the Desktop Having an Identity
 Crisis?&rdquo;</a>. <em>MIT
 Technology Review</em>. July 18, 2011.</li>
  <li>Wikipedia. <a href="http://en.wikipedia.org/wiki/History_of_the_graphical_user_interface">&ldquo;History of the
 GUI&rdquo;</a>.</li>
  <li>Jeremy Reimer. <a href="http://arstechnica.com/old/content/2005/05/gui.ars">&ldquo;A History of the
 GUI&rdquo;</a>.</li>
  <li>Brad A. Myers. <a href="http://www.cs.cmu.edu/~amulet/papers/uihistory.tr.html">&ldquo;A Brief History of Human Computer Interaction
 Technology&rdquo;</a>.
 <em>ACM interactions.</em> Vol. 5, no. 2, March, 1998. pp. 44–54.</li>
</ol>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Arthur C. Clark famously
<a href="http://en.wikipedia.org/wiki/Clarke's_three_laws">said</a>: &ldquo;Any
sufficiently advanced technology is indistinguishable from magic.&rdquo;
An abstract description of how this works is that a new technology
is initially simple, then accumulates additional functionality. As
its functionality grows, its operation becomes more complex. It then
requires a more expert user to take advantage of the added
functionality. But after a certain point, further advancements
address this complexity of operation by internalizing it. This makes
the technology simple to use for lay users, but in an opaque,
‘magical&rsquo; way. <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>John Siracusa&rsquo;s <a href="http://arstechnica.com/apple/reviews/2011/07/mac-os-x-10-7.ars">review of
Lion</a>
is probably the best starting point, and <a href="http://www.macobserver.com/tmo/article/the_future_of_the_mac_after_lion/">this Mac Observer
article</a>
discusses this as well. <a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>The mapping between iOS&rsquo;s direct manipulation and Lion&rsquo;s indirect
manipulation via touchpad is weird. Nothing in iOS maps to moving
the cursor around by dragging your finger across the touchpad.
Scrolling with one finger in iOS, maps to dragging with two fingers
in Lion. Touching in iOS maps to pressing down harder on the part of
the touchpad that you&rsquo;re already touching in Lion. And that&rsquo;s not
even getting into the crazy gestures. I think this actually produces
a UI &ldquo;<a href="http://en.wikipedia.org/wiki/Uncanny_valley">uncanny valley</a>&rdquo;
effect. <a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>If you think back to your first experience with a mouse, or watching
someone&rsquo;s first experience, you might agree that the mouse actually
isn&rsquo;t particularly intuitive so much as it is familiar.<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>This observation is probably more broadly applicable. For example,
languages with simpler grammars are probably easier to learn, but
less powerful in terms of conveying a lot of subtle information in
the shortest amount of time. I don&rsquo;t have the knowledge or resources
to explore this properly, though.<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Google, Motorola, and Patents</title>
			<link>http://blog.byjoemoon.com/google-motorola-and-patents</link>
			<pubDate>Tue, 16 Aug 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/google-motorola-and-patents</guid>
			<description><html><p><a href="http://googleblog.blogspot.com/2011/08/supercharging-android-google-to-acquire.html">Google is buying Motorola Mobility for $12.5
billion.</a>
It&rsquo;s Google&rsquo;s largest acquisition ever. And it&rsquo;s clearly about patents.
But before I go into the issue of patents more broadly, let&rsquo;s take a
quick look at what Google is buying exactly.</p>

<p>Motorola Mobility (MMI), which was the handset and cable box division of
Motorola, spun off <a href="http://www.computerworld.com/s/article/9203142/Motorola_Mobility_completes_spinoff_from_parent_company_">early this
year</a>.
Motorola obviously has a long history in the mobile industry (they own
the now expired patent on the cell phone) and have a deep and presumably
strong patent portfolio. This includes about 14,300 patents, and 6,700
patents pending. According to
<a href="http://seekingalpha.com/article/278932-can-motorola-mobility-be-sustainably-profitable">this</a>,
in July they also had $5.5B in cash and deferred tax assets.</p>

<p>So, without going into the issue of patent quality, we can broadly
compare this deal in terms of patent acquisition with the auction of the
Nortel patent portfolio that Google
<a href="http://www.bgr.com/2011/07/01/apple-rim-others-win-nortal-patents-at-auction/">lost</a>
in July to a consortium that includes Microsoft, RIM, and Apple.</p>

<p>Nortel: $4.5B / 6,000 patents = $750k per patent.</p>

<p>MMI: ($12.5B − $5.5B) / 14,300 patents = $489,510.49 per patent.</p>

<p>Plus 6,700 patents pending.</p>

<p>Plus a hardware company.</p>

<p>Plus, as Nilay Patel
<a href="https://twitter.com/#!/reckless/status/103263115268988928">notes</a>, a
very strong patent creation engine.</p>

<p>So a cursory examination suggests it&rsquo;s a pretty good deal for Google.</p>

<h3 id="motorola">Motorola</h3>

<p>From MMI&rsquo;s perspective, it&rsquo;s hard to see this as anything less than a
huge win. Despite its success with its Droid handsets, MMI hasn&rsquo;t been
profitable and has in fact been looking for a buyer since at least last
year. They clearly saw Google&rsquo;s (specifically Android&rsquo;s) recent patent
problems as an opportunity and seized it. MMI CEO Sanjay Jha&rsquo;s recent
<a href="http://www.unwiredview.com/2011/08/11/motorolas-sanjay-jha-openly-admits-they-plan-to-collect-ip-royalties-from-other-android-makers/">public
statements</a>
about suing Android manufacturers over IP were probably part of the
negotiations.</p>

<p>The recent escalation of the tech patent war was quite a boon for MMI.
<a href="http://arstechnica.com/tech-policy/news/2011/08/what-google-lostand-gainedby-not-buying-moto-in-2010.ars">Ars Technica
estimates</a>
that Google could have bought MMI last year for $6B less.</p>

<h3 id="android">Android</h3>

<p>What does this mean for Android?<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> Google has somewhat
<a href="http://googleblog.blogspot.com/2011/08/supercharging-android-google-to-acquire.html">contradictory
things</a>
to say about the acquisition.</p>

<blockquote>
  <p>&ldquo;Together, we will create amazing user experiences…&rdquo;</p>
</blockquote>

<p>And:</p>

<blockquote>
  <p>&ldquo;We will run Motorola as a separate business.&rdquo;</p>
</blockquote>

<p>While maybe not directly self-contradictory, the announcement of the
deal exposes the inherent cognitive dissonance in being a company that
both makes hardware and licenses its software to other hardware makers.
There are directly conflicting incentives set up now, which can&rsquo;t help
but cause friction.</p>

<p>One rising sentiment is that this is good because now that Google owns
the software and hardware, they can make a vertically integrated product
of much higher quality, able to compete more directly with Apple. But
it&rsquo;s not that simple. Apple products aren&rsquo;t good just because they&rsquo;re
vertically integrated. And Google has no experience making vertical
integrated products.</p>

<p>It&rsquo;s also misaligned with Google&rsquo;s ultimate
<a href="http://blog.byjoemoon.com/post/166900257/why-i-trust-google">incentive</a>
of commoditizing everything that comes between a user and the Web.</p>

<p>For the other Android manufacturers, HTC and Samsung being the most
relevant, it&rsquo;s a bit complicated.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> On the one hand, it must
certainly worry them for Google to be joining the hardware business. On
the other, the growing litigiousness of the industry was starting to
make Android licenses more costly both in terms of IP royalties and the
litigation itself. So Google joining the patent fight on the side of
Android may be a welcome development. Seems like a net wash to me. And
it&rsquo;s not like there are many alternatives to Android, anyway. Windows
Phone 7 is faltering in the market, while Android has become the leader
in terms of market share.</p>

<h3 id="patents">Patents</h3>

<p>Ultimately, Google is only in this position because of patents. Just in
the last few weeks, about $17B have changed hands over tech patents.
That&rsquo;s a lot of billion dollars.</p>

<p>That&rsquo;s money that could have gone into research and development. Not
that all of that money just disappeared, but there must be a significant
transactional cost. It&rsquo;s also that much more money that&rsquo;s now in some
sense ‘invested&rsquo; in the patent system. If the patent system were to be
abolished immediately, all that would immediately become a wasted
investment. As the industry players continue to pour money into patents,
the system only becomes more entrenched.</p>

<p>Which puts Google in an especially crappy position. Google believes
patents are a tax on innovation. And Google wants to encourage
innovation, if only because that further eliminates barriers between
users and the Web. But in the current environment, Google <em>can&rsquo;t not</em>
play the game and hope to get anywhere. So it has to work both sides. It
has to acquire patents to defend against patent litigation, while at the
same time
<a href="http://googleblog.blogspot.com/2011/08/when-patents-attack-android.html">fighting</a>
on the PR front and lobbying in Washington for patent reform.</p>

<p>Not only that, but Google handicaps itself by only using patents
defensively. To my knowledge, Google has never used patents offensively.
(Yet.) You can interpret that as self-interest or as admirable
ideological consistency. I see it as both.</p>

<p>And further, the mechanics of the patent system favor large companies
with large patent portfolios, because it both gives them the ability to
sue new upstart companies for patent infringement and protects them from
small companies with ‘legitimate&rsquo; patent suits. Incumbents <em>love</em> the
patent system. Since Google&rsquo;s business model disproportionately benefits
from universally increased innovation, it alone among big, wealthy,
influential companies has any incentive to fight for patent reform.</p>

<p>It&rsquo;s Google versus the World on two fronts.</p>

<h3 id="world-war-g">World War G</h3>

<p>On the surface it seems like a good deal for Google. But I think it&rsquo;s a
sign of bad things coming for Google, for innovation, and for the world.
I think what people lose sight of is that some (breathtaking, well
crafted, seamlessly designed) innovations change the world for rich
people (that&rsquo;s you and me). Others simply
<a href="http://www.technologyreview.com/communications/37877/?a=f">change</a> the
<a href="http://singularityhub.com/2011/08/16/80-android-phone-sells-like-hotcakes-in-kenya-the-world-next/">world</a>.</p>

<p>A class of people this is great for? Lawyers.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>In the short term, this is probably good for Android users. As a
Motorola Droid X owner, I&rsquo;ve had to go to great lengths to remove
Motorola&rsquo;s frankly terrible customizations of Android. It&rsquo;s easy to
imagine that Google will nix all that and maybe even use the
acquisition to push the entire industry into a less fragmented
ecosystem. <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><a href="http://www.google.com/press/motorola/quotes/">Their public
statements</a> are
basically meaningless PR speak, which journalists seem to be
interpreting according to their respective prejudices. I don&rsquo;t think
there&rsquo;s any information you can really get out of them. <a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Google's Existential Crisis</title>
			<link>http://blog.byjoemoon.com/googles-existential-crisis</link>
			<pubDate>Wed, 13 Jul 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/googles-existential-crisis</guid>
			<description><html><p>Google&rsquo;s success was founded on the execution<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> of two
innovations: 1. Leveraging the hyperlink to create the best index (i.e.
crowd-sourcing); and 2. Targeting advertising to search results to
monetize that index. Apparent diversification into new areas like
content (YouTube) and operating systems (Android and Chrome OS) can be
understood to exist in service to the two founding innovations. These
driving principles are Google&rsquo;s DNA, and as successful as the company
has been, if there are fundamental problems with these two principles,
it could mean trouble for the company, and trouble for the web at large,
to the extent that Google is good for it.</p>

<h3 id="the-algorithm">The Algorithm</h3>

<p>The search engine&rsquo;s ranking algorithm, PageRank, measures &lsquo;relevancy.&rsquo;
It began with the simple insight that the more links there are to a
particular page, the more important it is, but the algorithm very
quickly became increasingly sophisticated, and it now considers <a href="http://en.wikipedia.org/wiki/PageRank#Description">&ldquo;more
than 500 million variables and 2 billion
terms.&rdquo;</a></p>

<p>Google&rsquo;s relationship with the web is symbiotic. The web became useable
in no small part because Google made it possible to find relevant
things. As the web became more important, so did Google. And as Google
became more important, it also became a bigger target. Search engine
optimization became a prominent and diverse industry that ranges from
qualified professionals exhorting legitimate and sanctioned best
practices to cargo cultists offering opaque rituals to increase search
result rankings. Orthogonally, companies like Demand Media began to mine
real-time search data and hire cheap labor to create abundant shallow
content, exploiting holes in the web to make money on ads. Recently, it
even
<a href="http://searchengineland.com/demand-medias-ipo-the-google-seo-aspects-48286">seemed</a>
like such &lsquo;content farms&rsquo; could rise to become an existential threat to
Google by degrading search results into meaninglessness.</p>

<p>This shows Google&rsquo;s symbiosis with the web to be profoundly weird in a
Heisenbergian way: Google cannot observe the web without changing it.
Search engine optimizers and content farms both respond to Google&rsquo;s
algorithm. SEO changes the way publishers create and present data, as
well as what content they produce. Content farms form a more directly
parasitic relationship, because as search traffic data exposes demand
for content, they produce it. But as Google&rsquo;s relevancy algorithm
increases the measure for quality, the quality of content changes to fit
those criteria. This puts Google in the strange position of being able
to invoke content by defining what is of high quality. Which presents a
problem analogous to the one of standardized testing: it shifts
incentives to &lsquo;teach to the test&rsquo; instead of to teach what&rsquo;s important.
The simple solution is that if you can make a test good enough, the
incentives will align properly.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>As the test-maker, Google becomes in no small way the arbiter of truth.
The tension that this forms between Google and the content farm industry
describes a sort of Turing arms race: the growing ability of content
farms to cheaply produce content in an increasingly automatic fashion
pushes against the escalating precision of Google&rsquo;s quality algorithm.
The inevitable conclusion of this process is automatically generated,
demand-invoked content that&rsquo;s indistinguishable or better than anything
people can provide.</p>

<p>Not only that, but in <a href="http://searchengineland.com/google-now-personalizes-everyones-search-results-31195">December
2009</a>,
Google began to personalize search results according to users&rsquo;
locations, search histories, social graphs, etc. As the search profile
becomes more extensive, the personal search algorithm becomes more
intensely personal, and an &lsquo;objective,&rsquo; One True Search ceases to exist.</p>

<p>As I&rsquo;ve discussed
<a href="http://blog.byjoemoon.com/post/166900257/why-i-trust-google">before</a>,
Google&rsquo;s incentives are such that it wants to increase use of the web,
broadly. This is good, as long as the web is good, however you define
that. But as its power to create the web (or at least influence it)
waxes, so does its responsibility. It&rsquo;s not hard to imagine a dystopian
future where the entire web devolves into a broad, shallowly addictive
ad-ridden hell (this already describes some sectors of the web). Google
would have to have an extremely long-term outlook to decide to serve as
superego instead of as id to the web. But <a href="http://searchengineland.com/video-inside-googles-self-driving-cars-66806">maybe that&rsquo;s not so hard to
imagine</a>
either.</p>

<h3 id="ads">Ads</h3>

<p>Google famously makes the vast majority of its money on advertising,
specifically with its AdWords program, which auctions keywords to
advertisers. The key to the program&rsquo;s success is its highly targeted
nature, i.e. the relevance of the advertisements.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> The weird
tension here is that the quality of the search results is in conflict
with the quality of search ads. If the search results are perfect, and
exactly what the user wants every time, then there&rsquo;s no need for her to
ever bother with advertisements. Conversely, if the product being
advertised is maximally relevant, then it should be her first search
result anyway. Advertisements only offer value to a user if their
relevance has parity with search&rsquo;s. But they only offer value to an
advertiser if they are exposed where they wouldn&rsquo;t be organically, i.e.
where they are irrelevant, or distracting.</p>

<p>The other tension is one that stems from something I briefly touched on
in <a href="http://blog.byjoemoon.com/post/6542036868/project-depth">another
post</a>: Google
wants to encourage quality in the web and provide a good user experience
so that people continue to use the web and its services. Simultaneously,
Google wants to extract the most value from the user by exposing her to
the most ads, i.e. distracting her. Minimally distracting ads would be
nonexistent ones and maximally useful services would be ad-free.
Conversely, maximally value-extracting services would be all ads all the
time. The two incentives of quality and distraction are directly
opposed. The result of this cognitive dissonance is what we see when we
use Google products: generally tasteful, useful, and understated ads.</p>

<p>So when Google places relevant ads, advertisers are in some sense paying
Google simply to do its job. But when Google places irrelevant ads, it
undermines the quality of the web, its search results, and its user
experience. And maybe the scariest part of this to me is that the
quality incentive is one conceptual remove farther than the distraction
one.</p>

<p>Maybe it&rsquo;s no more perverse than any company&rsquo;s tension between making
the highest quality product in order to invoke desire and spending the
least amount of money possible in its production. And maybe Google is
culturally constituted to thinking in uniquely long-term ways that
ultimately make everyone better off. The recent battle with SEO-driven
content farms that resulted in the <a href="http://searchengineland.com/why-google-panda-is-more-a-ranking-factor-than-algorithm-update-82564">&lsquo;Panda&rsquo;
update</a>
to the search algorithm would seem to indicate so.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> But I
imagine it&rsquo;s easy to think long-term when you&rsquo;re succeeding. We&rsquo;ll see
what happens when the success inevitably slows.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Google&rsquo;s ranking algorithm was based on earlier ideas: <a href="http://en.wikipedia.org/wiki/Citation_analysis">citation
analysis</a> and
<a href="http://en.wikipedia.org/wiki/HITS_algorithm">Hyperlink-Induced Topic Search
(HITS)</a>.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>I&rsquo;m not competent to speak to whether that&rsquo;s possible in either
case. <a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>The auction part, which helps Google maximize its profits, is also
really important, but I would say secondarily so.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>More cynically, you could call it a response to growing criticism
about search quality. <a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>A New Metaphor for Social Networking</title>
			<link>http://blog.byjoemoon.com/a-new-metaphor-for-social-networking</link>
			<pubDate>Wed, 29 Jun 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/a-new-metaphor-for-social-networking</guid>
			<description><html><p>Yesterday, Google launched <a href="https://plus.google.com/">Google+</a>, which,
according to <a href="http://www.wired.com/epicenter/2011/06/inside-google-plus-social/all/1">Steven Levy&rsquo;s Wired
article</a>,
is less of a product and more of a new direction for the search company,
a redefinition of its approach to its core mission of &ldquo;organizing the
world&rsquo;s information.&rdquo; Treating a social networking product as an
extension of this mission is simultaneously reassuring and slightly
creepy. Creepy because of the way Google constitutionally seems to treat
people as just another category of information to be indexed, and, well,
reassuring for the same reason, because it seems like a way for Google
to finally get it right.</p>

<p>There don&rsquo;t appear to be any truly new features included in Google+.
Instead, there&rsquo;s a clean and uncharacteristically whimsical user
interface to a core set of features that many social networks have done
before: sharing, photos, group chat, group video chat, etc. One word
that&rsquo;s conspicuously missing from the announcement, though: &ldquo;open.&rdquo; No
mention of open source, APIs, protocols, federation, or
interoperability. Which is sad, but perhaps understandable after the
abject failures of Wave and Buzz.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>The feature I&rsquo;m most conflicted about is &ldquo;Circles.&rdquo; Circles is Google+&rsquo;s
take on friend/list management, and works a lot like Facebook&rsquo;s
&ldquo;Groups.&rdquo; Where Facebook&rsquo;s feature is a tacked-on afterthought, though,
reportedly ranging from barely useable to horribly broken, Circles, as a
tightly integrated core feature, looks a lot friendlier, faster, and
more fun to use. What&rsquo;s conflicting about it is that lists, while the
most obvious and seemingly flexible solution to the problem of selective
interaction, are actually quite limited and deceptively complex.</p>

<h3 id="two-problems-ambiguity-and-asymmetry">Two Problems: Ambiguity and Asymmetry</h3>

<p>It seems straightforward enough. Jill has different lists. When she
shares something on one list, everyone on that list can see the item.
Simple. It quickly gets more complicated, though. For example, what if
Jack wants to comment on that item? Who can see the comment? In Google
Reader, anyone who was in the original list can see the comment. Which
is potentially confusing for Jack, who has no idea who can see his
comment, and for Jim, who can suddenly see a comment from someone he
doesn&rsquo;t know on an item in his feed. This is normal on the open web,
where there is an explicit expectation of total publicity. But in a
social networking context of commenting on a friend&rsquo;s item, it can
easily feel like overexposure. An alternative way to do it might be for
only Jill to be able to see comments on her shared items, but this would
simply resolve into many one-on-one conversations between Jill and
anyone who comments on her item. A third way to handle this would be to
only expose Jack&rsquo;s comments to people in Jack&rsquo;s friend list. But which
of Jack&rsquo;s lists? And should it be only the overlap between Jack&rsquo;s list
and the original list Jill shared it to?</p>

<p>Straightforward lists start out simple, but once you make them
bi-directional, quickly gain an order of complexity. And not only are
the individual ways to do it somewhat hard to define and explicate, but
so is even the question of which of these is in use. The way Facebook
Groups and Google+ Circles address the issue is that everyone involved
can check to see who is on the list of people included in the share of
the particular item. Which may seem like it solves the problem, and
perhaps does to some degree, but it fails to address what the issue
really is, which is social context. An example: Jack is friends with
Jill both at work and outside of work. Jack shares something to his
&ldquo;Work&rdquo; Circle. Jill sees the item, but mistakenly thinking she&rsquo;s in a
non-work context, makes a crude comment. While this system does give you
access to the current social context, it doesn&rsquo;t make it immediately
obvious. I.e. Jill could conceivably check to see who will be able to
see her comment each time she makes a comment, but the cognitive load
and general uncertainty will push Jill to undershare, rather than have
to manually check the social context each time she makes a comment.</p>

<p>It also does not address the problem of asymmetry between my circles and
my friends&rsquo; circles. Let&rsquo;s say Jill logs onto Google+, clicks on her
&ldquo;Friends&rdquo; Circle, and begins to peruse her friends&rsquo; posts there. When
she clicks on one of Jack&rsquo;s shared items to comment on it, she might
have the expectation that her comment is going out to the Circle that
she&rsquo;s in at the moment, which is different from the Circle that Jack
posted to in the first place. This expectation is reinforced by the fact
that in her user interface, she&rsquo;s still in her Friends Circle. But her
comment will actually be going to Jack&rsquo;s, of course. Which is
potentially pretty confusing. The fundamental problem here being output
lists are different from input lists, but they&rsquo;re called the same thing,
and the conceptual difference isn&rsquo;t easy to keep track of.</p>

<p>Ultimately, the problem is that the way information flows in any of
these situations is non-obvious, complex, hard to model coherently in a
simple user interface, and there&rsquo;s no metaphor that you can map the
interaction onto.</p>

<h3 id="places">&ldquo;Places&rdquo;</h3>

<p>Here&rsquo;s a better idea than Groups or Circles: Instead of or in addition
to lists, imagine a Place that represents a semi-permanent grouping
that&rsquo;s easy to create and archive, centered around anything from topic
to group to actual physical location or event. A Place could be public
or private<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> (meaning no one outside of the Place could see
what happens there), and could be closed to or open for anyone to
join.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> The metaphor solves both problems of ambiguity and
asymmetry, by transparently conflating the input and output lists.</p>

<p>The primary way to use this sort of system would be to visit each Place
serially as separate feeds, such that the user would have a clear idea
of &ldquo;where&rdquo; she is as well as in what social context.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></p>

<p>So when Jill clicks on her &ldquo;Work&rdquo; Place, she always knows more or less
exactly who&rsquo;s there and, maybe more importantly, she understands the
social context. Likewise, Jack also always knows this, and when Jack
comments on Jill&rsquo;s post in Work, he goes to the same place in his UI
(Work) that Jill went in hers to make the original post, which
eliminates the confusion about how the information flows. It&rsquo;s also the
same list of people for both of them (and everyone else in the Place),
so there&rsquo;s no confusion about who&rsquo;s going to see the post or the
comment; it&rsquo;s always obvious.</p>

<p>Not that it would always be obvious. If Jill is a part of Jack&rsquo;s &ldquo;Bar
People&rdquo; Place, but doesn&rsquo;t really know everyone else there, she&rsquo;ll have
to check to see who&rsquo;s on the list. But importantly, she won&rsquo;t
necessarily have to in order to understand immediately what the social
context is, and to have a general idea of how familiar she is with the
Place.</p>

<p>Note that I am not proposing anything functionally very different from
the list systems that Facebook and Google+ already have in place. Places
are just slightly more restrictive and present the information in a
different way. They give the user a mental model that makes it much
easier to immediately and intuitively understand how her information
flows, who or at least what kind of people are involved in the
conversation, and what sort of tone would be appropriate.</p>

<p>Places also add conceptual use-cases, like event-centered streams.
Imagine Jill creates a Place called &ldquo;Birthday Party&rdquo; and invites
everyone who will be there. Then all the planning, conversation, and
photography can take place there, automatically aggregating all the
relevant data and simultaneously creating a record of the event in an
organic way. This could easily scale to even very large events,
including conferences, classes, and concerts. It&rsquo;s hashtag 2.0.</p>

<p>While I had originally been thinking about this concept in the context
of open, interoperable social networks and self-hosted identities and
social graphs, where current social networks would be what I call Places
(Twitter for shouting pithy aphorisms, LinkedIn for &lsquo;professional
networking,&rsquo; Flickr for sharing photos, Facebook for&hellip; I&rsquo;m not sure), I
thought it could be illuminating to see how well it would apply inside
of a social network. Pretty well, it would seem.</p>

<p>But, really, another (albeit prettier) data silo doesn&rsquo;t do much for me.
The fact that it&rsquo;s from Google makes me immediately interested, but I&rsquo;m
disappointed that it&rsquo;s not a more open initiative. I&rsquo;m still hopeful
that that part is in the works.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Though I really wouldn&rsquo;t attribute the failures of those products to
their &lsquo;open&rsquo; nature. <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>It seems important to me to give clear indication of the privacy
setting in each Place in the UI, perhaps by changing the color
scheme: e.g. dark background with light text for a private Place,
vice versa for a public one. <a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>I leave up to the reader to think about who sets the privacy
setting, whether and how the privacy setting can change, who can
invite people, who can control who can invite people, who can have
write access as well as read access, and who controls that, and the
different mechanisms that would have to be involved for each of
these decisions. I&rsquo;d like to point out here that Google+ seems to
subscribe to some of the basic content relationships I wrote about
<a href="http://blog.byjoemoon.com/post/582452757/webs-and-streams">here</a>.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>It&rsquo;s not that hard to imagine an alternate view that collapses all
the different Place streams together, but gives a clear indication
of which Place each individual post belongs to.<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Project: Depth</title>
			<link>http://blog.byjoemoon.com/project-depth</link>
			<pubDate>Tue, 14 Jun 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/project-depth</guid>
			<description><html><p>I don&rsquo;t particularly enjoy traditional social settings like bars or
parties. Partly this is because I can&rsquo;t hear very well, especially when
there&rsquo;s a lot of background noise. But probably a more important reason
is that I feel like a lot of the conversation that happens in these
settings isn&rsquo;t productive. Not productive in the sense of making me
money or advancing civilization, but in the sense of making us
understand each other better on a more than superficial level, or in the
sense of enlightening us or engaging us to some significant depth. From
my perspective, most of the idle chatter that happens in these settings
are basically overhead. Not all of it. And I understand that some people
prefer superficial conversation, so it&rsquo;s not overhead for them. Some
people enjoy eating bread by itself; to me, bread is for holding meat.</p>

<p>I prefer and seek depth in my endeavors. I&rsquo;m starting to think it&rsquo;s my
overarching project. I think it&rsquo;s what all my various little projects
have been about: Breakfast, the various incarnations of Since the War,
the party chat, NMBC, EMDN, and now EMSN.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> It&rsquo;s also what
motivates my thoughts on social networks, browsers, etc, as well as on
user interfaces and the rationality engine.</p>

<p>Depth is my project.</p>

<p>And since this blog is basically a brain-dump, and that&rsquo;s what I spend
most of my idle cycles on, I guess that&rsquo;s mostly what this blog&rsquo;s about.</p>

<p>There are two separate vectors of depth that I think about:</p>

<ol>
  <li>
    <p>Depth of experience. I refer to the immersive experience, in which
 you are so engrossed in your artifact that when you are interrupted,
 it&rsquo;s as disorienting as waking up in an unfamiliar place from a
 vivid dream. This experience can come in many forms. For me, it&rsquo;s
 usually reading (novels, essays, and long form journalism), but
 often films and sometimes video games
 (<a href="http://en.wikipedia.org/wiki/Ico">Ico</a> comes to mind).</p>
  </li>
  <li>
    <p>Depth of connection. Conversations can be engrossing, too. They can
 also enlighten (sometimes mutually), expand (in the transitive sense
 of the word), and increase intimacy. This is true both off- and
 online. And online, the internet equivalent of idle chatter is
 shallow interaction&ndash;snarky comments, likes, pithy aphorisms, how-to
 articles, etc. (This is not to say that shallow interaction has no
 value. It does<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>, but again, I like meat.)</p>
  </li>
</ol>

<h3 id="there-are-oceans-to-explore-but-we-stay-in-puddles">There are oceans to explore, but we stay in puddles.</h3>

<p>The problem of the proliferation of shallow communication is more
fundamental than the implementation of currently popular social
networks. I&rsquo;m starting to see it as three parts:</p>

<ol>
  <li>
    <p>Human nature. People have short attention spans and are easily
 distracted. They are also incredibly susceptible to Skinnerian
 novelty-seeking behavior. (Also, many or most people genuinely
 prefer shallow breadth, or even just shallows.)</p>
  </li>
  <li>
    <p>The architecture of the web. The web is made of hyperlinks. Any
 given page generally points to many other pages. If you imagine that
 the goal of reading a web page is to read it all the way through
 from start to finish, the page itself fights you by giving you many
 orthogonal avenues along the way.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p>
  </li>
  <li>
    <p>Currently viable online business models. Web pages make money by
 distracting you, either by getting you to look at an ad or by
 getting you to click on one. As long as advertising drives the web,
 and advertisers measure success in page views, this isn&rsquo;t going to
 change.</p>
  </li>
</ol>

<h3 id="look-forward-not-backward-upward-not-forward">Look forward, not backward; upward, not forward.</h3>

<p>There are huge, Lovecraftian commercial forces at work, with a vested
interest in keeping our attention spans short, and our feedback loops
shorter. These forces feed on &lsquo;eyeballs&rsquo; and &lsquo;clicks&rsquo; and measure us in
aggregate. It seems unlikely that we can change that.</p>

<p>But there are countervailing forces. Some commercial ones, like
<a href="http://flickr.com">Flickr</a> and <a href="http://stackexchange.com/">Stack
Exchange</a>,<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> harness the power of
communities to dig deep mines of richness on the web. Others are
non-commercial, like Wikipedia. Many come from people scratching their
own itches: <a href="https://www.readability.com/">Readability</a> and
<a href="http://www.instapaper.com">Instapaper</a>, made by and for people who
wanted to be able to read web writing without the now conventional
distractions of modern web design; any number of anti-procrastination
applications that block your internet connection for specified periods
of time to improve task focus; and services like
<a href="http://longform.org/">longform.org</a> and
<a href="http://delivereads.com/">delivereads</a> that encourage deep reading of
long form journalism. Physical devices that enable depth are becoming
wildly popular: the Kindle is an obvious example, as is the iPad, which,
despite <a href="http://blog.byjoemoon.com/post/5442955515/holding-the-web-in-your-hands">my
misgivings</a>,
enables an unprecedented immersion experience by presenting an intuitive
and emotionally satisfying interaction model. And of course there are
<a href="http://cognitivesocialweb.com/home/2011/6/7/on-the-purpose-and-the-engine-of-the-web.html">others</a>
thinking about the same problems.</p>

<p>We are all naturally insular to some degree. We are all anxious,
novelty-seeking apes. We are buoyant, in the working metaphor. And there
are two ways to dive deeper: learn to hold our breath longer; and make
tools to so we don&rsquo;t have to. Can&rsquo;t hurt to try both.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>I&rsquo;m not trying to take all the credit for all of these, just
explicating my underlying reason for participating.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>I find the concept of &ldquo;ambient intimacy&rdquo; to be an interesting one.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>There&rsquo;s a lot more here, like the fact that the internet is
basically a custom tailored novelty aggregator and interruption
machine, but others have covered this exhaustively, I think.
Probably the canonical example is Nicholas Carr&rsquo;s <a href="http://www.amazon.com/Shallows-What-Internet-Doing-Brains/dp/0393072223"><em>The Shallows:
What the Internet is Doing to Our
Brains</em></a>.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Probably not deep on either of the vectors I&rsquo;ve enumerated, but
Stack Exchange was built as a direct response to the shallow
cesspool of technical knowledge that was available on the internet.
It maybe represents a third vector: depth of information?<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>The Personal Cloud</title>
			<link>http://blog.byjoemoon.com/the-personal-cloud</link>
			<pubDate>Mon, 06 Jun 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/the-personal-cloud</guid>
			<description><html><p>Things seem inexorably trending toward cloud models of
computing.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> Which is great, right? You can have access to your
data, apps, accounts, and settings from anywhere. Somebody else does all
the administration, including security updates, backups, etc. And it all
only gets better as the internet does, i.e. as connections get more
ubiquitous and more ubiquitously faster.</p>

<p>But if these things are cyclical, what does it look like when the
pendulum swings back? Certainly there are reasons to want it to. There
are the obvious privacy implications, which I don&rsquo;t actually care that
much about. But there are also related legal issues, like law
enforcement agencies getting access to your data through the cloud
hosting services you use. It also creates a single point of failure for
your entire web life: your login. There are numerous horror stories of
people&rsquo;s accounts being hacked or revoked, featuring Kafkaesque
experiences of trying to prove account ownership, or even just trying to
find someone to prove ownership <em>to</em>. All of these basically boil down
to: the centralization of information, and the relinquishment of control
to the centralized authorities.</p>

<h3 id="carry-capacity">Carry Capacity</h3>

<p>I spent approximately 75% of my enlistment in the Marine Corps
complaining. A lot of that time I was complaining about how much gear we
had to carry. It was a ridiculous amount (I would say to anyone within
earshot) just barely not enough to completely immobilize us. With all of
modern technology at our disposal, why was it that we had to carry so
much weight?</p>

<p>What I didn&rsquo;t realize at the time (Nick pointed out to me later) was
that the amount of weight that soldiers have carried has stayed pretty
much constant since soldiering first became a thing. The range and
capabilities of an individual soldier has increased as technology made
gear lighter, but the fundamental point is that you still want to pack a
given soldier with as much stuff as he can carry.</p>

<p>Consumer computer hardware has historically worked in a similar way.
People have always spent generally the same amount of money on
computers, and as computers got faster and cheaper, we bought better
computers with the same amount of money, because they were never really
fast enough. But at some point, they sort of did get fast enough and we
stopped spending as much as we possibly could on computers. It was as if
soldiering technology got so good that a soldier&rsquo;s range and
capabilities were unlimited. You didn&rsquo;t have to load him up to capacity
any more.</p>

<p>So we started spending some of that money on mobile devices, which have
different constraints like size and power consumption, that make the
calculus of capacity limits very different. But the general principle
remains, and at some point in the near future, mobile devices are going
to be fast enough that we won&rsquo;t have to spend the maximum available
resources for acceptable performance, especially as cloud computing
becomes dominant and presents different bottlenecks (mostly wireless
bandwidth).</p>

<h3 id="eating-and-having-cake">Eating and Having Cake</h3>

<p>If the pendulum is going to swing back toward decentralization, then,
the imminent computing paradigm has to offer the best of personal
computing and cloud computing: the personal cloud. The term is kind of
nebulous right now, with some vendors using it to refer to their
web-accessible network storage devices<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>, and others
confusingly using it to refer to personal use of cloud services. What I
have in mind might be better described as the self-hosted cloud.</p>

<p>Under personal computing, the canonical data lives on the client device
(because there basically was no other place for it). Under cloud
computing, the canonical data lives on the hosting service&rsquo;s servers.
Your Gmail data, for example, lives on Google&rsquo;s servers, and it gets
synced to your browser or your mail client. When you take an action like
reading or sending mail, your client sends that information to the
server, the server takes the action and updates itself, then sends the
new canonical data to all of the clients. Under the personal cloud,
canonical data will live on a server you own, that sits in your home or
office. To access the data, you simply go to your home server&rsquo;s url from
anywhere with web access and log in. All the convenience, full control.</p>

<p>More interestingly, you&rsquo;ll be able to install applications to your
server, ideally with one click from the web. A webmail service, for
example, that you have complete control over and access to from
anywhere, including APIs for access from client devices and third-party
services. Calendars, notes, photos, music, movies, etc. with granular
control over access. Publishing platforms are basically already there.
Office productivity apps are too, more or less.</p>

<p>The operating system and applications could be silently and
automatically updated, like Chrome/Chrome OS. I would imagine that home
servers would be modular enough that to add capacity, you could simply
buy more, faster, larger-capacity boxes and add them to the network. You
might still use proper cloud services, but probably strictly for
encrypted backups of data that the hosts have no access to. No more
privacy/legal concerns. No more account revocation/customer service
nightmare.</p>

<p>The implications of widespread adoption of this model are interesting to
think about. I&rsquo;m not sure how the trade-off math would work for simple
web apps. On one hand, the infrastructure needs are reduced to
distribution of the application; no more user data, server
administration, or even much user account administration. On the other,
maybe more end user support? More profoundly, this would make
decentralized, interoperable social networks much more viable. You&rsquo;d
have all of your canonical social data on your own server, with full
ownership of your social graph, and granular control over who gets
access to what. It would also make spinning up and spinning down of
private or transient social networks much more viable, since everyone
has a server.</p>

<h3 id="apple-or-google-neither">Apple or Google? Neither.</h3>

<p>There are already approximations of this model in existence. Some people
VNC or ssh into remote machines to work using their device effectively
as a thin client. But the user experience would obviously have to be a
lot easier and more seamless for this model to be at all viable. It
would have to be at least as easily deployable as a regular Windows PC
is today, and I think the necessary client and server technology is only
now becoming fast/cheap enough for this to be really productized for the
consumer.</p>

<p>Apple does seem to be gesturing in this direction with <a href="http://thisismynext.com/2011/06/06/mac-os-lion-server-runs-50-july-adds-ipad-file-sharing-ios-push-notification-support/">their
release</a>
of Mac OS X Lion Server for $50. But Apple&rsquo;s core competency is in the
narrow user experience of the robust client and, again, the company only
seems to be interested in using cloud technologies to make that
experience as great as possible.</p>

<p>Google on the other hand, wants to commoditize everything between the
user and the web, more specifically, between the user and its own
products, where they can expose the user to ads. These products, like
free Android and ChromeOS on cheap devices to use Gmail and Google Docs
and other web services means no one has to pay for expensive software
suites anymore.</p>

<p>Microsoft got double-disrupted by Google and Apple. Apple&rsquo;s invention
(basically) of a viable mobile computing market means people barely even
need computers anymore.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> As everything, including enterprise,
goes web and mobile, Microsoft is getting squeezed out of the market.
It&rsquo;s going to take a while, but it&rsquo;s happening. They&rsquo;re certainly still
profitable and will be for a while, but the market trends are pretty
clear.</p>

<p>As Nick <a href="http://blog.byjoemoon.com/post/6228050657/windows-8#comment-219607914">points
out</a>,
Microsoft is moving toward a full cloud business, Azure, with Windows 8
as a stopgap. But the model isn&rsquo;t Microsoft&rsquo;s core competency the way it
is Google&rsquo;s (or even Amazon&rsquo;s), and it seems unlikely that Microsoft
will be able to price their services competitively. But maybe I&rsquo;m
totally wrong about that.</p>

<p>But selling and supporting home server software on commodity hardware
seems exactly like what Microsoft has always been good at. They&rsquo;ve
already been doing something very similar in the enterprise market for a
long time, and they have plenty of experience in the consumer market
(unlike, for example, Oracle).</p>

<p>If Apple has a flaw, it&rsquo;s a neglect of the web. If Google has a flaw,
it&rsquo;s over-centralization. I think Microsoft has a chance at disrupting
its disruptors, at riding the pendulum swing to long-term relevance in
the computing industry.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Even Apple joined in yesterday by
<a href="http://thisismynext.com/2011/06/06/icloud-announced-apple-wwdc/">unveiling</a>
their &ldquo;iCloud&rdquo; service, which is an interesting take on the cloud:
peculiarly (but perhaps predictably) device-centric, in an
increasingly web-centric world. I.e. iCloud leverages cloud
technology to make your experience with Apple devices much more
seamless, but it doesn&rsquo;t offer access to cloud data from the web or
enable any sort of collaboration, at least as far as I can tell.
&ldquo;Google&rsquo;s frame is the browser window. Apple&rsquo;s frame is the screen,&rdquo;
as John Gruber <a href="http://daringfireball.net/2011/06/demoted">put it</a>,
rather succinctly. <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>E.g. <a href="http://www.tonido.com/">Tonido</a>,
<a href="http://www.iomegacloud.com/landing_page.php">Iomega</a>, and
<a href="http://www.pogoplug.com/">Pogoplug</a>.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>With the addition of over-the-air syncing, OS activation, and
updates, this is now more true than ever.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Windows 8</title>
			<link>http://blog.byjoemoon.com/windows-8</link>
			<pubDate>Sat, 04 Jun 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/windows-8</guid>
			<description><html><p>Microsoft unveiled <a href="http://www.youtube.com/watch?v=p92QfWOw88I&amp;feature=player_embedded">a
demo</a>
of their next version of Windows on Wednesday, and it appears that
they&rsquo;re doubling down on Windows Phone 7&rsquo;s &ldquo;Metro&rdquo; user interface. Which
is great, because it (the UI) is the most novel, useful, and genuinely
interesting thing Microsoft has done in the consumer space in a long
time. And it&rsquo;s a surprisingly and encouragingly risky move, considering
Windows Phone 7&rsquo;s limited success so far.</p>

<p>There are some things I really like about the Metro UI. I love its
departure from the <a href="http://en.wikipedia.org/wiki/WIMP_(computing)">WIMP</a>
paradigm, and in a direction other than the grid of icons. The idea of
rich, interactive tiles is great,<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> as is the idea of making
shortcuts to discrete functions of apps as opposed to the apps
themselves and the concept of &ldquo;hubs&rdquo; that collect data from across
applicable apps.</p>

<p>Some of the ideas in the demo that extend Metro for Windows 8 are
fantastic, too. The implementation of multi-tasking (actually having two
apps on the screen at once, not running background apps) is basically
what I&rsquo;ve been wanting for a while.</p>

<p>Maybe the most interesting part, though, was the exposure of legacy
Windows 7 programs running alongside the new Windows 8 interface. While
it&rsquo;s superficially jarring to see two such different UI styles
juxtaposed, the really interesting idea that it presents is a
convertible device that exposes an intuitive, simplified, pared-down
interface in one orientation, and another more robust, complex, and
capable interface in the other.</p>

<p>The obvious application for this is a tablet that you can use as an
iPad-like consumption device for casual use that you can dock to a mouse
and keyboard for &ldquo;real work&rdquo; and maintain all the functionality of a PC.
There are some instances of something like this already: tablet/laptop
convertibles that dual boot into Android and Windows 7; as well as the
Motorola Atrix, an Android phone that you can dock into a laptop-like
terminal device and boots into a linux-based web browser environment.
Windows 8, though, forks the abstraction at a different level, which
makes it much more compelling by making it almost a matter of separating
function and data from presentation. Ideally, the two modes would offer
potentially the exact same data and (selective) functionality in two
separate but consistent experiences, instead of the weird split-brain
experiences of current offerings.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>In much the same way that the Metro UI&rsquo;s introduction answered the
question of mobile interface in a refreshingly novel way, this dualistic
(Platonic?) paradigm skirts the conceit of the iPad, and addresses the
inherent compromises in a qualitatively different manner. While the iPad
manages expectations by offering a completely separate and different
experience that&rsquo;s tailored to the device, Windows 8 offers full
functionality but in a crucially context-sensitive fashion.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>It&rsquo;s a really fascinating idea, but Microsoft has a very difficult
execution ahead of them. Unless they&rsquo;re prepared to seriously rethink
the dominant mode of robust input,<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> they&rsquo;re going to have to
figure out a way to seamlessly integrate and reconcile a fundamentally
touch-based direct-interaction interface with a fundamentally more
abstract, indirectly manipulated one. Additionally, they&rsquo;ll have to
address whether developers are going to have to create two separate
interfaces to all of their apps.</p>

<p>This exposes a more important and fundamental challenge, though. The
problem that Microsoft appears to be trying to solve with Windows 8 is
that of being able to take all of your computing with you wherever you
go, and have the full experience available to you, and its solution is
predicated on a particular confluence of technologies becoming mature
and robust enough for this to be feasible. The implementation of iOS was
similarly predicated on battery life and low power consumption
processors finally becoming powerful enough to run a serious operating
system on. But as Moore&rsquo;s Law&rsquo;s continuing influence makes
hardware/power/bandwidth even cheaper, another solution to this problem
that&rsquo;s being worked in parallel is becoming even more feasible,
interesting, capable, and solves another set of problems at the same
time: the cloud.</p>

<p>Which renders a lot of this dicussion moot. As web technologies become
more powerful and users realize more and more that they don&rsquo;t need
everything that traditional desktop applications have to offer, more of
the things that sit between the user and the browser becomes
overhead.<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup> After the web hits a certain level of robustness,
these differences between operating system user interfaces are ones
simply of chrome preference. They become commodities.</p>

<p>To reiterate: I like a lot of what Microsoft is trying to do with
Windows 8, and I applaud their direction of energy and ambition. But it
seems like a solidly middle-term goal that doesn&rsquo;t relate to a long-term
goal in any real way. Maybe it&rsquo;s a great way to maximize productivity
and extract value from a technological valley, but it in no way
addresses the mountain on the other side, which is steep and high enough
to poke through the clouds.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>They are, however, not exactly original. Android&rsquo;s widgets offer the
same utility, and more. Metro&rsquo;s solution is considerably more
elegant and unifying, which brings up something of a UI paradox: It
looks so much cleaner and prettier when everything is consistent and
uniform, but it does make it that much harder to
differentiate/distinguish the different tiles/icons, especially
quickly. <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>I imagine even the desktop mode and desktop applications will get a
UI revamp to make them more Metro-y, aesthetically at the very
least. <a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>As John Gruber
<a href="http://daringfireball.net/2011/06/ice_water_enthusiast">notes</a>, the
touch-based, direct-manipulation UI model is only one part of how
iOS simplifies the user experience. Other ways include: hiding the
file system, single-tasking full-screen apps, and platform-specific
apps built from the ground up with the specific interface in mind.
(Gruber also notes other differences like battery life that I don&rsquo;t
think relate to the discussion at hand.)<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Even if they do, how will legacy apps integrate into the new system?
It&rsquo;s an important question because legacy support is one of the
reasons Microsoft can&rsquo;t just do a complete reboot or a completely
separate operating system for tablets or simply extend Windows Phone
7 for tablets. <a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>That&rsquo;s obviously a sort of Google-centric way of seeing things, but
even Microsoft acknowledges the trend with its own web-based
offerings of Office, and with its announcement that Windows 8
development will be done in HTML and Javascript.<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Rat Holes</title>
			<link>http://blog.byjoemoon.com/rat-holes</link>
			<pubDate>Mon, 16 May 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/rat-holes</guid>
			<description><html><p>Rat holes I&rsquo;ve fallen into:</p>

<ul>
  <li>Programming Languages.</li>
  <li>Programming books.</li>
  <li>Text editors.</li>
  <li><a href="http://www.overclock.net/keyboards/491752-mechanical-keyboard-guide.html">Keyboards.</a></li>
  <li>Version control systems (Git).</li>
  <li>Password management.</li>
  <li>Backup.</li>
</ul>

<p>Rat holes I&rsquo;ve got one foot in, but haven&rsquo;t fallen all the way into, but
might still:</p>

<ul>
  <li>Web stuff. (Still sort of have one foot in this one.)<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></li>
  <li>Linux/Unix.</li>
</ul>

<p>I eventually settled on Java, so I can work on Android. I eventually
settled on buying a specific Java book. I eventually settled on Vim. I
eventually settled on a keyboard (this is a minor one). I eventually
settled on a version control system (and I don&rsquo;t even have a real use
for one yet).</p>

<p>Rat holes are things that I think I have to think about before
continuing my main project (learning to make software). They are
probably not things that are necessary to continue my project, but I
fall into them anyway. Some of them take a really long time to decide on
because they are often in some way religious or philosophical in nature
and the amount of research to be done is insanely deep and sometimes
leads in circles.</p>

<p>A positive, and partially true way of thinking about some of these rat
holes is that I want to use the best tools available to me, and I want
to learn to use these tools well before I start making something with
them. Chabo described it to me as depth-first traversal as opposed to
breadth-first traversal.</p>

<p>I think obsessive-compulsive completionism is one factor. The truth,
though, is probably that it&rsquo;s easier and more fun to read about and
learn about this stuff than it is to get about the business of learning
to program. It&rsquo;s peripherally useful procrastination, which is
potentially more harmful than plain goofing off because it can feel
productive because solving these more immediate concerns are more deeply
satisfying than making relatively small progress on the big project.</p>

<p>There is some value in rat holes. If you spend all day using a keyboard,
you should have a good one, just like if you spend all day sitting, you
should have a good chair; the value proposition simply dictates it. The
hard part is knowing how much value is in a given rat hole, and how much
work it will take to extract that value. But for someone new to a
domain, there isn&rsquo;t a really good way to gauge the value proposition,
because the research necessary is part of the work of extraction and the
uncertainty of that position and the fear of path dependence can be
paralyzing.</p>

<p>Some people seem to be more or less immune to rat holes. Maybe the
distinction is simply between constitutionally pragmatic people and
constitutionally idealistic people. Or maybe it&rsquo;s a distinction between
short-term thinking people and long-term thinking people. Either way,
the immune ones tend also not to worry much about what kind of keyboard
they have, or which text editor or IDE to use.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>Rat hole avoidance heuristic:</p>

<ol>
  <li>Be aware of the phenomenon.</li>
  <li>Self-monitor. Periodically ask myself if I am entering or have
entered a rat hole.</li>
  <li>Determine whether I am in a blocking or non-blocking rat hole. (Can
I delay this decision until I know more about the domain?)</li>
  <li>If non-blocking, skip. Else do only enough research to make a quick
decision, and only invest enough time to not be blocked and move on.
Remind myself that I can change my mind later.</li>
  <li>Periodically ask myself whether and how this rat hole advances the
big project.</li>
</ol>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>I&rsquo;d like to move my blog to a possibly self-hosted statically served
CMS like <a href="https://github.com/mojombo/jekyll/wiki/sites">Jekyll</a>.
Why? Good question. <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>I think this is especially true where it pertains to aesthetics.
Generally, I think immune ones value function much higher than form,
and don&rsquo;t worry about what their hardware or software looks like.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Holding the Web in Your Hands</title>
			<link>http://blog.byjoemoon.com/holding-the-web-in-your-hands</link>
			<pubDate>Thu, 12 May 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/holding-the-web-in-your-hands</guid>
			<description><html><p>I&rsquo;m typing this on a Cr-48, Google&rsquo;s pilot hardware for Chrome OS. On
Wednesday at the second keynote address of Google I/O, their developer
conference, Google
<a href="http://thisismynext.com/2011/05/11/google-chromebooks-coming/">announced</a>
that final retail hardware will be available this Summer from Samsung
and Acer. The day before, at the first keynote, they
<a href="http://thisismynext.com/2011/05/10/google-io-android-ice-cream-sandwich/">announced</a>
Android Ice Cream Sandwich (no version number yet), which will unify the
OS across phones, tablets, and Google TV.</p>

<p>While at first glance it may seem like Android and Chrome OS target the
same space, the respective tones of the two keynotes and the pricing
models of the coming Chromebooks indicate the following
<a href="http://twitter.com/#!/reckless/status/68373650872610816">break-down</a>:
Android for consumers; Chrome OS for enterprise.</p>

<p>That axis is interesting in certain respects, but it&rsquo;s not <a href="http://twitter.com/#!/chockenberry/status/68374633065361408">the
one</a> I find
most interesting: Android for touch interface; Chrome OS for separate
hardware inputs.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>Since the original iPad&rsquo;s introduction, I&rsquo;ve been ambivalent about the
form factor. On one hand, its direct manipulation interface paradigm
(and especially Apple&rsquo;s groundbreaking execution of it) is intuitive,
powerful, and genuinely fun. On the other, it&rsquo;s restrictive: the
interaction bandwidth is simply less than the venerable mouse + keyboard
combination; and the metaphor itself, while making certain
interactions&ndash;like swiping and pinch-to-zoom&ndash;natural, narrows the
available input options, relegating anything more than the first-order
gestures to hidden, basically undiscoverable gestures that bear no
relation to the metaphor.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>Text input is another problem. The on-screen keyboard&ndash;even with
automatic&ndash;real-time error correction, is no substitute for a real
hardware keyboard.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> It&rsquo;s transient and stultifying, which
can&rsquo;t but inform the text that passes through it to some, however
vanishing, degree. The form factor itself, furthermore, ergonomically
discourages text input.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></p>

<h3 id="technology-as-philosophy">Technology as Philosophy</h3>

<p>My biggest gripe about the iPad is that it&rsquo;s a reversion to pre-internet
media values. If implementations of technology are founded on
philosophical assumptions, the old broadcast media systems like
television and radio are founded on authoritarian, heirarchical ones. A
(more or less) centralized, consolidated authority produced the content
that the rest of us used to sit back and consume. In this respect, the
personal computing and internet revolutions&ndash;and blogging,
specifically&ndash;were intrinsically democratizing ones. The iPad is a
betrayal/reversal of this trend, a device that&rsquo;s optimized for passive
consumption, the interface optimized for interaction with the device,
instead of interaction with other people.<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup></p>

<p>The Chromebook, conversely, is an open embrace of the
web.<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup><sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup> It&rsquo;s an instantiation of the promise of the
removal of every barrier between us and a place where we can congregate,
communicate, and collaborate freely, as well as an investment in a
future where all of our computing is accessible from anywhere, from any
device.</p>

<p>I submit, too, that the laptop form factor is a more democratic one. In
lieu of a claustrophobic software keyboard, it dedicates half of the
visible hardware to input, metaphorically elevating the user&rsquo;s
production to the same level of importance as her consumption. It says
something about what the purpose of the device is: to participate in the
global conversation, in the exchange of ideas.</p>

<p><a href="http://www.youtube.com/watch?v=TVqe8ieqz10&amp;feature=player_embedded">This
ad</a>
introducing the Chromebook asserts that it&rsquo;s not a laptop, or even a
computer. Instead, &ldquo;it actually IS the web.&rdquo; It&rsquo;s a beautiful dream.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="http://dev.chromium.org/chromium-os/user-experience/form-factors/tablet">Tentative
plans</a>
for a Chrome OS tablet have not come to fruition, at least not yet.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Even Apple <a href="http://www.wired.com/gadgetlab/2011/01/apple-ios-multitouch/">has had to
resort</a>
to things like three- and four-finger swiping gestures that map to
commands that have no intuitive mapping to the the gestures. I&rsquo;ve
written more about the limitations of a the direct manipulation
paradigm
<a href="http://blog.byjoemoon.com/post/3556631202/touch-ui-is-not-the-future-of-everything">here</a>
. <a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>The tactile feedback of a physical keyboard is difficult to
replicate. I&rsquo;d hoped that Swype or other software keyboard
technologies would succeed at routing around this problem, but in my
experience you still have to pay attention to both the keyboard and
the text you&rsquo;re producing which is a lot of mental overhead that the
tactile feedback of keyboards obviates. <a href="http://www.youtube.com/watch?v=M9b8NlMd79w&amp;feature=player_embedded">Blind
type</a>
seems like it&rsquo;s close to a solution, but I haven&rsquo;t gotten to try it
yet. <a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>This is a big enough problem that Bev puts down the iPad and grabs
her iPhone to write emails. <a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>I think this bears out, too, in Old Media&rsquo;s
<a href="http://www.thedaily.com/">enthusiastic</a>
<a href="http://arstechnica.com/apple/news/2011/05/conde-nast-to-roll-out-ipad-subscriptions-starting-with-the-new-yorker.ars">embrace</a>
of the iPad as a publishing platform, despite Apple&rsquo;s fairly
draconian restrictions and price structure.<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Of course, Google, as a company, constitutionally embraces the web.<a href="#fnref:6" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>I do have misgivings, though, about the &ldquo;cloud&rdquo; model, and giving up
control of our data and identities to corporations.<a href="#fnref:7" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>The 2-Foot User Interface for the 10-Foot Screen</title>
			<link>http://blog.byjoemoon.com/2-ft-ui-for-the-10-ft-screen</link>
			<pubDate>Thu, 07 Apr 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/2-ft-ui-for-the-10-ft-screen</guid>
			<description><html><p>Television user interfaces could be so much better. I thought that with
the introduction of set-top box internet video solutions, user
interfaces would make menu and playback navigation and content
management easier, especially now that we have ways to interact with
these boxes using our smartphones.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> If anyone, I thought
Apple&ndash;with their customary attention to user experience and their
vertical integration&ndash;would have an elegant solution to interaction with
TVs. But when I hooked up our new Apple TV2<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> and tested out
the remote application on the iPod Touch and the iPad, all I got was a
simulation of the physical remote control.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> Everything else
I&rsquo;ve tried is pretty much the same, to include: Boxee and Windows Media
Center (couldn&rsquo;t even actually find an Android or iOS app to work with
this, unsurprisingly).</p>

<h3 id="the-10-foot-user-interface">The 10-Foot User Interface</h3>

<p>Used to be that all you needed was a few buttons on your remote: power
toggle, channel up and down, volume up and down; plus some optionals:
numbers 0–9, mute toggle, channel recall, etc. Once you got used to it,
you didn&rsquo;t even need to look at the remote, which was nice. But as
television sets became more complex, with more options, settings, and
configurations, remotes got loaded with more buttons. Peripheral devices
added even more buttons, usually on a separate remote, which you could
integrate into a single universal remote if you wanted to and had the
technological wherewithal to do so.</p>

<p>At first, with simple sets, this really was fine. And even as TVs got
more complicated and loaded with more functionality, you still needed a
way to control all the additional functionality. There wasn&rsquo;t any other
place for this than the remote and there wasn&rsquo;t any way to do it on the
remote without adding buttons.</p>

<p>Meanwhile, from the other direction, desktop computer interfaces were
simplified and enlarged for use on the TV screen (from 10 feet away).
Window management was (naturally) discarded for a simple environment
optimized for doing one thing: watching video.</p>

<p>As these two TV paradigms converge in devices like the Apple
TV,<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> Boxee Box, and Google TV, as well as in TVs themselves,
the functionality starts to overwhelm the UI bandwidth: UI elements
increasingly clutter up and obscure the screen, menu hierarchies become
deeper and more complicated, and the fact is that a TV is not a computer
monitor and it sucks to do computer monitor things on a TV
screen.<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup></p>

<h3 id="reboot">Reboot</h3>

<p>No one&rsquo;s really been in a position to be able to rethink TV UI until
very recently. Even now, one cannot make the assumption that a TV
watcher has a peripheral with which to interact with the TV<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>,
so the system designers still have to provide an on-screen UI by
default.</p>

<p>But I think it&rsquo;s getting closer. Watching TV with a laptop, tablet, or
smartphone in hand is becoming more popular, as people idly multitask,
google, or participate in social media as they watch.<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup> And if
you take it for granted that your user has a secondary screen, you can
start to really change the way TV UI works.</p>

<p>As a starting point, you could relegate all UI elements to the secondary
screen, leaving the primary screen completely over to what it&rsquo;s
specifically designed for: playing video. The secondary screen,
meanwhile, because it&rsquo;s a 2-foot or 1-foot UI device, can present a lot
more information, and present navigation that is faster, more
informationally dense, granular, and intuitive than the on-screen
navigations that we&rsquo;re used to.</p>

<p>You could also add more features, like playlists or queues,<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup>
app integration,<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup> games, and multiple video
streams.<sup id="fnref:10"><a href="#fn:10" rel="footnote">10</a></sup></p>

<h3 id="update">Update</h3>

<p>Looks like
<a href="http://scobleizer.com/2011/04/13/the-most-important-new-protocol-since-rss-airplay-three-cool-new-apps-that-use-it-to-change-how-we-view-tv/">this</a>
might solve all my complaints.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>And iPod Touches. <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Bev got me one for Valentine&rsquo;s Day.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>At least text entry was much easier; hitting a text field would
invoke the touch screen keyboard.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>The ATV isn&rsquo;t actually a convergence device, since it doesn&rsquo;t take
input from traditional signals like analog broadcast, cable, or
peripheral physical media players, but it sort of will be if IPTV
ever becomes a reality. <a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>This is why WebTV failed, or at least why it sucked.<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Obviously because not everyone has one and shipping an iPod Touch
equivalent as a remote is certainly cost-prohibitive.<a href="#fnref:6" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>Also, some high-end universal remotes cost as much as iPod Touches.<a href="#fnref:7" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>Which becomes more important as short web clips become a ubiquitous,
first-class medium. <a href="#fnref:8" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p>Twitter with automatic hashtags for whatever you&rsquo;re watching?<a href="#fnref:9" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:10">
      <p>Like picture-in-picture, but more robust.<a href="#fnref:10" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Rationality Engine</title>
			<link>http://blog.byjoemoon.com/rationality-engine</link>
			<pubDate>Tue, 29 Mar 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/rationality-engine</guid>
			<description><html><p>Paul Graham&rsquo;s <a href="http://paulgraham.com/disagree.html">essay on
disagreement</a>, the discussion C-Rob
started
<a href="http://cgr.tumblr.com/post/3760719822/notes-toward-a-better-online-discussion-solution">here</a>,
<a href="http://techzinglive.com/page/692/113-tz-interview-jeff-atwood-stack-exchange">an
interview</a>
with <a href="http://www.codinghorror.com/blog/">Jeff Atwood</a> I heard recently,
and a comment by Jake on GReader got me thinking about debate, though
really more about how debate is sort of an implementation of
rationality. Rationality, I think, is sort of self-evidently important,
but not easy. Humans have <a href="http://en.wikipedia.org/wiki/List_of_cognitive_biases">natural cognitive
biases</a> that are
hard to recognize, especially in oneself. Also, so much of our
collective brainpower and discussion is spent/wasted on faulty arguments
and corrections thereof. We could spend so much more of our cognition
and time more effectively if we could cut the argumentational fat.</p>

<p>So, how to learn to think more rationally? I idly peruse sites like
<a href="http://lesswrong.com/">Less Wrong</a> and <a href="http://www.overcomingbias.com/">Overcoming
Bias</a> and I often find the articles to
be interesting, but they&rsquo;re very often dry, difficult to digest, and
even more difficult to apply.</p>

<p>I think part of the problem is that while these places (and, I&rsquo;m sure,
others) provide
<a href="http://knol.google.com/k/explicit-vs-implicit-and-declarative-vs-procedural-language-learning">explicit/declarative</a>
knowledge, they don&rsquo;t provide implicit/procedural knowledge.</p>

<h3 id="proposal-rationality-fu1">Proposal: Rationality Fu<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></h3>

<p>I would like to see a rationality engine, a collaborative, pedagogical
tool to encourage rational thinking. I think it would consist of the
following components:</p>

<ol>
  <li>Khan Academy-style classes to teach the explicit concepts to
include:
    <ol>
      <li>Symbolic logic</li>
      <li>Logical fallacies.</li>
      <li>Cognitive biases.</li>
      <li>Statistics/statistical analysis.</li>
    </ol>
  </li>
  <li>Exercises to test and enforce understanding.</li>
  <li>A game system<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> to include:
    <ol>
      <li>Experience/karma and rank (or belts).</li>
      <li>Achievements/badges.</li>
    </ol>
  </li>
  <li>A discussion engine like the one C-Rob and I
<a href="http://cgr.tumblr.com/post/3760719822/notes-toward-a-better-online-discussion-solution#comment-163868512">discussed</a>
to include:
    <ol>
      <li>Symbolically logical statement templates in which you can plug
words to make an argument.</li>
      <li>Threaded conversation with configurable/visualizable views.</li>
    </ol>
  </li>
</ol>

<hr />

<p>Thinking in terms of symbolic logic can facilitate emotional
disengagement from a discussion, so that we can recognize our mistakes
and biases more clearly, and concede wrongheaded points more readily. I
think knowing about logical fallacies and cognitive biases doesn&rsquo;t
necessarily help us avoid them and I believe that thinking is action,
and rationality is habit. Having a system of discussion that exposes the
underlying mechanics of ideas can make our everyday thinking more clear
and more precise.</p>

<p>Higher-performance climbing shoes can improve your climbing but getting
stronger and learning technique are more pertinent and effective.
Similarly, we have many tools available to us to augment our thinking,
but more effective and more important would be for us to improve the
generative power: thinking itself.</p>

<p>I think we&rsquo;ve harvested the low-hanging fruit of our cognitive symbiosis
with the internet. Time to climb the tree.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Think fu? Brain fu? <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>In the above-mentioned interview, Jeff Atwood, co-founder of Q&amp;A
site <a href="http://stackexchange.com/">Stack Exchange</a> talks about how a
little bit of gamification encouraged <a href="http://stackoverflow.com/">the collaborative creation of
an incredibly rich (the richest?) resource for
programmers</a>, both as a static repository
for knowledge and as a place to get help from other users of the
site. <a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Obviously a very small subset of the population at large would be at
all interested in this idea, but I think it largely overlaps the set
of people who like to think about difficult things to think about,
as well as the set of people who like to debate things online.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Climbing Rocks</title>
			<link>http://blog.byjoemoon.com/climbing-rocks</link>
			<pubDate>Mon, 21 Mar 2011 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/climbing-rocks</guid>
			<description><html><p>I had been to a climbing gym once or twice, but only started climbing
seriously while I was stationed in Okinawa in 2005. A friend of mine,
Jay, took me outdoors and got me on a top-rope. I was hooked instantly.
It was exhilirating, challenging, and focused my attention both mentally
and physically in a way I&rsquo;d never really experienced before. Admittedly,
the gear was no small part of the attraction at the beginning, and sport
climbing involves a considerable amount of gear.</p>

<p>&ldquo;Free-climbing,&rdquo; or class five climbing, is defined as climbing a
vertical or near-vertical face under your own strength, without the aid
of special gear in your upward motion (though most people use safety
gear in case they fall).<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>Free-climbing is further divided into &ldquo;traditional,&rdquo; or &ldquo;trad,&rdquo;
climbing, &ldquo;sport&rdquo; climbing, and &ldquo;bouldering.&rdquo; Trad involves climbing
bare rock while placing cams and nuts into cracks as protection from
falling. Sport routes are generally bolted, meaning bolts and anchors
are drilled into the rock at certain intervals so you can place
protection.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>Like most, I started out sport climbing on top-rope, i.e. with a rope
already set up at the top of a route, but Jay very quickly got me on
lead by not telling me how much harder and scarier it was. Which worked
pretty well. (His other trick was to refuse to let me down until I&rsquo;d
finished a route.) And I really enjoyed it and stuck with it. I
continued to buy gear: higher-performance shoes, carabiners,
quick-draws; and I was preparing to make the leap to trad, an
intimidating prospect considering how much more money I&rsquo;d have to spend
on a rack of cams and nuts, as well as the substantial new body of
expertise I&rsquo;d have to absorb: protection placement.</p>

<p>But then a funny thing happened: I went in the opposite direction, I
started bouldering. All it took was an invitation to join a couple of
acquaintances from the University climbing gym on a bouldering trip to
Squamish.</p>

<p>Bouldering, of course, is the practice of climbing relatively low, but
generally much more difficult routes, or &ldquo;problems&rdquo; in bouldering terms.
The practice started as just that: practice, something for rock-climbing
pioneers to do in Yosemite Valley, between ascents of big walls, to
train for strength and technique. It soon became an activity unto
itself.</p>

<p>I had bouldered before the Squamish trip, of course, but only idly,
mostly between sport routes or when I couldn&rsquo;t find a partner. It had
always just seemed like sport climbing, except shorter. But that trip to
Squamish unlocked an understanding about the very divergent nature of
bouldering for me that&rsquo;s a bit difficult to articulate.</p>

<p>There are all the obvious differences: bouldering involves less gear,
just a pair of shoes and optionally a chalk bag and crash pad versus
shoes, chalk bag, rope, harness, belay device, a rack of quick-draws, a
helmet, and (importantly) a partner. And as I mentioned earlier, while
the equipment was initially a draw for me, I later found that it and
other meta-climbing periphera like learning strategy, clipping
techniques, knots, belay techniques, etc. served to distract me from the
business of actual climbing.</p>

<p>Bouldering also generally involves more strength and intensity whereas
sport climbing emphasizes endurance and economy of motion. It was
bouldering that opened my eyes not only to the incredible range of what
the human body is capable of, but also to a greater understanding of how
seemingly unrelated factors contribute in a holistic fashion to success.
Sport climbing necessitates strategic thinking, but bouldering narrows
focus onto specific technique: everything from minutely detailed
adjustments in foot placement,<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> to frequently
counter-intuitive body-positioning, to the all-important engagement of
core muscles.</p>

<p>In these and other general ways, bouldering is a more abstract and more
internally pure experience. The distinct values inherent to the separate
disciplines are right there in their respective objectives. The sport
climber&rsquo;s goal, to scale a natural formation, is decidedly more
external, because altitude is the ambition and measure of achievement,
whereas the boulderer&rsquo;s goal is better described as performing a
somewhat arbitrary, artificial, perhaps contrived series of
movements.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> This distinction isn&rsquo;t airtight, by any
means: the sport climber does not simply choose the easiest ascent of a
particular crag or peak or face, and neither does the boulderer
generally have completely arbitrary starting and finishing
points.<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup> But the distinction remains.</p>

<p>Where sport climbing is a struggle to conquer nature, bouldering is a
struggle to conquer one&rsquo;s own weakness.</p>

<p>The two disciplines are complementary, to be sure; each makes you better
to a large degree at the other. But bouldering teaches you more about
your own body, about your personal biomechanics,<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>
and about how to most effectively wield them. And it is for me an
ultimately more spiritual experience, in that for long moments at a
time, instead of all the nearly omnipresent internal voices chattering
about which holds to use and how far the next clip is, there is,
internally, only silence, only pure, focused will, a transcendently
complete collapse of mind and body.</p>

<p>Somewhat paradoxically, bouldering is also more social than sport
climbing. Whereas sport climbers generally work in pairs, alternating
between climbing and belaying, boulderers, with notable exceptions, tend
to gather in groups and work problems and problem sets together, with
mutual encouragement and instruction.<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup> I&rsquo;ve
definitely met more people bouldering than I ever did sport
climbing.<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup></p>

<p>I&rsquo;ve been climbing for about six years now, and find myself in a bit of
a dilemma. As I get stronger and more capable, and climb harder
problems, I put disproportionately more stress on certain joints, and my
aging body<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup> meanwhile is more susceptible to
injury, and needs longer recovery times. Certain knuckles are more or
less permanently inflamed now, and tendons in both wrists strained. And
at the same time, improvement that was steady for years has slowed to a
trickle.</p>

<p>All of which is merely to note that the low-hanging fruit of youthful
vigor and easy improvement have been harvested, and the real work is
simply yet to begin. It means I have to be a better steward of my body,
and that any further improvement is going to be hard-won. It means I&rsquo;ll
actually have to spend time on preventive weight-lifting and exercises,
focus training sessions on specific strength and technical objectives,
cross-train for endurance, generally pay more attention to what my body
is telling me.</p>

<p>As with any serious endeavor (I suspect), the more I learn about
climbing, the more I become aware of the humbling vastness of what I
don&rsquo;t know. After a period of soul-searching and asking myself if the
point of climbing was to get better at climbing, or to enjoy myself, I
realize, even as I write this, that the two are intertwined, and that my
own struggle with weakness is only now beginning.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Class six climbing is called &ldquo;aid climbing,&rdquo; and involves ascenders
and other tools to essentially create artifical hand- and footholds.
Class four, or &ldquo;scrambling,&rdquo; is climbing relatively gently sloped
surfaces with only the occasional need for
hands.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>More extreme (read: dangerous) forms of climbing include &ldquo;free
soloing,&rdquo; and the recently pioneered &ldquo;free basing.&rdquo; Free soloing
refers to climbing routes alone sans protective gear, with a fall
generally resulting in serious injury or death. As far as I know,
Dean Potter is the only person in the world to combine free soloing
with BASE jumping in what he calls free base climbing: he free solos
wearing a parachute that he can deploy if he falls. This allows him
to push the limits of free soloing, extending the range of what he
can attempt without any other equipment. Also, he generally jumps
from the top of a route he&rsquo;s
climbed.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Both on footholds (using toes, edges, and heel-hooks) and in support
positions (flagging).<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>To speak in broad generalities, in my experience, sport (and,
naturally, trad) climbers tend to think and talk a lot more about
nature and their relationship to it (adversarial or otherwise) than
boulderers, emphasize outdoor climbing a lot more, and get bored and
tired of indoor climbing much more
easily.<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>Either way, it&rsquo;s an absurd activity, rock-climbing, though I think
bouldering is more accepting of this inherenet
absurdity.<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>People have a fairly astounding variety of shapes and sizes. Height
is an obvious differentiator when it comes to climbing (though,
somewhat counterintuitively, excessive height is often a liability
when it comes to difficult bouldering because the strength to weight
ratio is a lot harder to maximize) but so are wingspan versus leg
versus torso lengths, flexibility, hand/finger size, and muscle
density. Two climbers of equal height and strength may have vastly
different climbing styles due to different limb lengths and
flexibility.<a href="#fnref:6" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>A friend once made the comparison to skateboarding, which I&rsquo;ve never
done, but the comparison definitely seems
apt.<a href="#fnref:7" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>The majority of our Portland friends are either climbers or people
we met through
climbers.<a href="#fnref:8" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p>Not that I&rsquo;m that old, but I never had a single injury and could
climb more energetically and for longer intervals with shorter
recovery times just a couple of years
ago.<a href="#fnref:9" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Touch UI is not the future of everything.</title>
			<link>http://blog.byjoemoon.com/touch-ui-is-not-the-future-of-everything</link>
			<pubDate>Sun, 27 Feb 2011 00:00:00 PST</pubDate>
			<guid>http://blog.byjoemoon.com/touch-ui-is-not-the-future-of-everything</guid>
			<description><html><p>Apple beat even the most optimistic expectations and sold over 15
million iPads in the first year of its release, and it looks like the
second version is just around the corner. The competitors are lining up
as well: the Motorola Xoom, running Android 3.0 Honeycomb, the HP
PalmPad, and the RIM Playbook all look like interesting
offerings.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>Many have pointed out the sad irony of the fact that Microsoft, who has
been pushing tablet PCs for over a decade, seems to have by far the
least compelling offering, and no apparent serious road map for future
products that address the deficiencies of desktop Windows on a tablet
form factor. Historically, Microsoft&rsquo;s tablet and even smartphone
operating systems have been desktop Windows pared down (and barely, in
the case of tablets) for use with a stylus. I had a convertible laptop,
in fact, and though I was excited about the idea, it never really worked
that well, for reasons I was never able to articulate.</p>

<p>In the aftermath of iOS, the reasons are pretty obvious.</p>

<p>There were many aspects of the iPhone that revolutionized and redefined
the smartphone market. The iTunes and iPod ecosystem certainly helped.
So did apps (though third party apps didn&rsquo;t come until later, so didn&rsquo;t
really contribute to the initial popularity), and so did Apple&rsquo;s
traditional focus on hardware design. But the single largest and most
revolutionary factor of the iPhone&rsquo;s (and the iPod Touch&rsquo;s as well as
the iPad&rsquo;s) success was the user interface. Apple designed the UI around
a new paradigm of touch and direct manipulation, shining a light on how
elegantly simple, refreshing, and intuitive human-computer interaction
could be. The dominant paradigm shifted and now the measure of a
smartphone UI basically boils down to how well the paradigm is
executed.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>Apple&rsquo;s touch interface allowed the dismissal of many of the
abstractions that people found difficult about computing: Buttons took a
step away from metaphor and toward the physical as did navigation and
scrolling. Diminishing these abstraction-distances made the idea of
transforming the device into the specific running application more
feasible and more emotionally satisfying.</p>

<p>While it makes complete and and perfect sense to collapse the
distinction between input/control and display for a mobile device like a
smartphone because by their nature these devices are constrained by
their form, I would argue that it makes less sense as the device gets
larger. Larger devices (including, arguably, current iPad-sized tablets)
are fatiguing to use for long periods of time, heavy to hold up and
ergonomically unsound. Occlusion can also be a problem.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> As
<a href="http://10gui.com/" title="10/GUI">this UI concept video</a> demonstrates, these
problems get bigger with the device. On a real workstation, these
problems become intractable.</p>

<p>With OS X 10.7 Lion, Apple wants to bring many of the things that make
iOS appealing to the desktop. For certain aspects, this is great: saving
application/document states, abstraction of the file system,
simplification of application management, etc. For other aspects, the
results are ambiguous: disappearing scroll bars and reversing the
direction of two-finger scrolling, for example. To take the latter
example, Apple is attempting to bring desktop multitouch gestures closer
to the iOS counterparts. In 10.6 dragging two fingers down on the
trackpad scrolls down a page. In 10.7, the same gesture scrolls up, just
as would happen if you were to touch an iPhone&rsquo;s screen and drag down.
There isn&rsquo;t necessarily a correct way to do this (it reminds me of the
change of the default in mouse-look direction in first person shooters),
but it&rsquo;s weird to change an established convention in a way that&rsquo;s not
really any more intuitive. The intractable problem here is that a direct
manipulation paradigm is fundamentally different from an indirect one,
and trying to force one onto the other creates a bad experience.</p>

<p>Still other paradigms Apple is trying to bring from iOS to the desktop
are just bad. Full screen applications, for example, are perfectly
sensible on a handheld, size-constrained device (and maybe even on
smaller laptops), but on a world of large desktop screens, it only
restricts the interface artificially. It&rsquo;s also really strange to see
Apple embrace this concept now when it makes the least sense after so
many of years of denying it completely.</p>

<p>Just as Microsoft tried to force the desktop&rsquo;s indirect manipulation UI
paradigm onto devices where a direct manipulation UI would have been
more appropriate, and suffered for it, Apple seems to be trying to do
the exact opposite. There&rsquo;s no denying that Apple is the uncontested
thought-leader in computer UI, which is why I&rsquo;d be much more interested
in seeing them offer new ideas in desktop UI with the very different
inherent ergonomic constraints of the desktop environment in mind. The
above-linked 10/GUI concept is still the most compelling I&rsquo;ve seen with
these considerations in mind. Especially with successive generations
growing up with ubiquitous technology, I really don&rsquo;t think dumbing down
our interactions with computers is helpful.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>I&rsquo;m not particularly interested in tablets because I always feel
restricted even on laptops. To me, the tablet is an even more
restricted laptop, one that&rsquo;s built much more for consumption than
production. For a portable, I&rsquo;m much more interested in the MacBook
Air. <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Android&rsquo;s primary influence before the iPhone was the Blackberry.
WebOS and Windows Phone 7 are widely regarded as excellent UIs (at
least conceptually), while Blackberry OS&rsquo;s half-hearted attempt to
convert to a touch paradigm is widely panned.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Good app design can mitigate this, but not eliminate it.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>End of Comments II: Browsers and Identity</title>
			<link>http://blog.byjoemoon.com/end-of-comments-ii-browsers-and-identity</link>
			<pubDate>Thu, 17 Feb 2011 00:00:00 PST</pubDate>
			<guid>http://blog.byjoemoon.com/end-of-comments-ii-browsers-and-identity</guid>
			<description><html><p>I think it&rsquo;s pertinent to note first that my thoughts on this thread and
some others comes from a place of deep interest in technology, but basic
technical laity. I have very little understanding of what&rsquo;s under the
floorboards of the internet. I mostly only see the levers and buttons
that are exposed to me, and can imagine many lever/button layouts that
would potentially make my life easier and better without first being
hobbled by the foreknowledge of the many (I&rsquo;m sure) technical
limitations. I will try to discuss the technical aspects as far as I am
able, but be aware that that&rsquo;s not very far.</p>

<p><a href="http://en.wikipedia.org/wiki/Dave_Winer">Dave Winer</a>, father of two of
my favorite things, RSS and podcasting, as well as huge influence on how
I think about the internet, stopped by and commented on <a href="http://blog.byjoemoon.com/post/3112676038/the-end-of-comments">my last blog
post</a>.
And then made the comment into a post on his own blog, <a href="http://scripting.com/">Scripting
News</a>. Which is a great illustration of the point
of my last post. This is unnecessary duplication and broken connection.
The way I wish it would work, and the way that I was trying to describe
in my last post is this: when Dave comments on my blog post, the comment
would also automatically appear on his blog, with a link to my post. Or
maybe more accurately, Dave would write a blog post (which would
naturally appear on his blog) with a reference to my blog post and then
my blog post would aggregate all responses to it (including Dave&rsquo;s) and
publish them underneath the text of the my post (with links back to all
of them).</p>

<p>I think the difference in mindset is still basically a Web 1.0/2.0 one
of read-only web vs. read-write web. When you stop making the
distinction between a user and something-above-a-user (publisher,
content-creator, aggregator, editor, administrator) you start to realize
that it&rsquo;s even flatter than we thought. Comments as they are now are
like a bunch of micro-blogs that everyone creates on everyone else&rsquo;s
sites. Disqus starts to connect them so that all of a user&rsquo;s comments
made through Disqus are a more like a single micro-blog. My point is
that microblogs are just a subset of blogs and all the contributions you
make to the internet should be connected. It should all be part of your
blog.</p>

<p>So I intended for this post to be just an elaboration on how I think the
browser should encourage richer interaction, but the comments on my last
post got me thinking more and more about how that&rsquo;s really a subset of
the problem of online identity (or lack thereof). So I&rsquo;ll talk about the
browser thing first and then talk about identity at large afterward.</p>

<p>Browser The thing about the internet is that you can be a producer of
content very easily (and more easily as tools get better). You can be
and are on the verge of contributing to the internet at all times. But
our tools can better expose this potential to us as users, as well as
encourage a richer contribution.</p>

<p>The components of my vision:</p>

<ol>
  <li>A browser.</li>
  <li>An automatically created personal blog for every user.</li>
  <li>A <a href="http://en.wikipedia.org/wiki/Trackback">TrackBack</a>-like protocol
that aggregates all posts with reference to a post.</li>
  <li>A set of filters for those TrackBacks.</li>
  <li>
    <p>Alternate views. I&rsquo;ll elaborate on these in order:</p>
  </li>
  <li>
    <p>I think it&rsquo;s pretty obvious how the browser fits into the scheme but
I also think that &ldquo;browser&rdquo; is no longer an adequate term for what
these applications do now. &ldquo;Browsing&rdquo; is very low on the list of
things I use my browser for, and already we&rsquo;re linguistically
confining ourselves to a passive role on the internet. &ldquo;User agent&rdquo;
is somewhat clunky (plus I don&rsquo;t really know precisely what it
technically refers to), but I&rsquo;ll go with that (or UA) until I think
of something better.</p>
  </li>
  <li>
    <p>The UA would also hook directly into a user&rsquo;s personal blog when she
logs into the UA (and ideally would take over most of the use-cases
for most of other web services we use, especially simple ones like
micro-blogs). Whenever the user feels the inclination, she may hit a
button to invoke a horizontal or vertical split in the UA&rsquo;s window
between whatever content she was viewing and a large, expansive,
non-modal WYSIWYG rich text field. (Alternatively, it could open a
new window for the text editor.) Optional title field, optional body
field. Optional tag field. Any post made from the UA would be
published on the personal blog. Dave Winer&rsquo;s <a href="http://scripting.com/stories/2011/01/05/upcomingTheMinimalBlogging.html">Minimal Blogging
Tool</a>
comes from a similar line of thought, I think (the collapse in
hierarchy between input and output by users).</p>
  </li>
  <li>
    <p>As far as I can tell, that last part could be implemented now on the
application level, basically in a vaccuum. But I think the TrackBack
part would need some sort of open standard protocol to work in a
decentralized way. What it would look like for a user would be that
at the bottom of any web page, or maybe by invoking another hard
browser button, you could see all the UA-level blog post responses
by anyone. Google actually implemented a very similar feature into
the Google toolbar called
<a href="http://www.google.com/sidewiki/intl/en/index.html">Sidewiki</a>. It
failed as far as I can tell, and was panned by users and site owners
alike, probably for good reason. My bet is that this was because of
execution and lack of filtering, which I&rsquo;ll talk about later. The
bigger problem to me is that I think this used Google infrastructure
to store and maintain the whole thing (corporate silo). I suppose
the current TrackBack system or something like it would work, but it
depends on 1. the original post to be able to receive communication
from referring posts and maintain a list of them; and 2. the
referring posts to notify the original post that they exist. I guess
there&rsquo;s not really any way around it needing some incarnation of
both of those parts to work, which would need some sort of
standarization of implementation of blogging platforms.</p>
  </li>
  <li>
    <p>A user should be able to filter these responses/TrackBacks in any
number of ways. I suppose the default would be a chronological
firehose, but there should be a way to promote responses from
different contact lists to the top of the pile. There should also be
a way to filter by community/distributed collaborative filtering.
This is more or less what Digg, Reddit, Slashdot, and Hacker News
already are. They share stories, comment on them, then crowd-source
the ranking of both stories and comments. The model of the Atlantic
Monthly blogs&rsquo; comment spaces of well-moderated, directed discussion
could also pretty easily be converted this way, I think.</p>
  </li>
  <li>The UA should have the ability to look at more than a single page at
a time. The particular application for this that I have in mind is a
view of an entire conversation thread. Tweetdeck for Android has a
neat feature with which you can drill down into an @reply tweet and
hit &ldquo;conversation&rdquo; and it&rsquo;ll show you, where applicable, the entire
conversation as far back as the back and forth @replies go. I like
this because context is always good and it exposes the
conversational, derivative, shoulders-of-giants, re-mix nature of
ideas in general. Imagine reading an especially compelling post,
then being able to hit a button that extends it up and down, showing
the conversational context and thought process that led to it, as
well as any number of responses.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></li>
</ol>

<p>This vision is only additive (and barely that). One could still navigate
every part of this with a regular old web browser, because it&rsquo;s still
really just web pages and hyperlinks. None of the technology is new.
It&rsquo;s simply a different way of presenting options and artifacts that
already exist in order to encourage better, more considered contribution
to the community/communities of ideas.</p>

<p>It&rsquo;s also really just another take on this idea of several posts ago
(all content is the same, everything is streams and filters).<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<h3 id="identity">Identity</h3>

<p>Let me now apply my lens of elevating the user/collapsing the
distinction between user and producer to identity on the internet. I&rsquo;ve
already spent a bit of time thinking about this, and I still have mostly
the same position I laid out in this post about persistent online
identity. The problem I see in all the current projects like OpenID and
OAuth, are the limited scope. OpenID allows me to not have another
password to memorize, and OAuth gives me the ability to allow granular
access to my data from one service to another, but neither of these do
anything to connect the services that are associated with an OpenID or
OAuth.</p>

<p>Here are the components of online identity as I see it:</p>

<p>A feed/feeds. Contact list/lists.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> Home page/Profile/public
firehose/private feeds. You can see how this is mostly already described
in my discussion of the User Agent.</p>

<p>Any User Agent home page or dashboard should contain not just incoming
feeds and subscriptions, etc. (as described here), but also provide a
field for output. A text entry field. And not small, but big, or barring
that, at least one that resizes as your post grows longer (expands with
your ideas).</p>

<p>It seems to me that the best chance of all of this happening is for
Google to do it.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> Google already owns every necessary
component: Chrome, Blogspot, Buzz, Contacts, and Profiles. And maybe
even a protocol to connect all of it, similarly to how I describe:
PubSubHubbub. And, indeed, they seem to be at the beginning of a slow
process of rolling out some sort of identity service, either in the <a href="http://blog.byjoemoon.com/post/3357516295/%22http://techcrunch.com/2011/02/16/google-new-toolbar/">top
bar</a>
of different Google services, or in the form of a toolbar.</p>

<p>The only thing really missing is a decentralized/federated way for
people to host their own implementation and data, which isn&rsquo;t
necessarily out of the question, either, considering Google&rsquo;s history.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>To take this in a slightly different direction: Currently, looking
at web pages one at a time is the primary activity afforded by
browsers. Reading feeds (for me in Google Reader) is sort of a
secondary, virtualized (because a web-app) activity that&rsquo;s not as
robust or as fast, presumably because it&rsquo;s not native (and none of
the native feed readers I&rsquo;ve tried have been particularly
compelling). I kind of wish it were the other way around, because
feeds are now primarily how I interact with the web.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>A distinction worth noting that I should add to the public-private
spectrum: Public (or published) vs. Broadcast. There are things that
I might want to put in front of everyone I know. There are other
things that I don&rsquo;t feel the need to hide from public view, but I
don&rsquo;t necessarily want to shove in front of everyone&rsquo;s faces. I.e. I
may want to have an exchange with someone that I don&rsquo;t care is
public, but I don&rsquo;t necessarily want to clutter up people&rsquo;s feeds.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>There is a tension between people&rsquo;s desire for granular control and
their hatred of granular management. This seems like kind of an
intractable problem to a degree, but hopefully once identity and
contact management are standardized and portable, you&rsquo;ll only have
to create lists once, and then the ongoing new contact sort would
remain relatively manageable after that.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>It looks like <a href="http://www.azarask.in/blog/post/identity-in-the-browser-firefox/">Mozilla was working on making the browser the center
for
identity</a>,
too, but that post is pretty old and it doesn&rsquo;t seem like it&rsquo;s gone
anywhere. Edit: I forgot about
<a href="http://hacks.mozilla.org/2010/04/account-manager-coming-to-firefox/">this</a>.<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>The End of Comments</title>
			<link>http://blog.byjoemoon.com/the-end-of-comments</link>
			<pubDate>Fri, 04 Feb 2011 00:00:00 PST</pubDate>
			<guid>http://blog.byjoemoon.com/the-end-of-comments</guid>
			<description><html><p>It&rsquo;s a truism that medium and message are intrinsically intertwined.
This is the problem I have with the emerging (or maybe more accurately:
emerged) social networking platforms like Facebook and Twitter.</p>

<p>I don&rsquo;t care about the privacy implications (besides Facebook&rsquo;s
dis-ingenuousness toward the issues and the inherent conflicts of
interest) so much as I care about the inherent restrictiveness of
expression. In <a href="http://www.nybooks.com/articles/archives/2010/nov/25/generation-why/">this
review</a>
of <a href="http://www.imdb.com/title/tt1285016/">The Social Network</a>, Zadie
Smith talks about how Mark Zuckerberg&rsquo;s personal limitations inform and
define Facebook&rsquo;s.</p>

<blockquote>
  <p>The striking thing about the real Zuckerberg, in video and in print,
is the relative banality of his ideas concerning the &ldquo;Why&rdquo; of
Facebook. He uses the word &ldquo;connect&rdquo; as believers use the word
&ldquo;Jesus,&rdquo; as if it were sacred in and of itself: &ldquo;So the idea is really
that, um, the site helps everyone connect with people and share
information with the people they want to stay connected with….&rdquo;
Connection is the goal. The quality of that connection, the quality of
the information that passes through it, the quality of the
relationship that connection permits&ndash;none of this is important. That a
lot of social networking software explicitly encourages people to make
weak, superficial connections with each other (as Malcolm Gladwell has
recently argued), and that this might not be an entirely positive
thing, seem to never have occurred to him.</p>
</blockquote>

<p>I&rsquo;m not and never have been a Facebook user, but as far as I can tell,
its domination of the market is basically an accident. It&rsquo;s never had a
novel feature-set or a particularly compelling or usable design. It was
marketing and the slow erosion of initial exclusivity timed perfectly to
coincide with when the mainstream was ready for a service like it,
riding a rising tide of recursive network effect.</p>

<p>It made Zuckerberg&rsquo;s &ldquo;connection&rdquo; accessible. But the price of
accessibility was a degraded signal. As Smith writes, the quality of the
connections on Facebook is very poor. And this is because the signals
are low bandwidth by design. Input fields for text updates are small and
restrictive. Photos are easy to share but poorly presented, the system
implemented not to share rich, expressive photos, but low-grade,
mundane, documentarian ones: I was here; I did this; I was with these
people. Ubiquitous interaction is reduced to comment fields that
encourage short, <em>ad hoc</em> reactions, or&ndash;worse&ndash;to the elemental, further
irreducible unary piece of information: the &ldquo;like&rdquo; (though I suppose the
&ldquo;poke&rdquo; contains even less information). Facebook casts an extremely wide
net into the waters of the collective mind-space of its users, but that
net penetrates only into the shallowest of depths.</p>

<p>Twitter provides accessibility with a similar sacrifice of bandwidth.
I&rsquo;ve always felt like Twitter&rsquo;s great innovation was not the arbitrary
character limit<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, but the frictionless interface that
collapses the separation between input and output. The system itself
doesn&rsquo;t judge how noteworthy your expression is, leaving that to the
network at large in as close to a democratic way as is really
conceivable. But the limitations, also similarly to Facebook&rsquo;s, enforce
a shallowness of sentiment, encourage pith over comprehensiveness, and
discourage real discourse, or any sort of conversation at all.</p>

<p>I&rsquo;m obviously not crazy about this trend. I think accessibility at the
cost of genuine communication of ideas is just connection for
connection&rsquo;s sake. What I want is an equally accessible system that is
robust enough to handle not only all the depth of communication we&rsquo;ve
experienced as a species so far, but even more.</p>

<p>There are certain services that are getting closer to what I want.
<a href="http://www.disqus.com/">Disqus</a> makes it part of the way there (by
creating persistent identities for commenting across the web).
<a href="http://www.posterous.com/">Posterous</a> and
<a href="http://www.tumblr.com/">Tumblr</a> are headed in that direction, too.
Posterous provides a bunch of tools that take ubiquitous capture and
turn it into ubiquitous capture and share. But Tumblr, especially, and
more profoundly, flattens and opens up the comment-space. In a
traditional commenting system, there is an explicit heirarchy between
the post and its comments that is enforced (often) by aspects of
presentation, input field size, and not least by comments&rsquo; isolation
from the rest of the web. In Tumblr, when you re-blog something, it
appears in your own feed/page where you have the option to simply share
it as is, or expand on it however you see fit, and the original post
essentially just aggregates all the re-blogs underneath the original
content. This effectively promotes comments to the level of posts in and
of themselves. It also ties comments to a persistent identity. Twitter
@mentions are similarly flattened and open[^2], in that they are
on the same(-ish) level as regular tweets and tied to a persistent
identity.<sup id="fnref:3"><a href="#fn:3" rel="footnote">2</a></sup></p>

<p>The problem with both Twitter and Tumblr, of course, is that they only
work within the confines of the respective services. An ideal platform
would be distributed, independent of any particular corporate silo. Of
course, there is already a platform that fits this description: the
World Wide Web.</p>

<p>There are certain technologies in place that get us most of the way
there. There are a bunch of fairly simple to set up blogging platforms,
for instance, that are basically interoperable with the rest of the web
through the magic of hyperlinks.
<a href="http://en.wikipedia.org/wiki/Trackback">TrackBacks</a> offer a halfway
solution to distributed responses. I think the thing that&rsquo;s really
lacking still is accessibility.</p>

<p>It seems pretty clear to me that the only way for persistent identity
and distributed responses to really become truly accessible in the way
that Facebook and Twitter are is for the browser to integrate these
features. The browser should keep your identity. It should have a large,
blank, easily-invokable input field for text, photos, videos, whole web
pages, with a drag-and-drop interface. And it should aggregate responses
to any given web page, from across the web and from user-selected
groups.</p>

<p>I&rsquo;ll leave the specifics vague for now and lay out more details in my
next post.</p>

<p>Edit: continued
<a href="http://blog.byjoemoon.com/post/3357516295/end-of-comments-ii-browsers-and-identity">here</a>.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The 140-character limit exists because Twitter started as a way to
propagate SMS to groups easily. My theory is that the founders of
Twitter have no idea why it was successful, and are afraid to kill
the limit because they don&rsquo;t want to mess with the voodoo. In
practice, from what I can tell, a significant percentage of Twitter
usage is in the form of a headline plus a link to a larger post,
which is not very different from things that came before, notably
RSS. 
[^2]: This is less the case than it used to be. Twitter used to publish
all @replies into a user&rsquo;s main tweet-stream but changed this
implementation in May of 2009. Now, you only see someone&rsquo;s @replies
if you also follow the person they are replying to.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Twitter has a similar function to aggregate replies; you can always
click on a user&rsquo;s profile and drill down into their @mentions (which
I would assume is a simply a front end to a search for that user&rsquo;s
@handle), but unfortunately you can&rsquo;t drill down into an individual
tweet&rsquo;s replies. There is a function by which you can drill down
into a tweet that is an @mention and see the tweet to which it&rsquo;s
replying, but it&rsquo;s inconsistent: support is on a per-client basis
(Tweetdeck seems to handle it quite well) and it only works if the
@mention was through a client&rsquo;s implementation of the official
@mention API, whose support by clients is also not universal. It
also naturally won&rsquo;t work if the @mention was simply manually typed
into the tweet input field. <a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>Kindle</title>
			<link>http://blog.byjoemoon.com/kindle</link>
			<pubDate>Sat, 01 Jan 2011 00:00:00 PST</pubDate>
			<guid>http://blog.byjoemoon.com/kindle</guid>
			<description><html><p>Sounds like a few people got Kindles for Christmas, so I thought I&rsquo;d
share my last couple months&rsquo; experiences:</p>

<p>The hardware is almost flawless, and a big step up from the previous
incarnations of the Kindle. The cleaner, more cohesive design is
understated and minimalist. The matte charcoal finish provides better
contrast to the negative space on the reading surface, making it appear
closer to white. The slightly rubbery soft-touch plastic finish feels
fanastic in the hand, with just the right amount of grip, and the newly
consistent bezel width around the screen rounds out the device&rsquo;s
proportions nicely. It&rsquo;s also light enough for easy, long-term,
fatigue-free single-handed operation. As someone who spends most of the
day indoors staring at glowing rectangles of various sizes, I&rsquo;m a big
fan of the e-ink screen. The keyboard is not particularly useful, but as
the size of the screen is probably limited by cost and the overall size
of the device is fine, I don&rsquo;t mind its addition at all. The paging
buttons are well placed, not too easily pressed, and provide a
satisfying tactile feedback.</p>

<p>The reading experience is quite good. For the main use-case of the
device&ndash;reading books&ndash;the software is basically perfect. Wireless
download is straightforward, storage is plentiful, book management is
simple, and the syncing to other Kindle client devices is amazing. The
software does a good job of staying out of your way when you&rsquo;re reading,
while providing practical functions&ndash;like the in-line dictionary&ndash;when
it makes sense. While the built-in typefaces are fine, I would
appreciate a few more options, and in the absence of dynamic
hyphenation, it would be nice to have some control over the text
alignment without resorting to hacks. The full justification does create
some awkward line spacing issues occasionally, and there doesn&rsquo;t seem to
be a good reason not to give this control to the user. Also, the web
browser is a basically a joke&ndash;though I can&rsquo;t imagine any web experience
being particularly usable with the refresh rate restrictions of an e-ink
screen&ndash;and Amazon basically acknowledges this by relegating it to the
&lsquo;Experimental&rsquo; menu.</p>

<p>What&rsquo;s really remarkable about the device is how relaxing it is to use.
The light weight and the relatively inexpensive price of $139 alleviate
a lot of the anxiety about dropping it, because it&rsquo;s easier to hold,
less likely to sustain damage from a drop, and more easily replaced (and
I understand Amazon&rsquo;s warranty policy is quite liberal and their
customer service is top-notch). The e-ink screen, at the cost of some
functionality and elegance, is not only relaxing to the eyes, but also
completely kills the low-grade but omnipresent anxiety of a touch
screen. Which means you don&rsquo;t have to worry about triggering a
destructive action by accidentally brushing the screen somewhere and
grip-placement is a lot less restrictive, i.e. if it&rsquo;s most comfortable
to hold by a corner of the actual screen, you can. The physical buttons
mean that you know for sure when you&rsquo;ve made an input. And the
incredible battery life means I have literally never had to think about
charging the device. I plug it in to sync to Instapaper (more on this
later) often enough that I&rsquo;ve never even come close discharging the
whole battery, even on long trips. And while you get used to having to
deal with all of these things using a modern smartphone, once you don&rsquo;t
have to anymore the immersiveness of the reading experience is truly
unmatched.</p>

<p>But if the reading experience is damn near perfect, the management of
reading material isn&rsquo;t quite. Buying books is fine (and actually does
reduce the anxiety of needing to buy books ahead of time, since the
entire catalog is available anytime you have wi-fi access), but trying
to do anything outside of Amazon&rsquo;s sanctioned way is a significantly
bigger hassle.</p>

<p>I tried a couple different ways to try to integrate Instapaper with the
Kindle and have had moderate success. The first thing I tried was
Instapaper&rsquo;s native Kindle support. There are two ways to do this
natively: e-mail and download. You can have Instapaper automatically
e-mail a .mobi file to your Kindle email daily or weekly, which in
theory should download wirelessly and automatically when your Kindle
connects to the internet. As a Kindle owner you get a free email
address, and another one that will charge you for each document. I never
got it to work with either email address, and I never figured out how or
in what circumstances you get charged.</p>

<p>The second native way is to download the .mobi file from the Instapaper
web site and send it to your Kindle over USB. You can use the free
Kindle library management software Calibre for this. What I didn&rsquo;t like
about this is that it only gives you the last 20 articles in your
reading list. I mark way more than 20 articles every day. I also didn&rsquo;t
like the way it manages the articles. Each day&rsquo;s .mobi file download is
treated as a separate collection, all simply entitled &ldquo;Instapaper.&rdquo;
Which, unless you read every article in a collection and then delete it,
leaves you with a library full of undifferentiated Instapaper
collections. The best way I&rsquo;ve found to manage it so far is Wordcycler.
It&rsquo;s a simple, light-weight Windows application (it&rsquo;s based on the Mac
application Ephemera) that sits in your tray. When you plug your Kindle
into your computer via USB, it automatically downloads all your new,
unread Instapaper articles and syncs them to your Kindle as separate
titles, and also takes any articles you deleted from your Kindle and
archives them on Instapaper. (You can also set it to automatically eject
your Kindle afterwards, so all you really have to do is plug it in, then
unplug it after it says it&rsquo;s done.)</p>

<p>This is nearly perfect two-way syncing. The only problem I have left
with this set up is when I want to share something. There&rsquo;s basically no
way to get an article out of this loop. What I do now is: if there&rsquo;s an
article I want to share after reading, I leave it on the Kindle (so it
doesn&rsquo;t archive it on the Instapaper) until I remember to share it when
I&rsquo;m at a computer, then go to the Instapaper site, scroll through the
list of articles until I find the one I want to share, then share it and
archive it. This effectively means I don&rsquo;t share long articles very
often anymore.</p>

<p>It&rsquo;d be nice if maybe there was a way to mark an item on the Kindle to
go to a certain folder in Instapaper when it gets synced. Even better
would be if there was a way to share it directly from the Kindle, maybe
using the Google Reader bookmarklet, or a third-party app of some sort.</p>

<p>Along these lines, I wish the Kindle OS was more configurable. Having to
hack it to align left, like I mentioned, was annoying. There doesn&rsquo;t
seem to be an easy or sanctioned way to change out the screen saver
images. It has wi-fi connectivity, so it feels really backwards to have
to physically plug the device into the computer at all. I haven&rsquo;t looked
at all the hacks that are available yet, so maybe that functionality is
out there, but it seems like a pretty obvious thing to have native
functionality for.</p>
</html></description>
		</item>
		
		<item>
			<title>Quick App Idea</title>
			<link>http://blog.byjoemoon.com/quick-app-idea</link>
			<pubDate>Wed, 22 Dec 2010 00:00:00 PST</pubDate>
			<guid>http://blog.byjoemoon.com/quick-app-idea</guid>
			<description><html><p>Here&rsquo;s an idea for a distributed, peer-to-peer (though not massively
distributed, probably no more than three nodes in any network) backup
system that&rsquo;s sort of a mix between Dropbox and a backup service like
Carbonite or Backblaze.</p>

<p>Off-site backup is probably a really good idea. But. I&rsquo;m not sure if
it&rsquo;s a good enough idea that I want to pay a monthly fee for it. And I
would imagine that most people have a significant amount of excess
storage in their homes. So this app would incorporate that excess
storage with the background syncing of one of the services mentioned
above.</p>

<p>Here&rsquo;s how it would look in practice: you and a friend install the
software and set aside a certain amount of space for the other user. You
set certain folders to backup, and the software waits until it&rsquo;s late at
night or cpu/bandwidth is not being used, and syncs the folders to an
encrypted thing on the other node.</p>

<p>It would be fairly fire-and-forget, though it would have to notify you
if you&rsquo;re getting close to the storage limit you&rsquo;ve been allocated by
your friend (stingy jerk).</p>

<p>You could also just set this up between your home and work computers,
say. I can imagine, too, for even more simplicity, a completely
closed-box system, a piece of hardware that&rsquo;s basically a big hdd with
an ethernet port and this app on it that you could stick on a friend&rsquo;s
home network or your work network.</p>

<p>I&rsquo;d pay (once) for this.</p>
</html></description>
		</item>
		
		<item>
			<title>Webs and Streams</title>
			<link>http://blog.byjoemoon.com/webs-and-streams</link>
			<pubDate>Sat, 08 May 2010 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/webs-and-streams</guid>
			<description><html><p>There was a lot of mayhem surrounding the release of Google Buzz that
I&rsquo;ve been thinking about. First, of course, were the many complaints
about privacy, some of which were entirely reasonable, and others of
which were not. I think most of the reasonable complaints boil down to
the actual, technical process of releasing the technology as opposed to
the technology itself, which when regarded alone doesn&rsquo;t add anything
particularly novel to the social networking ecosystem. These actual,
technical process issues, like integrating with GMail and automatically
populating a public contact list, etc. are not especially interesting to
me, and others (<a href="http://www.danah.org/papers/talks/2010/SXSW2010.html">danah
boyd</a>) have
written more intelligently than I could hope to on the subject.</p>

<p>There are two other aspects of Buzz that I find much more interesting.
One is the well trod issue of the public-private spectrum, and the other
is the discussion around &ldquo;crossing streams,&rdquo; or automatically feeding
content from one service to another/others. I&rsquo;ll address the former
briefly, but only in how it pertains to my larger point, which is mostly
about crossing streams.</p>

<p>The social web as we know it today has formed into distinct
<a href="http://www.tbray.org/ongoing/When/201x/2010/03/28/Compartmentalization">silos</a>
of information; to different degrees, you must subscribe to each
individual service in order to add to or draw from each distinct pool of
data. While some are explicitly more public venues (like Twitter), and
others are moving in public direction (Facebook), they&rsquo;re all more or
less isolated, and the only way to operate across them is to use some
sort of automated feed duplication service, which can be annoying for
several reasons. And while Buzz is not that interesting to me as another
one of these services (the roll-out was botched, the integration with
GMail is weird, and the UI of the service itself is just acceptable), it
is interesting in that it pushes open standards for social networking
content, making it a force for interoperability.</p>

<p>Interoperability changes things. Ultimately, it kind of elides the
differences between the types of content that the different services are
used for, and even introduces elision between content outside of social
networks. If you take interoperability to its logical extreme, almost
all digital communication is the same kind of stuff; <a href="http://www.buzzmachine.com/2010/04/07/what-is-content-then/">it&rsquo;s all just
content</a>.
To include:</p>

<ul>
  <li>News articles, essays, and blog posts</li>
  <li>Tweets and status updates</li>
  <li>Shared links</li>
  <li>Photos/photo sets</li>
  <li>Podcasts (maybe)</li>
  <li>Comments (<a href="http://www.buzzmachine.com/2010/03/23/the-problem-with-comments-isnt-them/">Jeff Jarvis on comments</a>.)</li>
  <li>&ldquo;Likes&rdquo;</li>
  <li>Email</li>
  <li>Chats</li>
  <li>SMS</li>
</ul>

<p>In the abstract, there are very few true distinctions between these
types of content. The distinctions are mostly artificial. The
public-private spectrum seems like one that&rsquo;s real and relevant, so I&rsquo;ll
explore that a bit here.</p>

<p>Let me enumerate all the distinct points on this spectrum that I can
think of.</p>

<p>First, there&rsquo;s public content. Public content goes from one point to
every other point. This is broadcast content, which most traditional web
1.0 content is.</p>

<p>There&rsquo;s also public, but directed content, like @replies on Twitter,
Buzz, Facebook&rsquo;s Wall, comments in general (sort of), etc. These are
viewable by anyone, but directed at someone specific, i.e. public
content from one point to another.</p>

<p>This kind of public content (tweets at least) can address more than one
person specifically, though, so the next would one would public content
from one point to many.</p>

<p>Then there are things like mailing lists, private IRC channels, and
protected tweets, which are not open to the public but involve
(potentially) many points. Each piece of content in these mediums goes
from one point to many but not all, i.e. private content from one to
many.</p>

<p>Then there are text messages, chats, and e-mails, that (usually) go from
one point to another, privately.</p>

<p>So five basic types: 1&rarr;all, 1&rarr;1[public], 1&rarr;many[public],
1&rarr;many[private], 1&rarr;1[private].</p>

<p>Fairly obviously, however, there&rsquo;s no fundamental difference between
1&rarr;1[private] and 1&rarr;many[private]; the former is simply a subset of the
latter. Likewise with the [public] counterparts. Which leave us with
three: 1&rarr;all, 1&rarr;many[private], and 1&rarr;many[public].</p>

<p>The difference between, 1&rarr;all and 1&rarr;many[public], though, is also
somewhat artificial in that it&rsquo;s really an extraneous piece of metadata.
The public nature of the content renders the fact of any intended
recipient sort of functionally meaningless.</p>

<p>So, basically, the only real difference between any of these content
types is publicity vs. privacy (or public-ness vs. private-ness).
(There&rsquo;s a problem here, too, namely the &ldquo;information wants to be free&rdquo;
one. Fact is: there&rsquo;s nothing stopping anyone from forwarding or
publishing anything sent to them, privately or not, and there are not
clear legal or social rules governing most private information retention
behavior (outside of medical and legal contexts). See: <a href="http://sam.bluwiki.com/blog/2010/03/confession-i-was-one-who-came-forward.php">Calacanis vs.
Odio</a>.)</p>

<p>It&rsquo;s not really a spectrum at all, then, but a simple binary, and a
tenuous one at that.</p>

<p>(Ok, so this is maybe abstracting the issue to the point of breaking.
Realistically it&rsquo;s probably more like what Jeff Jarvis is saying: <a href="http://www.buzzmachine.com/2010/05/08/confusing-a-public-with-the-public/">There
is public, and then there are
publics</a>.
This is worth talking about separately, I think.)</p>

<p>Most other differences between the above listed types of data,
distinctions like length (whether it&rsquo;s enforced or not), immediacy,
context, and format are so self-evidently arbitrary and artificial, that
I think they can be dealt with quite handily with proper use of
metadata.</p>

<p>So, let&rsquo;s imagine a web of total interoperability. I think a &ldquo;web&rdquo; is
actually no longer as appropriate a metaphor, and would have to give way
to a metaphor of a river or stream. All public data produced would be
available in real time all at once. Without intervening silos like the
the social networks that currently exist, there would be only two
fundamental points of filter: production and consumption. Producers of
content would be responsible for attaching appropriate metadata, which
would include many of the points of distinction above: intended
recipients, importance, subject (both in the sense of taxonomical
subject, e.g. &ldquo;technology,&rdquo; as well as in the sense of topic, such as
&ldquo;re:[html link]&rdquo;), location, etc. Consumers could then apply filters
using the available metadata, and display different filter sets in
customized ways, with the obvious different toggles for displaying a
specific filter set being chronology and real time updates.</p>

<p>So, let&rsquo;s imagine all above-defined content posted by everyone as a
single Stream of (as yet) undifferentiated data. What web services
currently do as clients of the Stream, essentially, is to filter it into
discrete streams and display them in conventional ways. Twitter filters
out everything but the people you follow on Twitter, and displays them
in reverse-chronological order. Your email client filters out everything
except things addressed to you (and spam, generally (and in herein you
already see some pretty intense algorithmic filtering coming into
play)).</p>

<p>You can even think of individual websites as this sort of a River
client. If individual people are the only real content creation points,
a website simply aggregates certain pieces of content from one or more
individuals (or, from a filter-only perspective, filters out everything
except certain pieces of content from one or more individuals).</p>

<p>What I like about looking at web activity this way is that, with the
right client, one could cut out all these intervening services and have
a direct stream of all the data you want to see, displayed the way you
want. My dream client would consist of a multi-column display with each
column representing a different filter or set of filters, each
independently configured to display its data in a way appropriate to its
contents. For example, my &ldquo;inbox&rdquo; would be all new, private content
addressed to me from whitelisted contacts, displayed in chronological
order. In the next column would be private messages to me from
non-whitelisted contacts, mailing lists, etc. The third column might be
the &ldquo;social&rdquo; column, which would aggregate everything posted by close
friends and family, stuff I don&rsquo;t want to miss any of, displayed in
chronological order. Then a &ldquo;news&rdquo; column, which would be posts from
blogs, twitter, etc. in reverse chronological order, so I could see just
the latest updates at any given time. This column would probably only
display headlines. (So, basically, this would be my twitter replacement,
since a large portion of my twitter feed is really just headlines or
short descriptions of blog posts and news items, with links.) I could
configure this column to scroll constantly, or not. Another alternative
display would be an algorithmic one, designed to show you first things
you will find most relevant and interesting, like Google Reader&rsquo;s
&ldquo;magic&rdquo; sort.</p>

<p>I also imagine this client to include output functions, which would
consist of more columns, each configured with different preset filters:
one broadcasting publicly, another to share with friends, family, etc. I
could alternatively configure certain outgoing filter-columns to append
metadata pertaining to the content, or manually tag each post as I write
it. I would tag this post, for example, with &ldquo;boring, unrealistic web
nonsense,&rdquo; and you could subscribe to my public feed, but filter out
everything I post with this tag. Or you could subscribe to only this
stuff. Or only this stuff and photos. Etc.</p>

<p>Like I mentioned, this is really just a simplification/streamlining of
what already happens, but I think what&rsquo;s powerful about the idea is that
it removes the current data silos like Twitter and Facebook, and
decentralizes them, so that each individual has more control over which
data is siloed and how.</p>

<p>I think it also flattens the publishing environment, putting more
emphasis on individual authors. Websites and news organizations could
more explicitly be understood for what they are: aggregators. One way I
can imagine a publishing model is for individual authors to introduce
advertisements inline in all or some of the items in the feeds they
consider &ldquo;published content.&rdquo;</p>

<p>None of this is to say that I think the entire Web should be a Stream.
There are clearly a lot of websites that should be website and not
streams of content. All stores (or more broadly: organizations), for
example, including online stores and the websites of physical stores
make more sense as more or less static websites. Web applications like
maps and Wikipedia and countless others make more sense as static web
pages. But I think a Stream is a better metaphor for a large part of
current communicative Web activity.</p>
</html></description>
		</item>
		
		<item>
			<title>Why I trust Google</title>
			<link>http://blog.byjoemoon.com/why-i-trust-google</link>
			<pubDate>Wed, 19 Aug 2009 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/why-i-trust-google</guid>
			<description><html><p>From <a href="http://www.joelonsoftware.com/articles/StrategyLetterV.html">this article</a>
that Nick shared a while ago:</p>

<blockquote>
  <p>Every product in the marketplace has <em>substitutes</em> and <em>complements</em>…
A complement is a product that you usually buy together with another
product. Gas and cars are complements. Computer hardware is a classic
complement of computer operating systems…</p>
</blockquote>

<p>Demand for a product increases when the prices of its complements
decrease. For example, if flights to Miami become cheaper, demand for
hotel rooms in Miami goes up&ndash;because more people are flying to Miami and
need a room. When computers become cheaper, more people buy them, and
they all need operating systems, so demand for operating systems goes
up, which means the price of operating systems can go up.</p>

<p>The whole thing is definitely worth reading. (Though, I have to say: the
principle doesn&rsquo;t seem to hold perfectly true all the time, or at least
it doesn&rsquo;t seem to work out that well, e.g. the netbook market is pretty
well commoditized, I think, which you&rsquo;d think would help Microsoft a
lot, but it turns out that, among other things, people don&rsquo;t want to pay
$300 for an operating system on a $500 computer. But that&rsquo;s a different,
and itself pretty involved topic.)</p>

<p>The article got me thinking about Google in these terms, and about how
Google&rsquo;s business model works and the incentives it sets up. And I
concluded that I trust Google. Okay, to be honest, I trust Google in the
sense of &ldquo;with my data&rdquo; because, well, it is convenient to do so. But I
trust Google to keep doing great things not because I think Google&rsquo;s
motives are altruistic, but because Google&rsquo;s incentives are ultimately
aligned with my own.</p>

<p>Google makes the vast majority of its money on advertising, Google
Search ads and Adsense, which really translates to web use at large. The
more people use the web, the more ads they see, and the more money
Google makes. So if you look at Google in the terms of the above-linked
article, its complements are everything that sit between consumers and
using the web on any medium. That includes everything from web browsers
and e-mail clients to operating systems, mobile and desktop, to actual
laptop and desktop hardware, and even telephony applications and
infrastructure at large.</p>

<p>And Google&rsquo;s many fragmentary product launches and announcements make a
lot more sense in this context. Google provides free alternatives like
Chrome, Android, Gmail, Google Apps, Google Voice, Youtube, and the
upcoming Chrome OS not because it wants to win in any of these markets,
but to create competition so that these markets provide a better
experience to the consumer, making it easier to spend more time on the
web, where they can see Google ads.</p>

<p>Google wants to make it easier for us to use the web faster and
everywhere, which is a win for everyone.</p>
</html></description>
		</item>
		
		<item>
			<title>David Foster Wallace</title>
			<link>http://blog.byjoemoon.com/david-foster-wallace</link>
			<pubDate>Mon, 20 Jul 2009 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/david-foster-wallace</guid>
			<description><html><p>I&rsquo;m reading through <a href="http://asupposedlyfunblog.wordpress.com/">A Supposedly Fun
Blog</a> and ran across <a href="http://asupposedlyfunblog.wordpress.com/2009/07/08/on-beauty/">this
passage</a>:</p>

<blockquote>
  <p>…all of this, the aggrieved and distrustful critical reaction to any
novel that comes within a quarter mile of what we might call the
postmodern, has been established for years. It was well established, I
mean to say, when David Foster Wallace wrote Infinite Jest. Which
means that he, like all authors today, wrote in the knowledge that the
literary world would be filled with exactly those kinds of readers and
critics who would dismiss his work out of hand for its artiness and
pretension. And yet Foster Wallace wrote on, like a lot of writers do,
in the stubborn belief in the good faith of his audience.</p>
</blockquote>

<p>(which is actually a blockquote from
<a href="http://www.ordinary-gentlemen.com/2009/07/a-few-thoughts-on-a-supposedly-fun-blog/">here</a>)</p>

<p>And it really speaks to what I love about DFW, because it&rsquo;s not like he
was without fear of being judged. It seems clear from his writing that
he was actually extraordinarily, excruciatingly self-conscious about it
all, but he did trek on.</p>
</html></description>
		</item>
		
		<item>
			<title>More Convergence: How Twitter (as an open protocol, ideally) could (and maybe will) replace IM and all web commenting and make them both better</title>
			<link>http://blog.byjoemoon.com/more-convergence</link>
			<pubDate>Thu, 21 May 2009 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/more-convergence</guid>
			<description><html><p>I&rsquo;ve been messing with different Twitter clients. My favorite desktop
client is still <a href="http://www.twittergadget.com/">TwitterGadget</a>, because
it&rsquo;s so lightweight and is fairly keyboard friendly (though it certainly
has room to improve on this front). It&rsquo;s actually a Gmail gadget, but I
use it as a <a href="http://labs.mozilla.com/projects/prism/">Prism</a>
application. <a href="http://www.tweetdeck.com/beta/">Tweetdeck</a> and
<a href="http://www.twhirl.org/">Twhirl</a> seem to be the most popular desktop
clients, but they&rsquo;re not that responsive for me maybe because they&rsquo;re
Adobe Air applications. I&rsquo;m sure they&rsquo;re great for heavy Twitter users,
but for the functionality that Twitter provides for me at this point,
all I really want is something simple and lightweight.</p>

<p><a href="http://motionobj.com/simplytweet/">SimplyTweet</a> is still my favorite
iPod Touch client, and I wish there was a desktop version, frankly. The
feature list is really great, and includes:
<a href="http://www.instapaper.com/">Instapaper</a> support(!); a &ldquo;View
Conversation&rdquo; button for each tweet, which is a bit buggy but is a
really cool idea; grouping, in the form of &ldquo;Saved Views&rdquo; that filters by
contact and &ldquo;Saved Searches&rdquo; which filters by keyword.</p>

<p>Really, I think those are about all the features you need in a desktop
client to replace IM and make Twitter more robust and give it a more
immediate, IRC-like conversational feel. With #hashtags and direct
messages, Twitter already has most of the functionality. The Adobe Air
applications that I mentioned earlier are almost there with their
columns, but I imagine the experience more like my hands-down favorite
IM client, Digsby. Each window/tab would represent either a contact (a
two-way conversation); a user-defined group (e.g. &ldquo;Real Friends&rdquo; or
&ldquo;Apps&rdquo; (which are actually two of my saved views in SimplyTweet)); a
keyword search (firehose of what people are saying about a given
subject); or a #hashtag (make-shift chatroom on a given subject, or,
really, not (on a given subject)). The client could use the direct
messaging functionality to make two-way or group conversations private.</p>

<p>For a two-way chat, the tab/window would simply be a filter for that
contact&rsquo;s @replies, and would transparently append the appropriate
@reply to each message sent from the tab/window. Groups and searches
would just be normal filtered views. #Hashtag chatrooms, then, would
just be a filtered view of the #hashtag that would append the #hashtag
onto outgoing messages. I think how private chats would work is with
some sort of obfuscated #hashtag and the client would transparently
send direct messages to all the parties involved.</p>

<p>Since I imagine #hashtags would have to become arbitrarily long,
Twitter would probably have to get rid of the artificial 140 character
limit, which I think is pretty stupid and arbitrary (does anyone really
use the text message interface to Twitter?) anyway.</p>

<p>I think something like this would make Twitter much more engaging.
Twitter isn&rsquo;t as amenable to long conversations as it is to unprompted
unidirectional musings, which are fine, but aren&rsquo;t suitable to everyone
(i.e. me). It also opens up all of the conversation that happens in
private right now over IM to the public. Not that all conversation
should be public, but there&rsquo;s no real reason for a lot of it to be
private, and if it&rsquo;s in the public, it can potentially engage and
connect many people who might otherwise be unaware of each other.</p>

<p>Also, <a href="http://www.readwriteweb.com/archives/integrate_twitter_comments_int.php">Twitter
comments</a>
becoming a standard would be cool (I&rsquo;m looking into implementing it
here<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>). This would expose stuff about what you&rsquo;re reading and
commenting on to your Twitter stream, creating a conversation space on
Twitter about a post, and also provides an element of persistent online
identity that I&rsquo;ve <a href="http://blog.byjoemoon.com/post/100514718/persistent-online-identity">written about
before</a>.
I can even imagine some sort of trackback system implementation that
aggregates commenting on a subject across websites to make one giant
conversation about a <em>subject</em> as opposed to a particular blog post or
whatever.</p>

<p>Twitter&rsquo;s simplicity allows it to be extremely flexible, which is why
it&rsquo;s already being used in so many different ways, and what gives it so
much potential. And also <a href="http://regulargeek.com/2009/02/25/twitter-is-not-broken-it-is-infrastructure/">why it should become an open
protocol</a>,
like email.</p>

<p>Aside: I&rsquo;m wondering if anyone uses the &ldquo;Favorites&rdquo; functionality of
twitter. Apparently you can mark individual tweets. I&rsquo;m not sure how
that&rsquo;s useful.</p>

<h3 id="notes">Notes</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Update: I just realized Disqus has integrated with Twitter, which is
close, but not all there. <a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</html></description>
		</item>
		
		<item>
			<title>A Few of My Favorite Things</title>
			<link>http://blog.byjoemoon.com/a-few-of-my-favorite-things</link>
			<pubDate>Mon, 27 Apr 2009 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/a-few-of-my-favorite-things</guid>
			<description><html><p>Most of my favorite Firefox add-ons have to do with maximizing browser
window real estate. Stuff like <a href="https://addons.mozilla.org/en-US/firefox/addon/4762" title="Hide Menubar">Hide
Menubar</a>
and <a href="https://addons.mozilla.org/en-US/firefox/addon/1568" title="Full Fullscreen">Full
Fullscreen</a>.
I&rsquo;ve also replaced most of my bookmark menu items with <a href="https://addons.mozilla.org/en-US/firefox/addon/10127" title="Site Launcher">Site
Launcher</a>,
which gives quick keyboard access to bookmarks (including bookmarklets
like <a href="http://googlereader.blogspot.com/2008/05/share-anything-anytime-anywhere.html" title="Share in Google Reader">Share in Google
Reader</a>).
<a href="https://addons.mozilla.org/en-US/firefox/addon/6665" title="Prism">Prism</a> is
also a really cool add-on that creates sort of desktop web-apps. I use
it with my twitter client of choice, <a href="http://www.twittergadget.com/" title="Twitter Gadget">Twitter
Gadget</a>, and with <a href="http://gmailblog.blogspot.com/2008/12/new-in-labs-tasks.html" title="Gmail Tasks">Gmail
Tasks</a>.</p>

<p><a href="https://addons.mozilla.org/en-US/firefox/addon/748" title="Greasemonkey">Greasemonkey</a>
is potentially really powerful, but the only scripts I have running
right now are
<a href="http://userscripts.org/scripts/show/8551" title="Autopagerizer">Autopagerize</a>,
which ties a lot of pagerized web pages together (e.g. I don&rsquo;t have to
hit the next link to see the next page of Google search results, it just
loads them dynamically under the first page of results as I scroll
down), and <a href="http://userscripts.org/scripts/show/45091" title="Twitter Search Results on Google">Twitter Search Results on
Google</a>,
which is really nice because it gives me a quick look at real-time
results for every search I perform on Google.</p>

<p>The thing I&rsquo;ve gotten the most mileage out of recently, though, is
<a href="http://www.instapaper.com/" title="Instapaper">Instapaper</a>. This bookmarklet,
in combination with the iPod Touch/iPhone app, has really changed the
way I read. The way it works is very similar to
<a href="http://lab.arc90.com/experiments/readability/" title="Readability">Readability</a>,
which we&rsquo;ve talked about before, and <a href="https://addons.mozilla.org/en-US/firefox/addon/7661" title="Read It Later">Read It
Later</a>,
or sort of a combination of the two. The bookmarklet basically saves the
webpage you&rsquo;re on and closes the tab, and then you can go back to the
Instapaper site and see a list of all the articles you&rsquo;ve saved. When
you hit a link on that list, it goes to a version of the article that&rsquo;s
formatted to be easily read and stripped of ads and other nonsense, much
like Readability.</p>

<p>The sort of profoundly game-changing part for me was the iPod Touch app,
though. Because you can sync the app when you&rsquo;re online, which downloads
articles to the iPod&rsquo;s hdd, you can take your web reading material with
you wherever you go. For me, this has meant that I&rsquo;m almost never
without reading material, and I can read stuff without having to be
sitting in front of the computer, which has done wonders for my reading
attention span. I&rsquo;ve been reading a lot more, and finishing articles,
even really long ones, at a much higher rate without the distractions of
being on the computer.</p>

<p>The free version is very robust, and I got along just fine with it, but
I liked it so much that I paid 10 bucks for the pro version (which adds
a few fairly minor interface tweaks, like tilt-scrolling, which I
thought was kinda dumb but then I tried it and it&rsquo;s actually really
cool). For those with smartphones that are not the iPhone, the mobile
webapp is not bad, but doesn&rsquo;t give you offline access, which maybe is
only a problem with an iPod Touch.</p>

<p>My experience with Instapaper has really turned me around on the idea of
the Kindle, too. My only gripes with the iPod app is that the screen is
small and it gets a little tiring to read on a backlit screen, so
something like the Kindle would probably be ideal. And Instapaper has
<a href="http://www.instapaper.com/user/kindle" title="Instapaper Kindle support">beta Kindle
support</a>,
so now I sort of really want one.</p>

<p>Another service that I&rsquo;ve found really useful is
<a href="http://www.tripit.com/" title="Tripit">Tripit</a>. Tripit lets you just forward
airline reservations to them and they parse it for pertinent info. And
with the really awesome Google Calendar support, which creates a
calendar (which you can add to your iPhone calendar) that adds events
with all the flight info automatically, including confirmation numbers,
as well as an iPhone app, you don&rsquo;t even really ever have to go to the
website. Maybe only really useful for frequent travelers, but it&rsquo;s easy
to set up and makes paper itineraries obsolete.</p>

<p>Anyone like to share some of their favorite things?</p>
</html></description>
		</item>
		
		<item>
			<title>Torture</title>
			<link>http://blog.byjoemoon.com/torture</link>
			<pubDate>Sun, 26 Apr 2009 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/torture</guid>
			<description><html><p>I&rsquo;ve been following the torture stuff moderately closely. For people who
haven&rsquo;t been, the main arguments seem to boil down to:</p>

<ol>
  <li>Is waterboarding (and other techniques authorized in the recently
released legal memos) torture?</li>
  <li>Did it save lives?</li>
  <li>Should the people who conducted it be prosecuted?</li>
  <li>Should the people who authorized it be prosecuted?</li>
</ol>

<p>On the first question, I feel like Christopher Hitchens&rsquo; take on it,
after subjecting himself to the experience, is pretty compelling. So is
John McCain&rsquo;s:</p>

<blockquote>
  <p>Anyone who knows what waterboarding is could not be unsure. It is a
horrible torture technique used by Pol Pot and being used on Buddhist
monks as we speak.</p>
</blockquote>

<p>Also, this argument that it can&rsquo;t be torture because we subject our own
servicemen to it in SERE school is retarded. It&rsquo;s there in SERE school
to train our servicemen specifically how to resist torture techniques
that the enemy might use. If anything, that&rsquo;s an argument for the fact
that it <em>is</em> torture.</p>

<p>On the second question, this sums up my thinking pretty nicely:</p>

<blockquote>
  <p>Moreover, even if we were starting from a blank slate and we could
simply ignore the fact that techniques like waterboarding are
proscribed by numerous laws and treaties, to make a policy case for
the use of such techniques, you would have to do much more than
establish that they occasionally have produced actionable
intelligence. Among other things, you would have to prove that 1) such
information could not have been extracted using other means, 2) that
the misinformation produced by such methods doesn&rsquo;t overwhelm the
accurate information to the point of rending the whole exercise
pointless, 3) that the strategic costs of using such techniques
(international outrage, increased radicalization of the Muslim world,
increased danger to U.S. troops, etc.) don&rsquo;t outweigh the benefits,
and 4) the value of the information produced is worth the tradeoff of
never being able to use that information (or the fruits thereof) in
court and severely jeopardizing any hope of ever convicting that
individual in any constitutionally compliant legal proceeding.</p>
</blockquote>

<p>The <a href="http://www.anonymousliberal.com/2009/04/who-cares-whether-torture-is-effective.html">whole
post</a>
is worth reading (and not that long).</p>

<p>As far as I can tell, pretty much everyone agrees that the answer to the
third question is &ldquo;no.&rdquo; (Though with the specific case of 183
waterboardings for KSM, I think it&rsquo;s more dubious because that is
outside the guidelines of even the recently released memos.)</p>

<p>The fourth question is the one I&rsquo;m still torn about. On one hand, isn&rsquo;t
that how the system&rsquo;s supposed to work? Trial and verdict? And if we
don&rsquo;t, what kind of precedent does that set? On the other hand,
<a href="http://politics.theatlantic.com/2009/04/why_obama_doesnt_care_about_yoo.php">this</a>
is an interesting read. Still not sure what to think.</p>

<p>Update: Two more articles on the issue of question 2, both NYT op-eds:
&ldquo;<a href="http://www.nytimes.com/2009/04/24/opinion/24zelikow.html">A Dubious C.I.A.
Shortcut</a>,&rdquo;
&ldquo;<a href="http://www.nytimes.com/2009/04/23/opinion/23soufan.html">My Tortured
Decision</a>.&rdquo;</p>
</html></description>
		</item>
		
		<item>
			<title>Twitter</title>
			<link>http://blog.byjoemoon.com/twitter</link>
			<pubDate>Thu, 19 Mar 2009 00:00:00 PDT</pubDate>
			<guid>http://blog.byjoemoon.com/twitter</guid>
			<description><html><p>I&rsquo;ve been on twitter for a few months now, but I have to admit, I wasn&rsquo;t
really getting it until I read
<a href="http://bhc3.wordpress.com/2009/03/17/how-to-tweet-your-way-out-of-a-job/">this</a>.
I mean, there are clearly many ways to use twitter, including the reason
that twitter gives you: to broadcast your current status.</p>

<p>But I, like many, don&rsquo;t really feel the need to tell people when I&rsquo;m
pooping, and also don&rsquo;t feel the need to know when others are pooping. I
think this is why many people dismiss twitter, like I did until
recently.</p>

<p>But, in the exchange linked to above, something really familiar happens:
someone overhears a public conversation. And that&rsquo;s what I now realize
is the most useful analogy for twitter: public conversation.</p>

<p>IM is cool, but it&rsquo;s limited by nature; it&rsquo;s a connection between two
points. Twitter allows similar exchanges, but in public, so anyone can
join, and anyone can search for what people are saying on any given
subject.</p>

<p>I think it actually accomplishes a lot of the same stuff as the party
chat we&rsquo;ve been using: it&rsquo;s low-involvement, people can pay as much
attention to it as they feel like, and not everyone has to be engaged in
the conversation to follow it.</p>

<p>I feel like a lot of people would be more interested, or at least less
resistant to it if they marketed it this way and provided an IM
interface. Apparently there used to be an IM interface, but they took it
down in October because it was buggy, and they&rsquo;ve put it on their
long-term to do list, with no update since.</p>

<p>There&rsquo;s a bot that uses jabber called <a href="http://excla.im/">excla.im</a>, but
I couldn&rsquo;t get it to work, and it apparently only allows you to send
tweets, not get them.</p>

<p>So, that&rsquo;s too bad. Digsby&rsquo;s twitter functionality is pretty good, but
the interface is not great. I don&rsquo;t see why they don&rsquo;t offer their
functionality in the form of a contact, with the exact same user
interaction as any other contact. That would make me happy.</p>
</html></description>
		</item>
		
		<item>
			<title>Persistent Online Identity</title>
			<link>http://blog.byjoemoon.com/persistent-online-identity</link>
			<pubDate>Tue, 10 Feb 2009 00:00:00 PST</pubDate>
			<guid>http://blog.byjoemoon.com/persistent-online-identity</guid>
			<description><html><p>OpenID, OpenSocial, Disqus all seem to point to the inevitability of
persistent online identities. I imagine this POLID as something that
will follow you from site to site, and be easily aggregable into a
lifestream or personal history of web interactions.</p>

<p>This probably raises privacy concerns, but (Chabo and I have agreed)
privacy is obsolete. Privacy is only going to become increasingly
difficult to maintain, and it runs counter to transparency. I think
transparency is good and the real way to deal with issues that privacy
concerns seek to address is to combine more transparency with tolerance
of deviant but harmless behavior.</p>

<p>Anyway, aside from that, I think the concept of POLID is good because it
adds an element to the web that has been heretofore lacking:
accountability. It adds the weight of history to online interactions,
and I think the implications for this are only good. It could mean the
end of spam, trolls, and a general increase in civility in online
interactions, without even necessarily sacrificing anonymity.</p>

<p>I don&rsquo;t know exactly what the mechanics of this would look like, but
here are a couple of ideas:</p>

<p>In comment systems, all comments under a certain ‘identificationicity&rsquo;
(some function of the history-weight of a POLID attached to the
commenter) would be subject to admin review before being published.</p>

<p>You probably wouldn&rsquo;t even take emails from addresses that aren&rsquo;t
attached to a POLID.</p>

<p>You could establish multiple POLIDs if you wanted to, thus keeping the
possiblity of an anonymous online presence. But even these alternate
anonymous POLIDs would be subject to being flagged as spammers or
trolls.</p>

<p>I guess the tricky part is the initial bootstrapping of a new POLID, but
I think OpenID fixes this by associating your ID with a website or
something (I&rsquo;ve looked into OpenID several times but still haven&rsquo;t
figured out how it works exactly).</p>
</html></description>
		</item>
		
		<item>
			<title>Convergence, or All The Things I've Been Thinking About That Turned Out To Be The Same Thing</title>
			<link>http://blog.byjoemoon.com/convergence</link>
			<pubDate>Tue, 30 Dec 2008 00:00:00 PST</pubDate>
			<guid>http://blog.byjoemoon.com/convergence</guid>
			<description><html><p>So, I came pretty close to buying an iPhone because I found a deal for a
refurbished 8GB 3G for $99. I ultimately decided not to after realizing
that the cost of the device is really pretty trivial compared to the
contractual commitment. I&rsquo;m sure I&rsquo;ll cave soon or later, but I&rsquo;m going
to try to wait until I have a real job and for there to be some more
Android phones (I saw my first G1 in the wild a few days ago) on the
market (which I think there will be a bunch more early in 2009, though
I&rsquo;ll probably be reluctant to buy a first generation Android phone from
any manufacturer).</p>

<p>But it got me thinking a lot about smart phones, and how they&rsquo;re slowly
turning into universal remotes, except not just for televisions and dvd
players, but for everything. Kind of like what Ben Kingsley had in
<a href="http://www.imdb.com/title/tt0105435/">Sneakers</a>.</p>

<p>It also got me thinking about how a giant (like 10&rdquo; screen) iPhone would
be a great netbook/tablet, especially with
<a href="http://blog.byjoemoon.com/?q=node/1260">Swype</a>, which makes me think
that netbooks are just big smart phones. A bunch of OEMs are supposed to
release new touchscreen convertible (swivel-screen) netbooks at CES in
January. I imagine the hardware will all be pretty similar, and really
usability, i.e. the user interface, will decide the quality of the
products. And I think both android and the iPhone OS (which is OSX, I
think?) are pretty much ideal for these.</p>

<p>I&rsquo;m pretty annoyed at how much service still costs, though. Both cell
phone voice/data/sms and just broadband in general. And it seems like
the problem in both cases stems from the fact that the initial
infrastructure costs prohibit a lot of competition, not to mention
building more infrastructure would be redundant. But it sucks that just
a few corporations own all the infrastructure (and can subsequently
charge 20 cents per text message, which is madness).</p>

<p>I separately started thinking about the recent proposal by Kevin Martin,
chairman of the FCC (who is apparently <a href="http://news.slashdot.org/article.pl?sid=08/12/30/2335200&amp;from=rss">still talking about
it</a>,
and even willing to drop the censorship provision (though I never really
cared, since his original proposal was opt-out)), to create a free
nationwide broadband wi-fi network. I have erotic fantasies about this
proposal.</p>

<p>I think the implications of this are bigger than just everyone being
able to check their email from anywhere. Because all of the old
media/communications are becoming digital and subsequently (though
slowly) migrating to the internet anyway.
<a href="http://www.hulu.com/">Television</a>,
<a href="http://www.netflix.com/HowItWorks">movies</a>,
<a href="http://www.last.fm/">music/radio</a>, newspapers, even telecom is moving
slowly VoIP-ward.</p>

<p>A ubiquitous wireless network of sufficient power makes all of the old
media obsolete.</p>

<p>We wouldn&rsquo;t even have to waste bandwidth on any of that other stuff. The
entire RF spectrum could all just be the global ubiquitous wireless
internet, and any communication between any two points could be through
the One Medium (though maybe this is bad for security?).</p>

<p>I think that would mean that hardware would lose the arbitrary
restrictions that the different systems put on them. The discrete points
of ‘cell phone,&rsquo; ‘smart phone,&rsquo; ‘netbook,&rsquo; ‘laptop&rsquo; and ‘desktop&rsquo; on the
spectrum of computing devices would flatten out and instead there would
exist devices at every point on the spectrum, tailored to suit
consumers&rsquo; preferences exactly.</p>

<p>The One Medium also makes cloud computing even more feasible, which
allows the hardware to be nothing more than an internet connection with
a UI.</p>

<p>Something that&rsquo;s really appealing about this, too, is that I think it
works more like a public utility or freeway system (remember the
‘information superhighway?&rsquo;), and no private corporation owns all of the
infos (except maybe the cloud computing providers (i.e. Google)? hm… uh
oh).</p>

<p>I&rsquo;m sure if that even if any of this is at all technically feasible,
it&rsquo;s still a long, long way off, but that&rsquo;s the direction I see it
going. Any thoughts?</p>
</html></description>
		</item>
		
	</channel>
</rss>
